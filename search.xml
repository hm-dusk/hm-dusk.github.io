<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CentOS7离线安装CDH]]></title>
    <url>%2F2019%2F08%2F16%2FCentOS7%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85CDH%2F</url>
    <content type="text"><![CDATA[CentOS7.6离线安装CDP，Cloudera Manager版本：6.3.0，CDH版本：6.3.0-1 本文环境 节点 IP地址 hdp001 192.168.171.10 hdp002 192.168.171.11 hdp003 192.168.171.12 环境准备磁盘准备离线安装包共计3G左右，请保证有足够空间。保证/opt目录有足够空间，至少20G 网络准备CDH支持IPV4，不支持IPV6 将主机名设置为全限定域名格式（FQDN：Fully Qualified Domain Name）sudo hostnamectl set-hostname foo-1.example.com 配置/etc/hosts文件，添加集群中所有全限定域名，也可以添加非限定名```bash 1.1.1 foo-1.example.com foo-1 2.2.2 foo-2.example.com foo-2 3.3.3 foo-3.example.com foo-3 4.4.4 foo-4.example.com foo-4 配置免密登录配置免密码登录教程请点击这里 关闭防火墙查看防火墙状态firewall-cmd --state或systemctl status firewalld临时关闭防火墙systemctl stop firewalld禁止开机启动systemctl disable firewalld 设置SELinux模式不关闭可能导致Apache http服务无法访问。 查看SELinux状态：getenforce如果是Permissive或者Disabled则可以继续安装，如果显示enforcing，则需要进行以下步骤修改模式 编辑/etc/selinux/config文件 修改SELINUX=enforcing行内容为SELINUX=permissive或者SELINUX=disabled 重启系统或者运行setenforce 0命令禁用SELinux 安装MySQL离线安装MySQL教程点击这里 新建数据库hive、ambari（为后续安装做准备）。 mysql> create database hive; Query OK, 1 row affected (0.00 sec) mysql> create database ambari; Query OK, 1 row affected (0.00 sec)]]></content>
      <categories>
        <category>大数据</category>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>CDH</tag>
        <tag>离线安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装VNC服务]]></title>
    <url>%2F2019%2F08%2F06%2FCentos7%E5%AE%89%E8%A3%85VNC%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[本文介绍在Centos7.6下安装VNC Viewer并通过Windows连接远程桌面的过程 安装gnome桌面如果Centos7为最小化安装，则需要单独安装gnome图形化桌面 [root@AccessGateway ~]# yum groupinstall -y "GNOME Desktop" 安装vnc server[root@AccessGateway ~]# yum install -y tigervnc-server 配置服务 复制一个服务设置模板，命名为vncserver@:1.service cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service 修改服务配置， [root@AccessGateway ~]# vim /etc/systemd/system/vncserver\@\:1.service [Unit] Description=Remote desktop service (VNC) After=syslog.target network.target [Service] Type=forking User=root #设置登录用户为root # Clean any existing files in /tmp/.X11-unix environment ExecStartPre=/bin/sh -c '/usr/bin/vncserver -kill %i > /dev/null 2>&amp;1 || :' #将这里的User改为root，-geometry 1920x1080选项指定连接分辨率，也可以不指定 ExecStart=/usr/sbin/runuser -l root -c "/usr/bin/vncserver -geometry 1920x1080 %i" PIDFile=/root/.vnc/%H%i.pid #这里指向root根目录地址 ExecStop=/bin/sh -c '/usr/bin/vncserver -kill %i > /dev/null 2>&amp;1 || :' [Install] WantedBy=multi-user.target 如果需要配置其他用户登录，则重复1,2步骤，再复制一个配置文件，修改相应用户 更新systemctl [root@AccessGateway ~]# systemctl daemon-reload 设置VNC密码VNC的密码跟系统的用户密码不一样，是使用VNC Viewer登陆时需要使用的密码 [root@AccessGateway ~]# vncpasswd Password: Verify: Would you like to enter a view-only password (y/n)? n A view-only password is not used # 这里不添加只读账号密码 # 每个不用的系统用户，设置密码时，需要切换到该用户下，执行此命令 # 如：su zhangsan 切换到zhangsan用户再执行上vncpasswd设置密码 启动服务启动刚才配置的服务，如果配置了多个，则需要启动相应的服务 [root@AccessGateway ~]# systemctl start vncserver@:1.service 设置开机自启动 [root@AccessGateway ~]# systemctl enable vncserver@:1.service 查看端口信息，VNC默认端口为5901，因为我启动了两个服务，所以还有一个5902端口存在 [root@AccessGateway ~]# netstat -lnpt|grep Xvnc tcp 0 0 0.0.0.0:5901 0.0.0.0:* LISTEN 10196/Xvnc tcp 0 0 0.0.0.0:5902 0.0.0.0:* LISTEN 11394/Xvnc tcp 0 0 0.0.0.0:6001 0.0.0.0:* LISTEN 10196/Xvnc tcp 0 0 0.0.0.0:6002 0.0.0.0:* LISTEN 11394/Xvnc tcp6 0 0 :::5901 :::* LISTEN 10196/Xvnc tcp6 0 0 :::5902 :::* LISTEN 11394/Xvnc tcp6 0 0 :::6001 :::* LISTEN 10196/Xvnc tcp6 0 0 :::6002 :::* LISTEN 11394/Xvnc 配置防火墙，开通端口如果没有开启防火墙，则这一步可以跳过根据监听的端口，进行端口开放，每个用户会对应一个端口，第一个用户默认为5901端口，如果配置多个，则需要开发相应端口 [root@AccessGateway ~]# firewall-cmd --add-port=5901/tcp --permanent [root@AccessGateway ~]# firewall-cmd --reload Windows安装VNC，连接Centos远程桌面 到官方下载地址：https://www.realvnc.com/en/connect/download/viewer/选择对应版本下载客户端 新建连接中VNC Server输入IP地址:1，输入密码即可连接成功]]></content>
      <categories>
        <category>Linux</category>
        <category>远程连接</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>VNC Viewer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDP与CDH对比]]></title>
    <url>%2F2019%2F07%2F03%2FHDP%E4%B8%8ECDH%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[Hortonworks HDP与Cloudera CDH对比。 HDP与CDH的对比版本更新对比HDP版本更新较快，因为Hortonworks内部大部分员工都是apache代码贡献者，尤其是Hadoop 2.0的贡献者。 CDH版本更新比Apache版本慢。 目前Apache社区Hadoop最新版本：3.2.0 目前CDH最新版支持Hadoop版本：3.0.0 目前HDP最新版支持Hadoop版本：3.1.1 原装支持组件对比 必要组件 HDP CDH Zookeeper √ √ HDFS √ √ Yarn/MapReduce √ √ Hive √ √ HBase √ √ Phoenix √（最高支持5.0） ×（需要单独配置，最高支持版本1.3） Spark √ √ Zeppelin √ ×（需要自己编译安装） Oozie √ √ Sqoop1 √ √ Sqoop2 × √ Kafka √ √ Flink × × Kerberos √ √ 次要组件 HDP CDH Tez √ × Druid √ × Knox √ × Ranger √ × Storm √ × Ambari √ × Nifi √ × Cloudera Manager × √ HBase Indexer（方便在Solr中建立HBase索引） × √ Hue × √ Impala ×（需要单独安装） √ 其他组件 HDP CDH Solr √ √ Flume √（HDP3.0之后不再支持） √ Pig √ ×（CDH6.X不再支持） Avro × √ 安全权限模块对比 HDP包含Ranger组件，即使在没有Kerberos的情况下，也能作一些简单的权限分配管理。由于100%开源，所以支持Ldap+Kerberos+Ranger的权限配置方式，分配权限简单易用。另外，Kerberos配置具有向导式界面。 CDH不具有类似Ranger的组件，想要做权限只能加Kerberos认证，然而express免费版只支持集成Kerberos，需要Ldap支持的需要企业版，Kerberos向导页面也只有企业版才支持（CDH免费版和企业版区别对比）。 运维管理对比HDP采用Apache Ambari进行统一管理，Ambari2.7之后的版本相对2.6有很大的改动，2.6个人看来也不够人性化，2.7界面布局更加人性化。 Ambari不支持中文，整个管理页面都是英文呈现。 组件比较重要的基本配置都以图形化的方式呈现，比直接配文字版体验效果好。鼠标hover到配置项上面会有该项配置的说明。 其他配置都是按照节点（如下图中的NameNode）、配置文件（如下图中的Advanced hdfs-site）来进行组织的，方便运维人员快速定位。另外配置有版本记录，可以回退到任意版本。 组件界面可以直接看到该组件的哪些服务以及服务情况，右边就有该服务的快速链接，下图为Yarn的界面。 部分组件可以看到链接地址，比如Hive。 Ambari服务本身不支持高可用。 CDH采用Cloudera Manager（下文统一用cm代替）进行统一管理。 cm可以根据浏览器配置进行语言选择，支持中文。 配置界面左边将所有配置按照范围、类别、状态进行分类，也能很方便的找到配置。右边提供每个配置的说明，点看可以看到各项配置的说明。 配置版本控制免费版不支持参考官网：Viewing and Reverting Configuration Changes 组件服务的快速链接在tab页上 cm服务可以配置高可用参考官网：Installing and Configuring Cloudera Manager Server for High Availability cm支持数据加密，无论是静态加密或保护数据传输，但是可惜的是免费版cm支持很有限。另外加密前官方强烈建议安装Kerberos参考官网：Encryption Overview CDH版本说明CDH6.X组件版本对应https://www.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_packaging.html CDH5.X组件版本对应https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball.html Impala版本说明Impala在3.1之后才支持ORC格式HDFS文件，目前最高版本为3.2，CDH6.1对应的Impala版本为3.1，CDH6.2对应的Impala版本为3.2 Docker QuickStart版本说明Cloudera Quickstart和HDP的sandbox类似，都是单机版的供学习交流使用的大数据集群。目前Docker版启动的quickstart CDH版本最新为5.13.0，对应部分组件版本为： 组件 组件包版本 压缩包下载地址 版本发布说明 更改文件 Apache Hadoop hadoop-2.6.0+cdh5.13.0+2639 Tarball Release notes Changes Hadoop Mrv1 hadoop-0.20-mapreduce-2.6.0+cdh5.13.0+2639 (none) (none) (none) Hbase hbase-1.2.0+cdh5.13.0+411 Tarball Release notes Changes Apache Hive hive-1.1.0+cdh5.13.0+1269 Tarball Release notes Changes Hue hue-3.9.0+cdh5.13.0+7079 Tarball Release notes Changes Apache Impala impala-2.10.0+cdh5.13.0+0 (none) Release notes Changes Apache Kudu kudu-1.5.0+cdh5.13.0+0 (none) Release notes Changes Apache Oozie oozie-4.1.0+cdh5.13.0+458 Tarball Release notes Changes Cloudera Search search-1.0.0+cdh5.13.0+0 Tarball Release notes Changes Apache Solr solr-4.10.3+cdh5.13.0+519 Tarball Release notes Changes Apache Spark spark-1.6.0+cdh5.13.0+530 Tarball Release notes Changes Apache Sqoop sqoop-1.4.6+cdh5.13.0+116 Tarball Release notes Changes Apache Sqoop2 sqoop2-1.99.5+cdh5.13.0+46 Tarball Release notes Changes Zookeeper zookeeper-3.4.5+cdh5.13.0+118 Tarball Release notes Changes CDH免费版和企业版区别注：1.snmp traps：SNMP是指简单网络管理协议，trap是它规定的一种通信方式，用于被管理的设备主动向充当管理者的设备报告自己的异常信息。 官网参考地址截图来自CSDN CDH官方文档地址（基于6.2.x版本）安装教程：https://www.cloudera.com/documentation/enterprise/6/6.2/topics/installation.html Impala安装要求：https://www.cloudera.com/documentation/enterprise/6/6.2/topics/impala_prereqs.html 集群所使用端口：https://www.cloudera.com/documentation/enterprise/6/6.2/topics/cm_ig_ports.html 集群组件服务主机分配建议：https://www.cloudera.com/documentation/enterprise/6/6.2/topics/cm_ig_host_allocations.html 定制化安装（离线安装）：https://www.cloudera.com/documentation/enterprise/6/6.2/topics/cm_ig_custom_installation.html Cloudera Manager API：https://www.cloudera.com/documentation/enterprise/6/6.2/topics/cm_intro_api.html 基于裸金属部署参考文档：https://www.cloudera.com/documentation/other/reference-architecture/topics/ra_bare_metal_deployment.html Cloudera Manager常见问题（FAQ）https://www.cloudera.com/documentation/enterprise/6/6.2/topics/cm_faqs.html CHD各组件服务依赖项https://www.cloudera.com/documentation/enterprise/6/6.2/topics/cm_ig_service_dependencies.html]]></content>
      <categories>
        <category>大数据</category>
        <category>选型</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>CDH</tag>
        <tag>HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文档网站生成工具选型]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%96%87%E6%A1%A3%E7%BD%91%E7%AB%99%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7%E9%80%89%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[对比多款静态网站生成工具，分析需求，找到合适的工具。 需求 要求部署一个静态网站，用于开发者文档的呈现。 要求所有文档采用markdown格式书写，md文件保存到git仓库（github），并且目录结构必须清晰 要求md文件解析操作是在前端完成 方案选择 方式一 预先渲染HTML 方式二 运行时解析md 描述 先将所有md文件解析成HTML文件，然后前端进行HTML文件的展示参考：Hexo 前端直接读取md文件，在渲染页面时进行md文件的解析参考：docsify 优点 由于前端不需要渲染，直接展示，理论上速度没有延迟 由于每次都是即时渲染，所以在更新md文件时不会有额外工作量 缺点 在更新md文件，哪怕只更新一点，也需要所有md文件重新编译解析一次，工作量较大 需要前端实时解析md文件，在文件较大时可能会有延迟 选择 x √ ## 网站生成逻辑 ## 各种开源工具对比 开源工具对比 Hexo VuePress :–: :–: :–: 文档生成方式 预先渲染HTML 预先渲染HTML 对SEO友好程度 友好 友好 官网地址 hexo vuepress 适用场景 个人博客 需要SEO支持的技术文档 特点 与主题解耦，更换主题成本低 采用vue，对vue开发友好 ## Docute与Docsify区别 1. Docsify官方文档更友好，内容更多，本身占用空间更小 2. 同样的md文件，Docute解析代码段有问题，Docsify没问题 3. Docute提供一些官方组件，Badge、Note等，但是插件较少，而Docsify没有自带组件，但是支持很多有用的插件，如评论插件（Gitalk等）、全文搜索、谷歌统计等。而Docsify也支持Note插件：flexible-alerts 4. Docsify支持封面主页，Docute不支持 5. Docsify样式配置可以通过md文件进行配置，而Docute只能在index.html中配置 6. Docsify支持热部署，更新配置和文章不需要重启服务，Docute不支持 ## 最终选型 最终选型：Docsify 查看演示：https://docs.hming.org]]></content>
      <categories>
        <category>部署</category>
        <category>网站生成</category>
      </categories>
      <tags>
        <tag>文档</tag>
        <tag>网站生成工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SandBox HDFS上传文件失败问题]]></title>
    <url>%2F2019%2F04%2F16%2FSandBox-HDFS%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[SandBox上传文件到HDFS时，文件创建成功，向文件写入数据失败问题分析和解决方案。 问题描述远程（非Docker容器内、非宿主机）上传文件，新建文件成功，写入文件内容失败。查看/var/log/hadoop/hdfs/hadoop-hdfs-namenode-sandbox-hdp.hortonworks.com.log文件，发现如下错误： 2019-04-15 10:51:33,322 INFO ipc.Server (Server.java:logException(2726)) - IPC Server handler 74 on 8020, call Call#4 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.18.0.3:48170 java.io.IOException: File /tmp/1.csv could only be written to 0 of the 1 minReplication nodes. There are 1 datanode(s) running and 1 node(s) are excluded in this operation. at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2121) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:286) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2706) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)原因分析上传文件过程中，Client先向NameNode发送上传文件请求，NameNode将DataNode的地址返回给Client，Client再通过该地址，写入文件内容。由于SandBox HDP是搭建在Docker容器内部，所以NameNode返回的是Docker容器的ip地址（这和Docker的网络模式有关），因为SandBox默认启动的是自定义网络，所以容器内部ip为172.18.0.3，返回给Client之后，Client通过该IP是无法找到DataNode的，所以导致文件的元数据存到了NameNode上，而文件内容无法写入DataNode。 解决方案方案一（推荐）修改代码和增加端口映射因为NameNode返回的是Docker的ip，Client访问不了DataNode，所以可以让NameNode返回主机名，然后Client配置host的方式请求到宿主机的地址 修改Client host文件配置，增加host映射 10.75.4.32 sandbox-hdp.hortonworks.com java代码修改： Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://10.75.4.32:8020/"); //增加下面一行，设置返回DataNode的主机名而不是ip conf.set("dfs.client.use.datanode.hostname","true"); 此时还是不能访问到DataNode，因为sandbox-proxy容器并没有映射DataNode的端口（默认为50010）。 修改sandbox-proxy端口映射，增加50010端口停止、删除sandbox-proxy容器 [root@sandbox-host ~]# docker stop sandbox-proxy sandbox-proxy [root@sandbox-host ~]# docker rm sandbox-proxy sandbox-proxy 修改generate-proxy-deploy-script.sh脚本，在tcpPortsHDP=(...)部分新增50010端口映射VMware版本脚本路径：/sandbox/proxy/generate-proxy-deploy-script.sh纯Docker版脚本路径：./assets/generate-proxy-deploy-script.sh ... tcpPortsHDP=( ... [50010]=50010 ... ) 重新执行docker-deploy-hdp30.sh脚本中配置代理容器的脚本 #Deploy the proxy container. sed 's/sandbox-hdp-security/sandbox-hdp/g' assets/generate-proxy-deploy-script.sh > assets/generate-proxy-deploy-script.sh.new mv -f assets/generate-proxy-deploy-script.sh.new assets/generate-proxy-deploy-script.sh chmod +x assets/generate-proxy-deploy-script.sh assets/generate-proxy-deploy-script.sh 2>/dev/null 方案二（不推荐）暴力取消Docker网络隔离层，这样也就失去了Docker容器网络隔离的特性，具体利弊需要斟酌。因为原因是Client连不上DataNode节点，所以直接将Docker容器的网络模式设置成host模式（详细参照Docker网络模式），将容器的ip和端口直接和宿主机打通，这样就能远程连接DataNode了。 纯Docker模式 停止、删除已生成容器（sandbox-proxy和sandbox-hdp） [root@sandbox proxy]# docker stop $(docker ps -aq) ef35a5989c71 25f814082615 [root@sandbox proxy]# docker rm $(docker ps -aq) ef35a5989c71 25f814082615 修改脚本文件docker-deploy-hdp30.sh，将容器启动改为host网络模式，注释代理容器相关代码 脚本文件为运行sandbox时的启动脚本 #!/usr/bin/env sh #This script downloads HDP sandbox along with their proxy docker container set -x # CAN EDIT THESE VALUES registry="hortonworks" name="sandbox-hdp" version="3.0.1" proxyName="sandbox-proxy" proxyVersion="1.0" flavor="hdp" # NO EDITS BEYOND THIS LINE # housekeeping # 这里已经没用了，注释 #echo $flavor > sandbox-flavor # create necessary folders for nginx and copy over our rule generation script there #这里也注释，不需要代理容器了 #mkdir -p sandbox/proxy/conf.d #mkdir -p sandbox/proxy/conf.stream.d # pull and tag the sandbox and the proxy container # 本地已经存在镜像文件，这里可以注释减少脚本执行时间 #docker pull "$registry/$name:$version" #docker pull "$registry/$proxyName:$proxyVersion" # start the docker container and proxy if [ "$flavor" == "hdf" ]; then hostname="sandbox-hdf.hortonworks.com" elif [ "$flavor" == "hdp" ]; then hostname="sandbox-hdp.hortonworks.com" fi version=$(docker images | grep $registry/$name | awk '{print $2}'); # Create cda docker network # 因为采用host网络模式，这里创建网络cda也注释 #docker network create cda 2>/dev/null # Deploy the sandbox into the cda docker network # 将原本的run语句注释，修改为以下语句（将网络模式修改为host） #docker run --privileged --name $name -h $hostname --network=cda --network-alias=$hostname -d "$registry/$name:$version" docker run --privileged --name $name -h $hostname --network=host -d "$registry/$name:$version" echo " Remove existing postgres run files. Please wait" sleep 2 docker exec -t "$name" sh -c "rm -rf /var/run/postgresql/*; systemctl restart postgresql-9.6.service;" #Deploy the proxy container. # 这里为代理容器配置，因为host模式自动将所有端口映射到宿主机上，所以不再需要sandbox-proxy容器的支持 #sed 's/sandbox-hdp-security/sandbox-hdp/g' assets/generate-proxy-deploy-script.sh > assets/generate-proxy-deploy-script.sh.new #mv -f assets/generate-proxy-deploy-script.sh.new assets/generate-proxy-deploy-script.sh #chmod +x assets/generate-proxy-deploy-script.sh #assets/generate-proxy-deploy-script.sh 2>/dev/null #check to see if it's windows # 以下为window环境代码，也注释 #if uname | grep MINGW; then # sed -i -e 's/\( \/[a-z]\)/\U\1:/g' sandbox/proxy/proxy-deploy.sh #fi #chmod +x sandbox/proxy/proxy-deploy.sh 2>/dev/null #sandbox/proxy/proxy-deploy.sh 重新运行docker-deploy-hdp30.sh脚本文件 [root@sandbox opt]# sh docker-deploy-hdp30.sh + registry=hortonworks + name=sandbox-hdp + version=3.0.1 + proxyName=sandbox-proxy + proxyVersion=1.0 + flavor=hdp + echo hdp + mkdir -p sandbox/proxy/conf.d + mkdir -p sandbox/proxy/conf.stream.d + '[' hdp == hdf ']' + '[' hdp == hdp ']' + hostname=sandbox-hdp.hortonworks.com ++ docker images ++ grep hortonworks/sandbox-hdp ++ awk '{print $2}' + version=3.0.1 + docker run --privileged --name sandbox-hdp -h sandbox-hdp.hortonworks.com --network=host -d hortonworks/sandbox-hdp:3.0.1 b91b70d7792a806310c067e7792f4c3930a5329261128d5a4c211b804a923342 + echo ' Remove existing postgres run files. Please wait' Remove existing postgres run files. Please wait + sleep 2 + docker exec -t sandbox-hdp sh -c 'rm -rf /var/run/postgresql/*; systemctl restart postgresql-9.6.service;' # 可以看到此时只剩下sandbox-hdp一个容器在运行 [root@sandbox opt]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b91b70d7792a hortonworks/sandbox-hdp:3.0.1 "/usr/sbin/init" 8 minutes ago Up 8 minutes sandbox-hdp 最后进行各种初始化配置即可进入sandbox容器，重置Ambari admin密码 [root@sandbox sandbox]# docker exec -it sandbox-hdp /bin/bash [root@sandbox-hdp /]# ambari-admin-password-reset Please set the password for admin: Please retype the password for admin: The admin password has been set. Restarting ambari-server to make the password change effective... Using python /usr/bin/python Restarting ambari-server Ambari Server is not running Ambari Server running with administrator privileges. Organizing resource files at /var/lib/ambari-server/resources... Ambari database consistency check started... Server PID at: /var/run/ambari-server/ambari-server.pid Server out at: /var/log/ambari-server/ambari-server.out Server log at: /var/log/ambari-server/ambari-server.log Waiting for server start............................ Server started listening on 8080 DB configs consistency check: no errors and warnings were found. 访问8080端口，到Ambari界面，登录后重启服务即可。 SandBox使用参照：SandBox-HDP使用详解注意：由于没有运行sandbox-proxy容器，1080端口已经无法访问 VMware模式 通过ssh连接22端口登录到VMware虚拟机（sandbox的宿主机）里面也可以通过XShell等工具进入默认root初始密码为hadoop [root@localhost ~]# ssh 10.75.4.6 -p 22 root@10.75.4.6's password: Last failed login: Tue Apr 16 15:38:03 UTC 2019 from 10.75.4.32 on ssh:notty There was 1 failed login attempt since the last successful login. Last login: Mon Apr 15 16:34:49 2019 from 10.75.4.11 [root@sandbox-host ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE hortonworks/sandbox-proxy 1.0 ca272ae0e63a 4 months ago 109MB hortonworks/sandbox-hdp-security 3.0 ae1d1779b081 4 months ago 27.5GB 停止、删除已生成容器（sandbox-proxy和sandbox-hdp-security） [root@sandbox-host proxy]# docker stop $(docker ps -aq) ef35a5989c71 25f814082615 [root@sandbox-host proxy]# docker rm $(docker ps -aq) ef35a5989c71 25f814082615 修改脚本文件/sandbox/sandbox-deploy.sh，将容器启动改为host网络模式，注释代理容器相关代码 #!/usr/bin/env bash flavor=$(cat /sandbox-flavor) if [ "$flavor" == "hdf" ]; then name="sandbox-hdf-standalone-cda-ready" hostname="sandbox-hdf.hortonworks.com" elif [ "$flavor" == "hdp" ]; then name="sandbox-hdp-security" hostname="sandbox-hdp.hortonworks.com" fi version=$(docker images | grep $name | awk '{print $2}'); image="hortonworks/$name:$version"; # Create cda docker network # 因为采用host网络模式，这里注释创建网络cda #docker network create cda # Deploy the sandbox into the cda docker network # 将原本的run语句注释，修改为以下语句（将网络模式修改为host） #docker run --privileged --name $name -h $hostname --network=cda --network-alias=$hostname -d $image docker run --privileged --name $name -h $hostname --network=host -d $image # Deploy the proxy container. This script was generated by running # 这里为代理容器配置，因为host模式自动将所有端口映射到宿主机上，所以不再需要sandbox-proxy容器的支持 #/sandbox/proxy/generate-proxy-deploy-script.sh #/sandbox/proxy/proxy-deploy.sh 运行脚本文件/sandbox/sandbox-deploy.sh，重新生成容器 [root@sandbox-host sandbox]# sh sandbox-deploy.sh 12e3df82d057057c6af78eea1c8bd9eb9156ebe0bac3dc90d2fec8377f48aa6f # 可以看到此时只剩下sandbox-hdp-security一个容器在运行 [root@sandbox-host sandbox]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 12e3df82d057 hortonworks/sandbox-hdp-security:3.0 "/usr/sbin/init" 6 seconds ago Up 3 seconds sandbox-hdp-security 最后进行各种初始化配置即可进入sandbox容器，重置Ambari admin密码 [root@sandbox-host sandbox]# docker exec -it sandbox-hdp-security /bin/bash [root@sandbox-hdp /]# ambari-admin-password-reset Please set the password for admin: Please retype the password for admin: The admin password has been set. Restarting ambari-server to make the password change effective... Using python /usr/bin/python Restarting ambari-server Ambari Server is not running Ambari Server running with administrator privileges. Organizing resource files at /var/lib/ambari-server/resources... Ambari database consistency check started... Server PID at: /var/run/ambari-server/ambari-server.pid Server out at: /var/log/ambari-server/ambari-server.out Server log at: /var/log/ambari-server/ambari-server.log Waiting for server start............................ Server started listening on 8080 DB configs consistency check: no errors and warnings were found. 访问8080端口，到Ambari界面，登录后重启服务即可。 SandBox使用参照：SandBox-HDP使用详解注意：由于没有运行sandbox-proxy容器，1080端口已经无法访问]]></content>
      <categories>
        <category>大数据</category>
        <category>SandBox</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDP</tag>
        <tag>SandBox</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SandBox-HDP使用详解]]></title>
    <url>%2F2019%2F04%2F04%2FSandBox-HDP%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[SandBox HDP版本3.0.1 官方文档介绍得非常详细，本文提取一些关键点作介绍，参考官网教程：Sandbox DocsSandbox Port ForwardsSandbox ArchitectureLearning the Ropes of the HDP Sandbox SandBox是什么 The Sandbox is a straightforward, pre-configured, learning environment that contains the latest developments from Apache Hadoop, specifically the Hortonworks Data Platform (HDP). The Sandbox comes packaged in a virtual environment that can run in the cloud or on your personal machine. The Sandbox allows you to learn and explore HDP on your own. SandBox是Hortonworks提供的单机版HDP或HDF环境，主要用于测试和学习使用，对于没有服务器集群又想使用HDP/HDF的情况，SandBox是不二之选。另外，SandBox里面内置了DAS（Data Analytics Studio），非SandBox版本是没有这个的，需要购买Hortonworks服务才的获取到安装包。SandBox提供三种安装方式：VirtualBox虚拟机、VMware虚拟机、Docker容器。本文主要针对讲SandBox-HDP，HDF安装使用和HDP大致相同。安装教程参考：安装SandBox HDP（Docker版）安装SandBox HDP（VMware版） 环境准备一个运行中的SandBox-HDP 3.0.1 web访问1080端口为sandbox容器web服务端口，可以通过浏览器访问该端口，得到以下界面：左侧launch dashboard直接进入ambari管理界面，登录admin账号需要进入容器修改ambari管理员密码右侧则是一些链接，包括ambari管理地址、Ranger地址、DAS地址等 4200端口则提供了一个浏览器访问命令行的接口：使用root登录，默认密码为hadoop，第一次登录会提示修改root密码，对密码强度会有要求 登录到HDP环境主机在运行docker的主机上可以通过2222端口登录到HDP docker主机中，也可以通过docker exec命令进入在其他机器上想登录到HDP主机就只能通过SSH了 # SSH登录需要输入密码，root初始密码为hadoop [root@sandbox opt]# ssh 127.0.0.1 -p 2222 root@127.0.0.1`s password: Last login: Thu Apr 4 08:22:27 2019 from 172.18.0.3 [root@sandbox-hdp ~]# # docker命令可以直接进入 [root@sandbox opt]# docker exec -it sandbox-hdp /bin/bash [root@sandbox-hdp /]# 登录到Ambari界面默认提供的账户，更多账号信息参考官网 用户 密码 admin 参考重置管理员密码 maria_dev maria_dev raj_ops raj_ops holger_gov holger_gov amy_ds amy_ds 重置Ambari管理员密码 以root用户登录到HDP主机[root@sandbox opt]# ssh 127.0.0.1 -p 2222 root@127.0.0.1`s password: Last login: Thu Apr 4 08:22:27 2019 from 172.18.0.3 [root@sandbox-hdp ~]# 运行ambari-admin-password-reset命令，根据提示修改密码[root@sandbox-hdp /]# ambari-admin-password-reset Please set the password for admin: Please retype the password for admin: The admin password has been set.Restarting ambari-server to make the password change effective… Using python /usr/bin/pythonRestarting ambari-serverWaiting for server stop…Ambari Server stoppedAmbari Server running with administrator privileges.Organizing resource files at /var/lib/ambari-server/resources…Ambari database consistency check started…Server PID at: /var/run/ambari-server/ambari-server.pidServer out at: /var/log/ambari-server/ambari-server.outServer log at: /var/log/ambari-server/ambari-server.logWaiting for server start……………………………………………………………………………………….DB configs consistency check: no errors and warnings were found.ERROR: Exiting with exit code 1.REASON: Server not yet listening on http port 8080 after 90 seconds. Exiting. &gt; 可能会遇到报错 `ERROR: Exiting with exit code 1. REASON: Server not yet listening on http port 8080 after 90 seconds. Exiting.` 这是由于SandBox中所有服务都在一个节点上，启动Ambari比较慢，超过了90秒，实际上这个错不会有任何影响 可以通过编辑`/etc/ambari-server/conf/ambari.properties`文件，添加一行`server.startup.web.timeout = 150`来增加超时时间的方法解决 3. 执行命令后Ambari服务会重启，然后就可以通过新的admin密码登录Ambari ### 新增host映射 Ambari中有些内部链接是通过`sandbox-hdp.hortonworks.com`域名去访问的，比如`HDFS NameNode UI` 可以在需要访问的主机上增加host映射方便访问 ### 数据库初始密码 #### PostgreSQL 查看ambari用户的密码，默认为`bigdata` ```bash [root@sandbox-hdp ~]# grep &quot;password&quot; /etc/ambari-server/conf/ambari.properties server.jdbc.rca.user.passwd=/etc/ambari-server/conf/password.dat server.jdbc.user.passwd=/etc/ambari-server/conf/password.dat [root@sandbox-hdp ~]# cat /etc/ambari-server/conf/password.dat bigdata使用ambari用户登录postgreSQL [root@sandbox-hdp ~]# psql -U ambari -W Password for user ambari: psql (9.6.11) Type "help" for help. ambari=> MySQL内置MySQL使用的是Hive新建的MySQL，初始密码为hortonworks1登录ambari postgreSQL查找密码 [root@sandbox-hdp ~]# psql -U ambari -W Password for user ambari: psql (9.6.11) Type "help" for help. ambari=> select version,config_id,type_name,config_data from clusterconfig where type_name='hive-site'; 在结果里查找内容：javax.jdo.option.ConnectionPassword 常见错误远程向HDFS上传文件失败问题参照SandBox HDFS上传文件失败问题]]></content>
      <categories>
        <category>大数据</category>
        <category>SandBox</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDP</tag>
        <tag>SandBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装SandBox HDP（VMware版）]]></title>
    <url>%2F2019%2F04%2F02%2F%E5%AE%89%E8%A3%85SandBox-HDP%EF%BC%88VMware%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[SandBox HDP版本3.0.1，安装环境为Windows 10 参考官网教程：Deploying Hortonworks Sandbox on VMWare 环境准备 方面 要求 软件 安装VMware 内存 推荐16G以上（会开一个内存为10G的虚拟机） ### 运行原理 VMware启动了一个Linux虚拟机，在Linux虚拟机里面会启动两个docker容器 ```bash [root@sandbox-host ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE hortonworks/sandbox-proxy 1.0 ca272ae0e63a 4 months ago 109MB hortonworks/sandbox-hdp-security 3.0 ae1d1779b081 4 months ago 27.5GB ``` sandbox-proxy容器负责代理转发一些端口 sandbox-hdp-security容器则是HDP环境 所以，要对HDP环境进行修改，比如修改ambari管理员密码，就需要进入docker容器里面 可以通过ssh登录2222端口，也可以通过docker exec命令进入，docker相关命令参考Docker替换镜像源与常用命令 ### 下载镜像文件 到官网下载ova格式的镜像文件（HDP_3.0.1_vmware_181205.ova 20.5G） 下载可能需要注册，随便填就行了 ### 将.ova镜像导入到VMware中 1. 打开VMware，点击文件-&gt;打开，导入刚才下载的文件：HDP_3.0.1_vmware_181205.ova 2. 开启虚拟机（这里可以看到虚拟机的一些信息，包括内存磁盘等） 3. 开启后可能会遇到无法连接网络的问题，这时候选择桥接模式，重启一下就行 ### 如何使用 成功运行后窗口会打印一些信息，可以通过这些信息连接到虚拟机 上图中的1080端口为sandbox端口，可以通过浏览器访问该端口，得到以下界面： 左侧launch dashboard直接进入ambari管理界面，右侧则是一些链接，包括ambari管理地址、Ranger地址、DAS地址等 4200端口则提供了一个浏览器访问命令行的接口： 使用root登录，默认密码为hadoop，第一次登录会提示修改root密码，对密码强度会有要求 本文到此为止，更详细的使用教程，请参照SandBox-HDP使用详解]]></content>
      <categories>
        <category>大数据</category>
        <category>SandBox</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDP</tag>
        <tag>SandBox</tag>
        <tag>虚拟机</tag>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装SandBox HDP（Docker版）]]></title>
    <url>%2F2019%2F04%2F02%2F%E5%AE%89%E8%A3%85SandBox-HDP%EF%BC%88Docker%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[SandBox HDP版本3.0.1，安装环境为CentOS7 参考官网教程：Deploying Hortonworks Sandbox on Docker 环境准备 方面 要求 软件 安装docker 内存 官方推荐docker容器至少10G 磁盘 docker镜像目录至少30G（sandbox镜像27.5G） ### 下载脚本文件 到官网下载zip格式的shell脚本文件 下载可能需要注册，随便填就行了。压缩包内容如下： 需要将脚本上传到Linux并解压 ```bash [root@sandbox opt]# ls assets docker-deploy-hdp30.sh ``` ### 执行脚本 #### 1. 在Linux中执行docker-deploy-hdp30.sh脚本，拉取镜像，运行容器 ```bash [root@sandbox opt]# sh docker-deploy-hdp30.sh + registry=hortonworks + name=sandbox-hdp + version=3.0.1 + proxyName=sandbox-proxy + proxyVersion=1.0 + flavor=hdp + echo hdp + mkdir -p sandbox/proxy/conf.d + mkdir -p sandbox/proxy/conf.stream.d + docker pull hortonworks/sandbox-hdp:3.0.1 Trying to pull repository docker.io/hortonworks/sandbox-hdp … 3.0.1: Pulling from docker.io/hortonworks/sandbox-hdp 70799bbf2226: Pull complete 40963917cdad: Pull complete 3fe9adbb8d7e: Pull complete ee3ec4e8cb3d: Pull complete 7ea5917732c0: Pull complete 2d951411620c: Pull complete f4c5e354e7fd: Pull complete 22ffa6ef360f: Pull complete 2060aa0f3751: Pull complete ca01ba34744d: Pull complete 83326dded077: Pull complete eb3d71b90b73: Pull complete bdd1cab41c81: Pull complete 500cc770c4bd: Pull complete 0cb1decd5474: Pull complete b9591f4b6855: Pull complete f28e56086127: Pull complete e7de4e7d0bca: Pull complete ec77967d2166: Pull complete 4fdcae170114: Pull complete 6347f5df8ffc: Pull complete 6a6ecc232709: Pull complete ea845898ff50: Pull complete 02135573b1bf: Pull complete cb0176867cd8: Pull complete 3c08321268fd: Pull complete 82e82a97c465: Pull complete 8aaaa48ed101: Pull complete 74b321ac2ac5: Pull complete 569da02c0a66: Pull complete af40820407ef: Pull complete Digest: sha256:7b767af7b42030fb1dd0f672b801199241e6bef1258e3ce57361edb779d95921 Status: Downloaded newer image for docker.io/hortonworks/sandbox-hdp:3.0.1 + docker pull hortonworks/sandbox-proxy:1.0 Trying to pull repository docker.io/hortonworks/sandbox-proxy … 1.0: Pulling from docker.io/hortonworks/sandbox-proxy 951bdea65c93: Pull complete 4b9047c5fbbb: Pull complete 773156407aae: Pull complete d8524176841d: Pull complete Digest: sha256:42e4cfbcbb76af07e5d8f47a183a0d4105e65a1e7ef39fe37ab746e8b2523e9e Status: Downloaded newer image for docker.io/hortonworks/sandbox-proxy:1.0 + ‘[‘ hdp == hdf ‘]’ + ‘[‘ hdp == hdp ‘]’ + hostname=sandbox-hdp.hortonworks.com ++ docker images ++ awk ‘{print $2}’ ++ grep hortonworks/sandbox-hdp + version=3.0.1 + docker network create cda 7f641a6c16cf73df1079f241e76a318f3094f4303feaeae1c0a50c1b58c9d1ee + docker run –privileged –name sandbox-hdp -h sandbox-hdp.hortonworks.com –network=cda –network-alias=sandbox-hdp.hortonworks.com -d hortonworks/sandbox-hdp:3.0.1 59cb51cd71faa11218a12ee3f8c8ea1e58790025428a4573e476c1ddd118c202 + echo ‘ Remove existing postgres run files. Please wait’ Remove existing postgres run files. Please wait + sleep 2 + docker exec -t sandbox-hdp sh -c ‘rm -rf /var/run/postgresql/*; systemctl restart postgresql-9.6.service;’ + sed s/sandbox-hdp-security/sandbox-hdp/g assets/generate-proxy-deploy-script.sh + mv -f assets/generate-proxy-deploy-script.sh.new assets/generate-proxy-deploy-script.sh + chmod +x assets/generate-proxy-deploy-script.sh + assets/generate-proxy-deploy-script.sh + grep MINGW + uname + chmod +x sandbox/proxy/proxy-deploy.sh + sandbox/proxy/proxy-deploy.sh c1f52cfec560982477e4b6c69f4cc95309bd93907196761ed5eff7222744743e ``` 注意：镜像文件特别大，国内拉取非常慢，可通过代理等方式拉取。 2. 使用docker ps查看生成的容器可以看到有两个容器生成正在运行sandbox-proxy容器负责将HDP中的各个端口映射到主机上sandbox-hdp则是HDP主要环境的容器，所有的hdp组件都是在这个容器里面运行 3. 执行完脚本，相应的目录下会生成一个文件sandbox-flavor和一个文件夹sandbox[root@centos4 opt]# ls assets docker-deploy-hdp30.sh sandbox sandbox-flavor 4. 脚本文件只需要执行一次，如果需要停止或重启HDP环境，只需要停止/重启相应的docker容器停止HDP集群 docker stop sandbox-hdp docker stop sandbox-proxy 启动HDP集群 docker start sandbox-hdp docker start sandbox-proxy 删除HDP容器 docker stop sandbox-hdp docker stop sandbox-proxy docker rm sandbox-hdp docker rm sandbox-proxy 移除sandbox镜像 docker rmi hortonworks/sandbox-hdp:{release} 如何使用上图中的1080端口为sandbox端口，可以通过浏览器访问该端口，得到以下界面：左侧launch dashboard直接进入ambari管理界面，右侧则是一些链接，包括ambari管理地址、Ranger地址、DAS地址等4200端口则提供了一个浏览器访问命令行的接口：使用root登录，默认密码为hadoop，第一次登录会提示修改root密码，对密码强度会有要求 本文到此为止，更详细的使用教程，请参照SandBox-HDP使用详解 可能遇到的问题[root@centos4 opt]# docker logs sandbox-proxy 2019/04/04 05:53:28 [emerg] 1#1: host not found in upstream "sandbox-hdp" in /etc/nginx/conf.d/http-hdp.conf:9 nginx: [emerg] host not found in upstream "sandbox-hdp" in /etc/nginx/conf.d/http-hdp.conf:9 这种情况是因为docker网络没有配置好，导致proxy容器无法使用nginx代理hdp容器检查docker网络配置]]></content>
      <categories>
        <category>大数据</category>
        <category>SandBox</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Docker</tag>
        <tag>HDP</tag>
        <tag>SandBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux制作离线yum源]]></title>
    <url>%2F2019%2F03%2F29%2FLinux%E5%88%B6%E4%BD%9C%E7%A6%BB%E7%BA%BFyum%E6%BA%90%2F</url>
    <content type="text"><![CDATA[制作离线yum源的方法 参考 基本步骤 制作或挂载一个本地yum源目录 修改或增加repo配置文件指向离线yum包制作利用官方包下载ISO文件到官网http://isoredirect.centos.org/下载镜像包，尽量下载Everything ISO版本，这里面的包最全，另外一个“DVD ISO”是通用版，里面的包并不全，还有一个“Minimal ISO”是Centos最小安装版（相当于是windows的纯净系统）。下载文件名如：CentOS-7-x86_64-Everything-1810.iso挂载ISO到目录 自己制作包查看rpm包依赖与下载依赖包 查看.rpm 包依赖：rpm -qpR [package] 通过yum install --downloadonly --downloaddir=[download_dir] [package] 来只下载所有依赖包不安装包(前提是当前环境没有安装该包) 在CentOS/RHEL 6或更早期的版本中，你需要安装一个单独yum插件(名称为yum-plugin-downloadonly)才能使用--downloadonly命令选项： yum install -y yum-plugin-downloadonly #如果没有该插件，你会在使用yum时得到以下错误：Command line error: no such option: –downloadonly``` 运行yum list [package] --showduplicates 来查看包的多个版本 利用rpm包制作yum包 安装createrepo工具createrepo命令用来制作yum包，没有安装该软件可以通过下载createrepo的rpm包，通过rpm命令进行安装。 将所有的rpm包放到一个目录下 到rpm包的目录执行createrepo .命令 [root@hadoop001 yum-repo]# createrepo . 之后会生成一个repodata的目录,该目录就成了一个yum源 离线yum源配置]]></content>
      <categories>
        <category>Linux</category>
        <category>yum源</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>yum</tag>
        <tag>离线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux修改命令终端提示符]]></title>
    <url>%2F2019%2F03%2F29%2FLinux%E4%BF%AE%E6%94%B9%E5%91%BD%E4%BB%A4%E7%BB%88%E7%AB%AF%E6%8F%90%E7%A4%BA%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[Linux命令行是操作Linux的重要手段，命令行提示符千篇一律的格式有时会让命令和输出难以辨认，本文将介绍如何修改命令行提示符的格式。 命令行提示符代表含义命令行提示符一般格式含义：[root@sandbox ~]# 其中@前root表示当前用户，@后sandbox表示当前主机名，~表示当前目录为家目录 Linux命令行结尾的提示符有#和$两种不同的符号，如下所示： [root@sandbox ~]# #&lt;==这是超级管理员root用户对应的命令行。 [liming@sandbox ~]$ #&lt;==这是普通用户liming对应的命令行。 修改命令行提示符格式Linux命令提示符由PS1环境变量控制，可以通过全局配置文件/etc/bashrc或/etc/profile中进行按需配置和调整。查看当前PS1设置： [root@sandbox /]# set|grep PS1 PS1='[\u@\h \W]\$ ' PS1变量最终使用： export PS1='[\[\e[32;1m\]\u@\h \W\[\e[0m\]]\$ ' 参考：1]]></content>
      <categories>
        <category>Linux</category>
        <category>命令行</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>命令行提示符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Alpine容器介绍]]></title>
    <url>%2F2019%2F03%2F06%2FDocker-Alpine%E5%AE%B9%E5%99%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[最小化容器Alpine介绍 http://www.voidcn.com/article/p-ulbbnkky-brh.htmlhttps://blog.phpgao.com/docker_alpine.htmlhttps://marshal.ohtly.com/2016/12/21/docker-container-and-alpine-bash-not-found/]]></content>
      <categories>
        <category>容器</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>Docker</tag>
        <tag>Alpine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7离线安装HDF]]></title>
    <url>%2F2019%2F02%2F14%2FCentOS7%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85HDF%2F</url>
    <content type="text"><![CDATA[CentOS7离线安装HDF，Ambari版本：2.7.3.0，HDF版本：3.3.1.0 本文环境已存在Ambari和HDP环境，Ambari搭建参照CentOS7离线安装HDP 下载离线包HDF仓库地址找到对应操作系统的包，下载HDF Management Pack与HDF RPM tarball两个包即可。（本文为CentOS7的包） 注意：HDF RPM tarball包大小3.6G左右，HDF Management Pack包96M左右，请确保保存路径有足够空间 [root@master ambari]# ll -h total 3.7G drwxr-xr-x 3 root root 4.0K Jan 16 15:00 ambari -rw-r--r-- 1 root root 3.6G Dec 15 02:36 HDF-3.3.1.0-centos7-rpm.tar.gz -rw-r--r-- 1 root root 96M Dec 15 02:13 hdf-ambari-mpack-3.3.1.0-10.tar.gz drwxr-xr-x 3 ambari-qa users 4.0K Dec 11 11:49 HDP drwxr-xr-x 3 ambari-qa users 4.0K Aug 13 2018 HDP-UTILS 其中ambari、HDP、HDP-UTILS为CentOS7离线安装HDP中制作的yum本地源地址 制作HDF yum镜像源参考制作本地源，将HDF-3.3.1.0-centos7-rpm.tar.gz包解压，制作yum本地源。 解压到httpd服务路径(本文httpd服务路径为/cloud/ambari)```bash[root@master ambari]# pwd/cloud/ambari[root@master ambari]# tar -zxvf HDF-3.3.1.0-centos7-rpm.tar.gz[root@node1 ambari]# ll -htotal 3.7Gdrwxr-xr-x 3 root root 4.0K Jan 16 15:00 ambaridrwxr-xr-x 3 ambari-qa users 4.0K Dec 15 02:19 HDF -rw-r–r– 1 root root 3.6G Dec 15 02:36 HDF-3.3.1.0-centos7-rpm.tar.gz-rw-r–r– 1 root root 96M Dec 15 02:13 hdf-ambari-mpack-3.3.1.0-10.tar.gzdrwxr-xr-x 3 ambari-qa users 4.0K Dec 11 11:49 HDPdrwxr-xr-x 3 ambari-qa users 4.0K Aug 13 2018 HDP-UTILS 2. 修改`./HDF/centos7/3.3.1.0-10/hdf.repo`文件为以下内容 ```bash #VERSION_NUMBER=3.3.1.0-10 [HDF-3.3.1.0] name=HDF Version - HDF-3.3.1.0 baseurl=http://192.168.0.151:88/ambari/HDF/centos7/3.3.1.0-10 gpgcheck=1 gpgkey=http://192.168.0.151:88/ambari/HDF/centos7/3.3.1.0-10/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 [HDP-UTILS-1.1.0.22] name=HDP-UTILS Version - HDP-UTILS-1.1.0.22 baseurl=http://192.168.0.151:88/ambari/HDP-UTILS/centos7/1.1.0.22 gpgcheck=1 gpgkey=http://192.168.0.151:88/ambari/HDP-UTILS/centos7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 1)其中192.168.0.151:88为httpd的路径和端口，需根据实际情况修改。2)HDP-UTILS如果在HDP中已经配置过，则这里可以删除。 拷贝hdf.repo文件到/etc/yum.repos.d/目录下，进行yum更新 [root@master ambari]# cp hdf.repo /etc/yum.repos.d/ [root@master ambari]# yum clean all [root@master ambari]# yum makecache [root@master ambari]# yum repolist 如果yum报错，则可能是hdf源没有配置成功，或者hdf.repo文件有误，更正后重试即可。 将hdf.repo拷贝到其他节点，然后每个节点进行yum更新 安装HDF Management Pack此处参考官方文档 使用ambari-server install-mpack命令安装Management Pack[root@master ambari]# ambari-server install-mpack --mpack=./hdf-ambari-mpack-3.3.1.0-10.tar.gz --verbose Using python /usr/bin/python Installing management pack INFO: Loading properties from /etc/ambari-server/conf/ambari.properties INFO: Installing management pack ./hdf-ambari-mpack-3.3.1.0-10.tar.gz INFO: Loading properties from /etc/ambari-server/conf/ambari.properties INFO: Download management pack to temp location /var/lib/ambari-server/data/tmp/hdf-ambari-mpack-3.3.1.0-10.tar.gz INFO: Loading properties from /etc/ambari-server/conf/ambari.properties ... INFO: Loading properties from /etc/ambari-server/conf/ambari.properties INFO: Successfully switched addon services using config file /var/lib/ambari-server/resources/mpacks/hdf-ambari-mpack-3.3.1.0-10/hooks/HDF-3.3.json INFO: Loading properties from /etc/ambari-server/conf/ambari.propertiesAmbari Server ‘install-mpack’ completed successfully. 2. 使用`ambari-server restart`命令重启ambari服务 ```bash [root@master ambari]# ambari-server restart Using python /usr/bin/python Restarting ambari-server Waiting for server stop... Ambari Server stopped Ambari Server running with administrator privileges. Organizing resource files at /var/lib/ambari-server/resources... Ambari database consistency check started... Server PID at: /var/run/ambari-server/ambari-server.pid Server out at: /var/log/ambari-server/ambari-server.out Server log at: /var/log/ambari-server/ambari-server.log Waiting for server start....................... Server started listening on 8080 DB configs consistency check: no errors and warnings were found.更新ambari服务中HDF源地址参考官方文档 浏览器进入Ambari服务地址（默认端口为8080） 在右上角admin下拉框中选择Manage Ambari 选择左边栏的Versions，点击HDP版本链接 此时会发现Repositories中多出HDF-3.3一栏，填入之前制作的本地源地址即可本文地址为：http://192.168.0.151:88/ambari/HDF/centos7/3.3.1.0-10 点击save保存 返回主界面，添加service时就会发现多了NiFi等HDF支持的组件]]></content>
      <categories>
        <category>大数据</category>
        <category>HDF</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>离线安装</tag>
        <tag>HDF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven使用Hortonworks依赖包]]></title>
    <url>%2F2019%2F01%2F12%2FMaven%E4%BD%BF%E7%94%A8Hortonworks%E4%BE%9D%E8%B5%96%E5%8C%85%2F</url>
    <content type="text"><![CDATA[在项目中连接HDP时，会出现HortonWorks的Maven依赖包下载不了的情况，本文提供解决方案。 参考链接：where can i find HDP maven Repos在项目中连接HDP时，会出现HortonWorks的Maven依赖包下载不了的情况，只需要在pom.xml中添加如下代码： &lt;repositories> &lt;repository> &lt;releases> &lt;enabled>true&lt;/enabled> &lt;/releases> &lt;snapshots> &lt;enabled>true&lt;/enabled> &lt;/snapshots> &lt;id>hortonworks.extrepo&lt;/id> &lt;name>Hortonworks HDP&lt;/name> &lt;url>http://repo.hortonworks.com/content/repositories/releases&lt;/url> &lt;/repository> &lt;repository> &lt;releases> &lt;enabled>true&lt;/enabled> &lt;/releases> &lt;snapshots> &lt;enabled>true&lt;/enabled> &lt;/snapshots> &lt;id>hortonworks.other&lt;/id> &lt;name>Hortonworks Other Dependencies&lt;/name> &lt;url>http://repo.hortonworks.com/content/groups/public&lt;/url> &lt;/repository> &lt;/repositories> 另外，在mvnrepository官网最新版可能没有更新，可以去HortonWorks依赖包官网查看最新版本]]></content>
      <categories>
        <category>大数据</category>
        <category>HDP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Maven</tag>
        <tag>HDP</tag>
        <tag>Hortonworks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7离线安装HDP]]></title>
    <url>%2F2019%2F01%2F09%2FCentOS7%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85HDP%2F</url>
    <content type="text"><![CDATA[CentOS7离线安装HDP，Ambari版本：2.7.3.0，HDP版本：3.1.0.0 本文环境 节点 IP地址 hdp001 192.168.171.10 hdp002 192.168.171.11 hdp003 192.168.171.12 ### 环境准备 #### 磁盘准备 离线安装包共计10G左右，解压后共计11G左右，请保证有足够空间。 #### 配置免密登录 配置免密码登录教程请点击这里 #### 关闭防火墙 查看防火墙状态 firewall-cmd --state或systemctl status firewalld 临时关闭防火墙 systemctl stop firewalld 禁止开机启动 systemctl disable firewalld 设置SELinux模式不关闭可能导致Apache http服务无法访问。 查看SELinux状态：getenforce如果是Permissive或者Disabled则可以继续安装，如果显示enforcing，则需要进行以下步骤修改模式 编辑/etc/selinux/config文件 修改SELINUX=enforcing行内容为SELINUX=permissive或者SELINUX=disabled 重启系统或者运行setenforce 0命令禁用SELinux 安装jdk、Python（所有节点）、MySQL（安装一个即可） 配置java环境教程点击这里 安装/更新Python yum -y install python 离线安装MySQL教程点击这里新建数据库hive、ambari（为后续安装做准备）。mysql> create database hive; Query OK, 1 row affected (0.00 sec) mysql&gt; create database ambari;Query OK, 1 row affected (0.00 sec) #### 下载离线包（包含HDP、ambari、HDP-UTILS、HDP-GPL（非必须）） [Ambari-2.7.3.0下载地址](https://docs.hortonworks.com/HDPDocuments/Ambari-2.7.3.0/bk_ambari-installation/content/ambari_repositories.html) [HDP-3.1.0.0相关包下载地址](https://docs.hortonworks.com/HDPDocuments/Ambari-2.7.3.0/bk_ambari-installation/content/hdp_31_repositories.html) &gt; 注意：ambari包大小`1.81G`左右，HDP包`8.44G`左右，HDP-UTILS包`86.4M`左右，请确保保存路径有足够空间 #### 安装httpd服务（Apache服务，ambari-server节点安装即可） &gt; 注意：selinux未关闭可能导致Apache服务地址403。 ```bash [root@hdp001 ~]# yum -y install httpd [root@hdp001 ~]# service httpd restart Redirecting to /bin/systemctl restart httpd.service访问服务器80端口，查看httpd服务是否开启。 注意：配置信息如端口、映射路径可以通过编辑/etc/httpd/conf/httpd.conf文件进行修改 将压缩包解压到/var/www/html/下 注意：1.如果httpd映射路径修改过，则以修改后的为准。2.解压后一共约11G左右大小，请确保有足够空间。 [root@hdp001 ambari]# pwd /var/www/html/ambari [root@hdp001 html]# du -h -d 1 11G ./ambari 11G . [root@hdp001 ambari]# ls ambari-2.7.3.0-centos7.tar.gz HDP-3.1.0.0-centos7-rpm.tar.gz HDP-UTILS-1.1.0.22-centos7.tar.gz # 解压... [root@hdp001 ambari]# ls ambari ambari-2.7.3.0-centos7.tar.gz HDP HDP-3.1.0.0-centos7-rpm.tar.gz HDP-UTILS HDP-UTILS-1.1.0.22-centos7.tar.gz 访问服务器80端口相应/ambari/地址，可以访问到文件和文件夹即可 制作本地源 修改repo源文件 [root@hdp001 ambari]# vim ambari/centos7/2.7.3.0-139/ambari.repo 修改baseurl与gpgkey值为Apache httpd服务能访问到的地址，如下： #VERSION_NUMBER=2.7.3.0-139 [ambari-2.7.3.0] #json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json name=ambari Version - ambari-2.7.3.0 baseurl=http://hdp001:80/ambari/ambari/centos7/2.7.3.0-139 gpgcheck=1 gpgkey=http://hdp001:80/ambari/ambari/centos7/2.7.3.0-139/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins priority=1 HDP源修改方式同上 [root@hdp001 ambari]# vim HDP/centos7/3.1.0.0-78/hdp.repo #VERSION_NUMBER=3.1.0.0-78 [HDP-3.1.0.0] name=HDP Version - HDP-3.1.0.0 baseurl=http://hdp001:80/ambari/HDP/centos7/3.1.0.0-78 gpgcheck=1 gpgkey=http://hdp001:80/ambari/HDP/centos7/3.1.0.0-78/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 [HDP-UTILS-1.1.0.22] name=HDP-UTILS Version - HDP-UTILS-1.1.0.22 baseurl=http://hdp001:80/ambari/HDP-UTILS/centos7/1.1.0.22 gpgcheck=1 gpgkey=http://hdp001:80/ambari/HDP-UTILS/centos7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 将repo文件拷贝到/etc/yum.repos.d/目录 [root@hdp001 ambari]# cp ambari/centos7/2.7.3.0-139/ambari.repo /etc/yum.repos.d/ [root@hdp001 ambari]# cp HDP/centos7/3.1.0.0-78/hdp.repo /etc/yum.repos.d/ 将repo文件拷贝到子节点 [root@hdp001 ambari]# cd /etc/yum.repos.d/ [root@hdp001 yum.repos.d]# pwd /etc/yum.repos.d [root@hdp001 yum.repos.d]# scp ambari.repo hdp002:/etc/yum.repos.d/ ambari.repo 100% 329 467.5KB/s 00:00 [root@hdp001 yum.repos.d]# scp ambari.repo hdp003:/etc/yum.repos.d/ ambari.repo 100% 329 510.4KB/s 00:00 [root@hdp001 yum.repos.d]# scp hdp.repo hdp002:/etc/yum.repos.d/ hdp.repo 100% 483 570.3KB/s 00:00 [root@hdp001 yum.repos.d]# scp hdp.repo hdp003:/etc/yum.repos.d/ hdp.repo 100% 483 352.4KB/s 00:00 每个节点清除yum缓存，重新建立缓存 该环节遇到报错说明yum源配置不正确，检查一下所有repo文件 [root@hdp001 ambari]# yum clean all ... [root@hdp001 ambari]# yum makecache ... [root@hdp001 ambari]# yum repolist ... 安装Ambari-server本次安装使用第三方数据库MySQL模式，默认为PostgreSQL模式（生产环境不推荐）。需提前准备好MySQL数据库连接jar包，MySQL连接驱动包下载方法 Ambari-server节点（主节点）安装Ambari-server[root@hdp001 ~]# yum -y install ambari-server ... 初始化设置使用ambari-server setup命令进行初始化操作。 以下代码段中#(1)类似标识为注解。 [root@hdp001 home]# ambari-server setup Using python /usr/bin/python Setup ambari-server Checking SELinux... SELinux status is 'disabled' Customize user account for ambari-server daemon [y/n] (n)? y #(1)选择自定义配置 Enter user account for ambari-server daemon (root): #(2)选择用户 Adjusting ambari-server permissions and ownership... Checking firewall status... Checking JDK... [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Custom JDK ============================================================================== Enter choice (1): 2 #(3)选择2，自定义jdk WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: /home/jdk #(4)输入自己安装的jdk路径 Validating JDK on Ambari Server...done. Check JDK version for Ambari Server... JDK version found: 8 Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server. Checking GPL software agreement... GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html Enable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? #(5)是否安装GPL，这里默认选择否 Completing setup... Configuring database... Enter advanced database configuration [y/n] (n)? y #(6)高级数据库配置，选是 Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL / MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere [7] - BDB ============================================================================== Enter choice (1): 3 #(7)选择MySQL Hostname (localhost): hdp002 #(8)MySQL地址，这里选择hdp002主机上安装的MySQL Port (3306): #(9)MySQL端口，默认3306 Database name (ambari): #(10)MySQL中数据库名称，默认ambari（之前步骤提前建好的） Username (ambari): root #(11)MySQL用户名 Enter Database Password (bigdata): #(12)MySQL密码 Re-enter password: #(13)再次输入密码 Configuring ambari database... Enter full path to custom jdbc driver: /home/mysql-connector-java-5.1.47.jar #(14)这里输入提前准备好的MySQL连接包地址 Copying /home/mysql-connector-java-5.1.47.jar to /usr/share/java Configuring remote database connection properties... WARNING: Before starting Ambari Server, you must run the following DDL directly from the database shell to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql #(15)这里会提示开启ambari服务之前需要执行DDl建表语句 Proceed with configuring remote database connection properties [y/n] (y)? #(16)继续配置远程数据库连接属性 Extracting system views... ambari-admin-2.7.3.0.139.jar .... Ambari repo file contains latest json url http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.json, updating stacks repoinfos with it... Adjusting ambari-server permissions and ownership... Ambari Server 'setup' completed successfully. 根据上文提示执行DDL语句。将/var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql文件拷贝到MySQL安装节点，并在ambari数据库中执行该脚本。 [root@hdp001 home]# scp /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql hdp002:/home Ambari-DDL-MySQL-CREATE.sql 100% 82KB 39.1MB/s 00:00 [root@hdp002 home]# pwd /home [root@hdp002 home]# ls Ambari-DDL-MySQL-CREATE.sql jdk [root@hdp002 home]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 4 Server version: 5.7.24 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\h' for help. Type '\c' to clear the current input statement. mysql> use ambari; Database changed mysql> source /home/Ambari-DDL-MySQL-CREATE.sql; ... ... 启动ambari-server执行ambari-server start命令启动服务 [root@hdp001 home]# ambari-server start Using python /usr/bin/python Starting ambari-server Ambari Server running with administrator privileges. ERROR: Exiting with exit code -1. REASON: Before starting Ambari Server, you must copy the MySQL JDBC driver JAR file to /usr/share/java and set property "server.jdbc.driver.path=[path/to/custom_jdbc_driver]" in ambari.properties. 报错。根据提示信息，将MySQL连接包拷贝到/usr/share/java/目录下，并设置参数路径（也可在之后安装hive相关组件时设置该参数）。可能会遇到/usr/share/java不是一个目录的情况，此时删掉该文件，新建一个java目录即可。 [root@hdp001 home]# cp /home/mysql-connector-java-5.1.47.jar /usr/share/java/ 再次启动ambari-server即可启动成功 [root@hdp001 home]# ambari-server start Using python /usr/bin/python Starting ambari-server Ambari Server running with administrator privileges. Organizing resource files at /var/lib/ambari-server/resources... Ambari database consistency check started... Server PID at: /var/run/ambari-server/ambari-server.pid Server out at: /var/log/ambari-server/ambari-server.out Server log at: /var/log/ambari-server/ambari-server.log Waiting for server start............................................................ Server started listening on 8080 DB configs consistency check: no errors and warnings were found. Ambari Server 'start' completed successfully. 访问服务器8080端口默认用户名和密码都为admin根据提示安装集群设置集群名字，比如my_hadoop选择HDP版本，配置yum源地址配置host与ssh确认后开始在节点上安装ambari-agent 安装agent时可能报错： ... Connection to node1 closed. SSH command execution finished host=node1, exitcode=0 Command end time 2019-01-15 15:52:22 Registering with the server... Registration with the server failed. 解决方法：报错的节点编辑文件：/etc/ambari-agent/conf/ambari-agent.ini，将hostname修改为正确值 ... [server] hostname=master1 url_port=8440 secured_url_port=8441 connect_retry_delay=10 max_reconnect_retry_delay=30 ... ambari-agent安装成功选择hadoop组件进行安装，建议安装少量组件，之后可以再添加选择主节点安装位置（如NameNode）选择从节点安装位置（如DataNode）进行其他设置（如密码、数据保存路径、用户/用户组、参数配置等）配置完成后，查看配置项是否无误，确认无误后点击发布开始安装等待安装进度完成即可，如果安装过程中出错，可根据报错信息进行修改直到安装成功 使用HDPHDP安装路径 名称 安装路径 HDP各组件默认安装目录 /usr/hdp/版本号 ambari-server安装目录 /usr/lib/ambari-server ambari-agent安装目录 /usr/lib/ambari-agent 日志安装目录 /var/log ambari安装的hdp路径是不能更改的，但是可以用软链接将以上路径链接到其他路径。]]></content>
      <categories>
        <category>大数据</category>
        <category>HDP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>离线安装</tag>
        <tag>HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive2之LLAP搭建]]></title>
    <url>%2F2019%2F01%2F03%2FHive2%E4%B9%8BLLAP%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hive2.0之LLAP搭建 官方介绍llap-longlived-execution-in-hiveHortonworks介绍参考搭建文章：tez hive llap安装Hive llap服务安装说明及测试Hive Llap尝试（0）]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>LLAP</tag>
        <tag>Tez</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tez以及Tez UI安装方法]]></title>
    <url>%2F2019%2F01%2F03%2FTez%E4%BB%A5%E5%8F%8ATez-UI%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Tez是一个基于Hadoop Yarn的新应用程序框架，可以执行一般数据处理任务的复杂有向非循环图。在许多方面，它可以被认为是map-reduce框架的一个更灵活和更强大的继承者。 本文Tez版本：0.9.1 安装Tez安装Tez 0.9.0 Tez UI安装官方文档yarn timeline server大致步骤： 安装tomcat（ui需要运行在tomcat下） 将tez-ui.war包解压到tomcat中webapp/tez-ui/目录下 修改../tomcat/webapps/tez-ui/config/configs.env文件指定timeline地址和resourceManager地址 修改tez-site.xml文件，修改yarn-site.xml文件，使其支持timeline 启动timeline，yarn-daemon.sh start timelineserver或者yarn timelineserver 启动tomcat，访问http://hadoopmaster:8080/tez-ui地址]]></content>
      <categories>
        <category>大数据</category>
        <category>Tez</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Tez</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下搭建HBase集群+HBase基本操作]]></title>
    <url>%2F2018%2F12%2F15%2FCentOS7%E4%B8%8B%E6%90%AD%E5%BB%BAHBase%E9%9B%86%E7%BE%A4%2BHBase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[搭建HBase集群，使用外部Zookeeper集群 本文环境 节点 IP地址 hadoopmaster 192.168.171.10 hadoop001 192.168.171.11 hadoop002 192.168.171.12 ### 下载安装包 下载地址：http://archive.apache.org/dist/hbase/ 根据需要选择合适的版本，本文为hbase-1.4.8-bin.tar.gz 上传、解压使用rz 命令上传到服务器并解压 [root@hadoopmaster opt]# tar -zxvf hbase-1.4.8-bin.tar.gz 配置环境变量（每个节点都需要配置）vim /etc/profile # hbase export HBASE_HOME=/home/hbase # hbase解压安装路径 export PATH=$PATH:$HBASE_HOME/bin 配置后使用source /etc/profile刷新配置文件 配置java路径，关闭内置zk集群修改hbase/conf/hbase-env.sh，修改或增加以下内容 ... export JAVA_HOME=/home/jdk ... export HBASE_MANAGES_ZK=fakse 修改配置文件 修改hbase/conf/hbase-site.xml配置文件 更多参数配置参考：hbase_default_configurations &lt;configuration> &lt;property> &lt;name>hbase.cluster.distributed&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;property> &lt;name>hbase.rootdir&lt;/name> &lt;value>hdfs://hadoopmaster:9000/hbase&lt;/value> &lt;/property> &lt;property> &lt;!-- 配置master节点ip地址/主机名 --> &lt;name>hbase.master.hostname&lt;/name> &lt;value>hadoopmaster&lt;/value> &lt;/property> &lt;property> &lt;!-- 配置master节点端口 --> &lt;name>hbase.master.port&lt;/name> &lt;value>16000&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.replication&lt;/name> &lt;value>2&lt;/value> &lt;/property> &lt;property> &lt;name>hbase.zookeeper.quorum&lt;/name> &lt;value>hadoopmaster:2181,hadoop001:2181,hadoop002:2181&lt;/value> &lt;/property> &lt;!-- 不需要 &lt;property> &lt;name>hbase.zookeeper.property.dataDir&lt;/name> &lt;value>/home/centos/hbase/zk&lt;/value> &lt;/property> --> &lt;/configuration> 注意： 需要指定HDFS中储存路径，hadoop集群搭建参考：CentOS7下搭建Hadoop集群 需要指定Zookeeper服务，Zookeeper集群搭建参考：搭建Zookeeper集群 修改hbase/conf/regionservers文件 # 增加从节点地址（这里由于配置了hosts，直接使用主机名，也可以配ip地址） hadoop001 hadoop002 将文件夹copy到其他子节点通过scp 命令将修改好的文件夹拷贝到各个从节点上[root@hadoopmaster ~]# scp -r /opt/hbase-1.4.8/ hadoop001:/opt ... [root@hadoopmaster ~]# scp -r /opt/hbase-1.4.8/ hadoop002:/opt 确保HDFS与Zookeeper启动xzk_cluster脚本参考：zk集群脚本编写xzk-cluster.sh start start-dfs.sh 启动HBase集群[root@hadoopmaster opt]# start-hbase.sh running master, logging to /home/hbase/logs/hbase-root-master-hadoopmaster.out hadoop001: running regionserver, logging to /home/hbase/bin/../logs/hbase-root-regionserver-hadoop001.out hadoop002: running regionserver, logging to /home/hbase/bin/../logs/hbase-root-regionserver-hadoop002.out 出现警告：Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0解决方法：注释hbase/conf/hbase-env.sh脚本中以下代码，分发到节点 # Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+ export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m" export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m" 验证web界面访问：http://hadoopmaster:16010 单例管理HBase的每个进程$> start-hbase.sh //启动所有节点 $> stop-hbase.sh //停止所有节点 $> hbase-daemon.sh start master //启动master节点 $> hbase-daemons.sh start regionserver //启动所有rs节点 $> hbase-daemon.sh start regionserver //启动单个rs节点 HBase在HDFS中目录表示含义 路径 含义 /hbase/WALs 写前日志 /hbase/data 数据 /hbase/data/default hbase内置默认名字空间 /hbase/data/hbase hbase内置的名字空间（相当于hive中的数据库） 文件表示完整路径示意： /hbase/data/${namespace}/${tablename}/${region_name}/${column_family}/${file_name} ### HBase常用命令 ```bash $&gt; hbase shell //进入hbase shell $hbase&gt; help //查看帮助 $hbase&gt; list_namespace //查看名字空间(数据库) $hbase&gt; list_namespace_tables ‘hbase’ //查看指定空间（hbase空间）下的表 $hbase&gt; scan ‘hbase:meta’ //查看表 $hbase&gt; create_namespace ‘ns1’ //创建名字空间 $hbase&gt; create ‘ns1:t1’ , ‘f1’ //创建表 $hbase&gt; scan ‘ns1:t1’ //扫描表 $hbase&gt; describe ‘t1’ 或者 desc ‘t1’ //查看表结构 $hbase&gt; truncate ‘ns1:t1’ //清空表数据 $hbase&gt; put ‘ns1:t1’,’row1’,’f1:id’,1 //插入数据table,row,f:c,value $hbase&gt; delete ‘ns1:t1’,’row1’,’f1:id’ //删除 ```]]></content>
      <categories>
        <category>大数据</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Zookeeper集群]]></title>
    <url>%2F2018%2F12%2F15%2F%E6%90%AD%E5%BB%BAZookeeper%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[搭建zookeeper完全分布式环境 本文环境 节点 IP地址 hadoopmaster 192.168.171.10 hadoop001 192.168.171.11 hadoop002 192.168.171.12 ### 下载安装包 下载地址：http://mirrors.shu.edu.cn/apache/zookeeper/ 根据需要选择合适的版本，本文为zookeeper-3.4.12.tar.gz 上传、解压使用rz 命令上传到服务器并解压 [root@hadoopmaster opt]# tar -zxvf zookeeper-3.4.12.tar.gz 配置环境变量（每个节点都需要配置）vim /etc/profile # zookeeper export ZK_HOME=/home/zookeeper # zookeeper解压安装目录 export PATH=$PATH:$ZK_HOME/bin 配置后使用source /etc/profile刷新配置文件 配置参数文件 配置conf/zoo.cfg文件 拷贝配置文件cp zoo_sample.cfg zoo.cfg [root@hadoopmaster conf]# ls configuration.xsl log4j.properties zoo_sample.cfg [root@hadoopmaster conf]# cp zoo_sample.cfg zoo.cfg [root@hadoopmaster conf]# ls configuration.xsl log4j.properties zoo.cfg zoo_sample.cfg 编辑配置文件(每个节点配置一样) tickTime=2000 initLimit=5 syncLimit=2 dataDir=/home/zookeeper/data clientPort=2181 server.1=hadoopmaster:2888:3888 server.2=hadoop001:2888:3888 server.3=hadoop002:2888:3888 配置data/myid文件 新建myid文件，路径为zoo.cfg文件中dataDir指定的路径，本文为/home/zookeeper/data hadoopmaster节点： [root@hadoopmaster conf]# echo 1 >> /home/zookeeper/data/myid hadoop001节点： [root@hadoop001 conf]# echo 2 >> /home/zookeeper/data/myid hadoop002节点： [root@hadoop002 conf]# echo 3 >> /home/zookeeper/data/myid 启动服务器集群（每个节点都要启动）[root@hadoopmaster opt]# /home/zookeeper/bin/zkServer.sh start [root@hadoop001 opt]# /home/zookeeper/bin/zkServer.sh start [root@hadoop002 opt]# /home/zookeeper/bin/zkServer.sh start 使用zkServer.sh status命令查看状态[root@hadoopmaster conf]# /home/zookeeper/bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Mode: follower zk集群脚本编写将脚本放到/usr/local/bin/目录方便使用vim /usr/local/bin/xzk-cluster.shchmod 755 /usr/local/bin/xzk-cluster.sh修改权限脚本内容如下#!/bin/bash cmd=$1 servers="hadoopmaster hadoop001 hadoop002" for s in $servers ; do tput setaf 3 echo ========== $s ========== tput setaf 7 ssh $s "source /etc/profile ; zkServer.sh $cmd" done 使用脚本范例：[root@hadoopmaster conf]# xzk-cluster.sh status ========== hadoopmaster ========== ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Mode: follower ========== hadoop001 ========== ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Mode: leader ========== hadoop002 ========== ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Mode: follower [root@hadoopmaster conf]# xzk-cluster.sh stop ========== hadoopmaster ========== ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Stopping zookeeper ... STOPPED ========== hadoop001 ========== ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Stopping zookeeper ... STOPPED ========== hadoop002 ========== ZooKeeper JMX enabled by default Using config: /home/zookeeper/bin/../conf/zoo.cfg Stopping zookeeper ... STOPPED]]></content>
      <categories>
        <category>大数据</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive使用Spark引擎替代MR（Hive on Spark）]]></title>
    <url>%2F2018%2F12%2F09%2FHive%E4%BD%BF%E7%94%A8Spark%E5%BC%95%E6%93%8E%E6%9B%BF%E4%BB%A3MR%EF%BC%88Hive-on-Spark%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Hive on Spark安装（Hive-2.3.4、Spark-2.0.0） 官方文档参考文章 编译成功示例： ... [INFO] [INFO] --- maven-antrun-plugin:1.8:run (default) @ spark-assembly_2.11 --- [INFO] Executing tasks main: [INFO] Executed tasks [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM 2.0.0 ..................... SUCCESS [02:06 min] [INFO] Spark Project Tags ................................. SUCCESS [01:33 min] [INFO] Spark Project Sketch ............................... SUCCESS [ 12.649 s] [INFO] Spark Project Networking ........................... SUCCESS [ 22.077 s] [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 15.268 s] [INFO] Spark Project Unsafe ............................... SUCCESS [ 24.808 s] [INFO] Spark Project Launcher ............................. SUCCESS [ 54.464 s] [INFO] Spark Project Core ................................. SUCCESS [07:09 min] [INFO] Spark Project GraphX ............................... SUCCESS [ 31.348 s] [INFO] Spark Project Streaming ............................ SUCCESS [01:04 min] [INFO] Spark Project Catalyst ............................. SUCCESS [02:22 min] [INFO] Spark Project SQL .................................. SUCCESS [02:50 min] [INFO] Spark Project ML Local Library ..................... SUCCESS [ 27.201 s] [INFO] Spark Project ML Library ........................... SUCCESS [02:25 min] [INFO] Spark Project Tools ................................ SUCCESS [ 7.599 s] [INFO] Spark Project Hive ................................. SUCCESS [01:15 min] [INFO] Spark Project REPL ................................. SUCCESS [ 9.934 s] [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [ 14.017 s] [INFO] Spark Project YARN ................................. SUCCESS [ 39.637 s] [INFO] Spark Project Assembly ............................. SUCCESS [ 3.878 s] [INFO] Spark Project External Flume Sink .................. SUCCESS [ 24.054 s] [INFO] Spark Project External Flume ....................... SUCCESS [ 21.908 s] [INFO] Spark Project External Flume Assembly .............. SUCCESS [ 5.095 s] [INFO] Spark Integration for Kafka 0.8 .................... SUCCESS [ 20.951 s] [INFO] Spark Project Examples ............................. SUCCESS [ 37.750 s] [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 6.523 s] [INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 20.073 s] [INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [ 5.019 s] [INFO] Spark Project Java 8 Tests 2.0.0 ................... SUCCESS [ 11.790 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 27:45 min [INFO] Finished at: 2018-12-09T20:51:07+08:00 [INFO] ------------------------------------------------------------------------ + rm -rf /opt/spark-2.0.0/dist + mkdir -p /opt/spark-2.0.0/dist/jars + echo 'Spark 2.0.0 built for Hadoop 2.7.2' + echo 'Build flags: -Pyarn,hadoop-provided,hadoop-2.7,parquet-provided' ...]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL连接驱动包下载方法]]></title>
    <url>%2F2018%2F12%2F09%2FMySQL%E8%BF%9E%E6%8E%A5%E9%A9%B1%E5%8A%A8%E5%8C%85%E4%B8%8B%E8%BD%BD%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[mysql连接驱动包mysql-connector-java-x.x.x.jar很多地方都会用到，本文介绍官方下载方式 下载链接地址官方下载地址：https://dev.mysql.com/downloads/connector/j/ 下载步骤选择版本默认为最新版本，可以按如下所示选择历史版本 选择操作系统 确认下载后缀为.tar.gz 跳过登录直接下载 压缩包中jar包]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>驱动包</tag>
        <tag>连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下离线安装MySQL]]></title>
    <url>%2F2018%2F12%2F08%2FCentOS7%E4%B8%8B%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85MySQL%2F</url>
    <content type="text"><![CDATA[CentOS7下离线安装MySQL 下载社区版离线安装包本文为mysql-5.7.24-1.el7.x86_64.rpm-bundle.tar下载路径：https://dev.mysql.com/downloads/mysql/5.7.html#downloads 选择适合CentOS的版本 跳过登录直接下载卸载系统自带的mariadb-lib[root@hadoopmaster opt]# rpm -qa|grep mariadb mariadb-libs-5.5.56-2.el7.x86_64 [root@hadoopmaster opt]# rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 解压mysql 使用rz 命令上传文件到服务器 解压[root@hadoopmaster mysql]# tar -xf mysql-5.7.24-1.el7.x86_64.rpm-bundle.tar [root@hadoopmaster mysql]# ls mysql-5.7.24-1.el7.x86_64.rpm-bundle.tar mysql-community-client-5.7.24-1.el7.x86_64.rpm mysql-community-common-5.7.24-1.el7.x86_64.rpm mysql-community-devel-5.7.24-1.el7.x86_64.rpm mysql-community-embedded-5.7.24-1.el7.x86_64.rpm mysql-community-embedded-compat-5.7.24-1.el7.x86_64.rpm mysql-community-embedded-devel-5.7.24-1.el7.x86_64.rpm mysql-community-libs-5.7.24-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.24-1.el7.x86_64.rpm mysql-community-minimal-debuginfo-5.7.24-1.el7.x86_64.rpm mysql-community-server-5.7.24-1.el7.x86_64.rpm mysql-community-server-minimal-5.7.24-1.el7.x86_64.rpm mysql-community-test-5.7.24-1.el7.x86_64.rpm 安装使用rpm -ivh命令依次进行安装按顺序安装以下软件包mysql-community-common-5.7.24-1.el7.x86_64.rpm mysql-community-libs-5.7.24-1.el7.x86_64.rpm mysql-community-client-5.7.24-1.el7.x86_64.rpm mysql-community-server-5.7.24-1.el7.x86_64.rpm 有时可能需要安装以下包mysql-community-libs-compat-5.7.24-1.el7.x86_64.rpm 安装具体如下 [root@hadoopmaster mysql]# rpm -ivh mysql-community-common-5.7.24-1.el7.x86_64.rpm 警告：mysql-community-common-5.7.24-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY 准备中... ################################# [100%] 正在升级/安装... 1:mysql-community-common-5.7.24-1.e################################# [100%] [root@hadoopmaster mysql]# rpm -ivh mysql-community-libs-5.7.24-1.el7.x86_64.rpm 警告：mysql-community-libs-5.7.24-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY 准备中... ################################# [100%] 正在升级/安装... 1:mysql-community-libs-5.7.24-1.el7################################# [100%] [root@hadoopmaster mysql]# rpm -ivh mysql-community-client-5.7.24-1.el7.x86_64.rpm 警告：mysql-community-client-5.7.24-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY 准备中... ################################# [100%] 正在升级/安装... 1:mysql-community-client-5.7.24-1.e################################# [100%] [root@hadoopmaster mysql]# rpm -ivh mysql-community-server-5.7.24-1.el7.x86_64.rpm 警告：mysql-community-server-5.7.24-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY 准备中... ################################# [100%] 正在升级/安装... 1:mysql-community-server-5.7.24-1.e################################# [100%] 注意：安装mysql-community-server-5.7.24-1.el7.x86_64.rpm时可能会遇到问题: [root@hadoopmaster mysql]# rpm -ivh mysql-community-server-5.7.24-1.el7.x86_64.rpm 警告：mysql-community-server-5.7.24-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY 错误：依赖检测失败： libaio.so.1()(64bit) 被 mysql-community-server-5.7.24-1.el7.x86_64 需要 libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.24-1.el7.x86_64 需要 libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.24-1.el7.x86_64 需要 net-tools 被 mysql-community-server-5.7.24-1.el7.x86_64 需要 解决办法： 1）缺少libaio [root@hadoopmaster mysql]# yum -y install libaio 2）缺少net-tools [root@hadoopmaster mysql]# yum -y install net-tools 初始化数据库初始化后会在/var/log/mysqld.log生成随机密码 [root@hadoopmaster mysql]# mysqld --initialize [root@hadoopmaster mysql]# cat /var/log/mysqld.log 2018-12-08T07:53:52.970182Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2018-12-08T07:53:54.251035Z 0 [Warning] InnoDB: New log files created, LSN=45790 2018-12-08T07:53:54.313973Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2018-12-08T07:53:54.388855Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 68ce693c-fabe-11e8-a2ff-000c298184c2. 2018-12-08T07:53:54.389442Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened. 2018-12-08T07:53:54.390177Z 1 [Note] A temporary password is generated for root@localhost: qHQHahCw(7n) 最后一串随机字符串为初始密码，本文中为qHQHahCw(7n) 修改用户及用户组，启动mysql数据库修改mysql数据库目录的所属用户及其所属组，然后启动mysql数据库 [root@hadoopmaster mysql]# chown mysql:mysql /var/lib/mysql -R [root@hadoopmaster mysql]# systemctl start mysqld.service [root@hadoopmaster mysql]# systemctl status mysqld.service ● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since 六 2018-12-08 15:59:12 CST; 6s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 23162 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS) Process: 23145 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS) Main PID: 23165 (mysqld) CGroup: /system.slice/mysqld.service └─23165 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid 12月 08 15:59:10 hadoopmaster systemd[1]: Starting MySQL Server... 12月 08 15:59:12 hadoopmaster systemd[1]: Started MySQL Server. 设置开机自启动查看是否开启开机自启动 [root@hadoopmaster mysql-install]# systemctl list-unit-files | grep mysqld mysqld.service enabled mysqld@.service disabled 注意：mysql5.7.23安装后已默认设置为开机启动，如果没有设置，可以使用下面命令设置为开机启动 [root@hadoopmaster mysql]# systemctl enable mysqld.service 登录，更改root用户密码登录mysql，更改root用户密码（系统强制要求，否则不能操作mysql） [root@hadoopmaster mysql-install]# mysql -uroot -p'qHQHahCw(7n)' mysql: [Warning] Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 2 Server version: 5.7.24 Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\h' for help. Type '\c' to clear the current input statement. mysql> set password=password('1234'); Query OK, 0 rows affected, 1 warning (0.00 sec) mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) 远程登录授权命令为：grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;1234&#39; with grant option;flush privileges; *.* 表示授权任何库任何表，如果想只授权test库的user表可以写为：test.user&#39;root&#39;@&#39;%&#39; 其中root表示以root用户授权，@为连接符，%表示匹配所有的主机，如果想单独给某主机授权，可以将%替换为需要授权的主机ip地址&#39;1234&#39; 表示授权访问的密码，可以自行设置密码设置授权后需要用flush privileges命令刷新一下 示例： mysql> grant all privileges on *.* to 'root'@'%' identified by '1234' with grant option; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql> flush privileges; Query OK, 0 rows affected (0.00 sec)]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用图形化界面工具DbVisualizer连接Hive数据库]]></title>
    <url>%2F2018%2F12%2F08%2F%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%95%8C%E9%9D%A2%E5%B7%A5%E5%85%B7DbVisualizer%E8%BF%9E%E6%8E%A5Hive%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[使用图形化界面工具DbVisualizer连接Hive数据库 参考链接：https://www.cnblogs.com/qingyunzong/p/8715250.html]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>DbVisualizer</tag>
        <tag>图形化工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下搭建Hive]]></title>
    <url>%2F2018%2F12%2F06%2FCentOS7%E4%B8%8B%E6%90%AD%E5%BB%BAHive%2F</url>
    <content type="text"><![CDATA[CentOS7下搭建Hive，Hive元数据 搭建Hadoop集群参考CentOS7下搭建Hadoop集群 下载Hive压缩包找到合适版本下载hive，本文压缩包为apache-hive-2.3.4-bin.tar.gz下载地址 解压，配置环境变量 解压到指定目录（本文目录为/home/hive）tar -zxvf apache-hive-2.3.4-bin.tar.gz 配置环境变量vim /etc/profile，添加以下代码 # hive export HIVE_HOME=/home/hive # 为hive解压目录 export PATH=$PATH:${HIVE_HOME}/bin 刷新配置文件source /etc/profile 验证hive版本hive --version [root@hadoopmaster hive]# hive --version Hive 2.3.4 Git git://daijymacpro-2.local/Users/daijy/commit/hive -r 56acdd2120b9ce6790185c679223b8b5e884aaf2 Compiled by daijy on Wed Oct 31 14:20:50 PDT 2018 From source with checksum 9f2d17b212f3a05297ac7dd40b65bab0 hive-env.sh配置 复制/home/hive/conf/hive-env.sh.template -&gt; /home/hive/conf/hive-env.sh[root@hadoopmaster ~]# cd /home/hive/conf [root@hadoopmaster conf]# cp hive-env.sh.template hive-env.sh 修改/home/hive/conf/hive-env.sh，找到以下配置，根据实际情况修改export JAVA_HOME=/home/jdk #jdk安装目录 export HADOOP_HOME=/home/hadoop #hadoop安装目录 export HIVE_HOME=/home/hive #hive安装目录 hive-site.xml配置 复制/home/hive/conf/hive-default.template -&gt; /home/hive/conf/hive-site.xml [root@hadoopmaster conf]# cp hive-default.xml.template hive-site.xml 修改/home/hive/conf/hive-site.xml，找到文件中以下配置，根据实际情况修改hive其他配置参考：Hive Configuration Properties &lt;configuration> &lt;!-- mysql数据库配置 --> &lt;property> &lt;name>javax.jdo.option.ConnectionPassword&lt;/name> &lt;value>1234&lt;/value> &lt;description>mysql密码&lt;/description> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionUserName&lt;/name> &lt;value>root&lt;/value> &lt;description>mysql用户名&lt;/description> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionURL&lt;/name> &lt;value>jdbc:mysql://hadoopmaster:3306/hive?useSSL=false&lt;/value> &lt;description>JDBC连接路径&lt;/description> &lt;/property> &lt;property> &lt;name>javax.jdo.option.ConnectionDriverName&lt;/name> &lt;value>com.mysql.jdbc.Driver&lt;/value> &lt;description>Driver class name for a JDBC metastore&lt;/description> &lt;/property> &lt;!-- 其他配置 --> &lt;property> &lt;name>hive.cli.print.header&lt;/name> &lt;value>true&lt;/value> &lt;description>是否显示查询结果的列名，默认为不显示。 &lt;/description> &lt;/property> &lt;property> &lt;name>hive.cli.print.current.db&lt;/name> &lt;value>true&lt;/value> &lt;description>是否显示数据库名称，默认为不显示&lt;/description> &lt;/property> &lt;property> &lt;name>hive.server2.webui.port&lt;/name> &lt;value>10002&lt;/value> &lt;description>HiveServer2 Web页面端口设置&lt;/description> &lt;/property> &lt;property> &lt;name>hive.exec.local.scratchdir&lt;/name> &lt;value>/home/hive/tmp&lt;/value> &lt;description>Hive作业的本地临时空间&lt;/description> &lt;/property> &lt;property> &lt;name>hive.downloaded.resources.dir&lt;/name> &lt;value>/home/hive/downloads&lt;/value> &lt;description>用于在远程文件系统中添加资源的临时本地目录。&lt;/description> &lt;/property> &lt;property> &lt;name>hive.querylog.location&lt;/name> &lt;value>/home/hive/querylog&lt;/value> &lt;description>Hive 实时查询日志所在的目录，如果该值为空，将不创建实时的查询日志。&lt;/description> &lt;/property> &lt;property> &lt;name>hive.server2.logging.operation.log.location&lt;/name> &lt;value>/home/hive/server2_logs&lt;/value> &lt;description>如果启用了日志记录功能，则存储操作日志的顶级目录&lt;/description> &lt;/property> &lt;/configuration> 复制mysql驱动到hive安装目录的lib下mysql-connector-java-5.1.46.jar点我查看驱动包下载方式 初始化元数据 在mysql中创建hive数据库 运行hive的SchemaTool进行初始化hive的元数据schematool -dbType mysql -initSchema[root@hadoopmaster ~]# cd /home/hive/bin [root@hadoopmaster bin]# ./schematool # 查看帮助 [root@hadoopmaster bin]# ./schematool -dbType mysql -initSchema]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker搭建Nginx图片服务器]]></title>
    <url>%2F2018%2F12%2F05%2F%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BANginx%E5%9B%BE%E7%89%87%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[使用Docker搭建Nginx图片服务器 安装Docker见Docker常用命令 编写Nginx配置文件vim default.conf server { listen 80; server_name localhost; #(5) #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } #(1) location /images/ { root /mnt/; autoindex on; #(2) autoindex_exact_size off; #(3) autoindex_localtime on; #(4) charset utf-8,gbk; #(5) } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } 参数说明：(1)：添加图片目录映射，映射目录为/mnt/images/(2)：在Nginx下默认是不允许列出整个目录的。如需此功能，将该项设置为on(3)：默认为on，显示出文件的确切大小，单位是bytes&emsp;&emsp;&emsp; 改为off后，显示出文件的大概大小，单位是kB或者MB或者GB(4)：默认为off，显示的文件时间为GMT时间&emsp;&emsp;&emsp; 注意:改为on后，显示的文件时间为文件的服务器时间(5):设置编码（防止中文乱码），可以设置对全局生效或者部分路径生效 编写Dockerfilevim Dockerfile # 使用最小化镜像（只有17.7m） FROM nginx:stable-alpine # 这里替换为自己的信息 MAINTAINER liming # 覆盖容器里默认配置 COPY default.conf /etc/nginx/conf.d/default.conf EXPOSE 80 构建镜像docker build -t image-nginx . 运行生成容器docker run -d --name image-nginx -p 80:80 -v /mnt/nginx/images:/mnt/images image-nginx -v 将服务器本地/mnt/nginx/images映射到容器内/mnt/images目录，容器目录与default.conf文件中配置对应 访问图片通过ip地址/images/aaa.jpg访问图片配置文件服务器同理]]></content>
      <categories>
        <category>Linux</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Nginx</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Boot使用JDBC连接Hive]]></title>
    <url>%2F2018%2F11%2F19%2FSpring-Boot%E4%BD%BF%E7%94%A8JDBC%E8%BF%9E%E6%8E%A5Hive%2F</url>
    <content type="text"><![CDATA[项目中需要使用SpringBoot操作Hive进行开发，这里记录SpringBoot使用JdbcTemplate连接Hive的方法。开发环境使用Maven。 添加Maven依赖&lt;!-- hive --> &lt;dependency> &lt;groupId>org.apache.hive&lt;/groupId> &lt;artifactId>hive-jdbc&lt;/artifactId> &lt;version>2.3.3&lt;/version> &lt;/dependency> &lt;!-- hive === end --> 编写配置文件数据库连接池使用HikariCP，参数设置参考：https://github.com/brettwooldridge/HikariCP 在../resources目录下创建配置文件application-hive.yml，文件格式也可以使用properties格式 hive: jdbcurl: jdbc:hive2://10.75.4.31:10000/mydb driver-class-name: org.apache.hive.jdbc.HiveDriver username: root password: 123456 # 下面为连接池的补充设置，应用到上面所有数据源中 # 初始化大小，最小连接数，最大连接数 initialSize: 1 minimumIdle: 3 maximumPoolSize: 10 # 等待来自池的连接的最大毫秒数(创建连接超时时间) connectionTimeout: 120000 # 验证超时时间 validationTimeout: 10000 # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 idleTimeout: 30000 # 配置一个连接在池中最大生存的时间，单位是毫秒 maxLifeTime: 600000 在application.yml文件中指定激活application-hive.yml配置文件（这里也可以不配置，在config类中另外配置）。 spring: application: name: ProjectName profiles: active: hive 编写config配置类，将Hive的JdbcTemplate加载到Spring容器中 采用手动加载DataSource配置方式（不推荐）import com.zaxxer.hikari.HikariDataSource; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.core.env.Environment; import org.springframework.jdbc.core.JdbcTemplate; import javax.annotation.Resource;import javax.sql.DataSource;import java.util.Objects; /** Hive-JDBC配置 @author liming @date Created in 2018/11/15 11:58 /@Configurationpublic class HiveJdbcConfig { private static final Logger logger = LoggerFactory.getLogger(HiveJdbcConfig.class); @Resource private Environment env; @Bean(name = “hiveJdbcDataSource”) public DataSource dataSource() { DataSourceProperties dataSourceProperties = new DataSourceProperties(); dataSourceProperties.setUrl(env.getProperty(&quot;hive.url&quot;)); dataSourceProperties.setDriverClassName(env.getProperty(&quot;hive.driver-class-name&quot;)); dataSourceProperties.setUsername(env.getProperty(&quot;hive.username&quot;)); dataSourceProperties.setPassword(env.getProperty(&quot;hive.password&quot;)); HikariDataSource dataSource = dataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build(); //下面的配置根据实际情况添加、修改 //连接超时时间 dataSource.setConnectionTimeout(Long.valueOf(Objects.requireNonNull(env.getProperty(&quot;hive.connectionTimeout&quot;)))); //连接池最大连接数 dataSource.setMaximumPoolSize(Integer.valueOf(Objects.requireNonNull(env.getProperty(&quot;hive.maximumPoolSize&quot;)))); //最小闲置连接数 dataSource.setMinimumIdle(Integer.valueOf(Objects.requireNonNull(env.getProperty(&quot;hive.minimumIdle&quot;)))); //配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 dataSource.setIdleTimeout(Long.parseLong(Objects.requireNonNull(env.getProperty(&quot;hive.idleTimeout&quot;)))); //连接最大生存时间 dataSource.setMaxLifetime(Long.parseLong(Objects.requireNonNull(env.getProperty(&quot;hive.maxLifeTime&quot;)))); //验证超时时间 dataSource.setValidationTimeout(Long.parseLong(Objects.requireNonNull(env.getProperty(&quot;hive.validationTimeout&quot;)))); logger.debug(&quot;Hive DataSource Inject Successfully...&quot;); return dataSource; } @Bean(name = “hiveJdbcTemplate”) public JdbcTemplate hiveJdbcTemplate(@Qualifier(“hiveJdbcDataSource”) DataSource dataSource) { return new JdbcTemplate(dataSource); }} 2. 采用自动加载DataSource配置方式（推荐） 该方式需要yml配置文件中变量名与DataSource类里面的变量名对应 ```java package tech.segma.bi.config; import com.zaxxer.hikari.HikariDataSource;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.jdbc.core.JdbcTemplate; import javax.sql.DataSource; /** Hive-JDBC配置 @author liming @date Created in 2018/11/15 11:58 /@Configurationpublic class HiveJdbcConfig { private static final Logger LOGGER = LoggerFactory.getLogger(HiveJdbcConfig.class); @Bean @ConfigurationProperties(prefix = “hive”)//需要配置前缀 public DataSourceProperties dataSourceProperties() { return new DataSourceProperties(); } @Bean(name = “hiveJdbcDataSource”) @ConfigurationProperties(prefix = “hive”)//需要配置前缀 public DataSource dataSource() { return dataSourceProperties().initializeDataSourceBuilder().type(HikariDataSource.class).build(); } @Bean(name = “hiveJdbcTemplate”) public JdbcTemplate hiveJdbcTemplate(@Qualifier(“hiveJdbcDataSource”) DataSource dataSource) { LOGGER.debug(&quot;Hive DataSource Inject Successfully...&quot;); return new JdbcTemplate(dataSource); }} 在代码中使用JdbcTemplate@Resource(name = "hiveJdbcTemplate") private JdbcTemplate hiveJdbcTemplate; public boolean loadFileToTable(String filePath, String tableName) { // String filePath = "/home/hadoop/user_sample.txt"; String sql = "load data local inpath '" + filePath + "' into table " + tableName; try { hiveJdbcTemplate.execute(sql); return true; } catch (DataAccessException dae) { logger.error("Load data into table encounter an error: " + dae.getMessage()); return false; } }]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Spring Boot</tag>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop修改端口后Hive连接方法]]></title>
    <url>%2F2018%2F11%2F16%2FHadoop%E4%BF%AE%E6%94%B9%E7%AB%AF%E5%8F%A3%E5%90%8EHive%E8%BF%9E%E6%8E%A5%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Hadoop修改端口后，hive继续操作会报错，找不到HDFS，此时需要到元数据库中（本文为mysql）修改对应数据 元数据库mysql里面两张表：DBS ： Hive数据仓库的路径SDS ： Hive每张表对应的路径找到对应路径端口，修改]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下搭建Kubernetes环境]]></title>
    <url>%2F2018%2F10%2F31%2FCentOS7%E4%B8%8B%E6%90%AD%E5%BB%BAkubernetes%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[CentOS7使用kubeadm安装kubernetes 1.11版本多主高可用（进行中） 1.11多主高可用1.10kubeadm安装 安装配置docker v1.11.0版本推荐使用docker v17.03, v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。 测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker # 卸载安装指定版本docker-ce [root@master ~]# yum remove -y docker-ce docker-ce-selinux container-selinux [root@master ~]# yum install -y --setopt=obsoletes=0 \ docker-ce-17.03.1.ce-1.el7.centos \ docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker [root@master ~]# systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl 如下操作在所有节点操作 # 配置源 [root@master ~]# cat &lt;&lt;EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 安装 [root@master ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 禁用交换分区swap查看交换分区：free -h [root@localhost ~]# free -h total used free shared buff/cache available Mem: 2.8G 159M 404M 112M 2.2G 2.3G Swap: 3.0G 2.8M 3.0G swapoff -a 临时禁用所有交换 [root@k8s003 save]# swapoff -a [root@k8s003 save]# free -h total used free shared buff/cache available Mem: 2.8G 158M 403M 113M 2.2G 2.3G Swap: 0B 0B 0B 永久禁用，vim /etc/fstab注释swap行，重启 # # /etc/fstab # Created by anaconda on Mon Nov 5 19:49:25 2018 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # /dev/mapper/centos-root / xfs defaults 0 0 UUID=daffff7e-bc4d-4cf0-bcdd-9b4a99a77ccc /boot xfs defaults 0 0 # 注释该行 # /dev/mapper/centos-swap swap swap defaults 0 0 初始化[root@k8smaster opt]# kubeadm init --kubernetes-version=v1.10.0 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.40.100 k8s1.6 pod apiPod是kubernetes REST API中的顶级资源类型。在kuberentes1.6的V1 core API版本中的Pod的数据结构如下图所示： docker指令和k8s指令对比指令对比]]></content>
      <categories>
        <category>容器</category>
        <category>K8s</category>
      </categories>
      <tags>
        <tag>集群</tag>
        <tag>K8s</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Phoenix操作HBase]]></title>
    <url>%2F2018%2F10%2F30%2F%E4%BD%BF%E7%94%A8Phoenix%E6%93%8D%E4%BD%9CHBase%2F</url>
    <content type="text"><![CDATA[使用Phoenix操作HBase HBase集群环境搭建参考：CentOS7下搭建HBase集群+HBase基本操作 下载与HBase版本兼容的Phoenixapache-phoenix-4.14.1-HBase-1.4-bin.tar.gz下载地址：http://archive.apache.org/dist/phoenix/ 解压复制phoenix-4.14.1-HBase-1.4-server.jar到hbase/lib下并分发到从节点 [root@hadoopmaster phoenix]# pwd /home/phoenix [root@hadoopmaster phoenix]# cp phoenix-4.14.1-HBase-1.4-server.jar /home/hbase/lib/ //分发 [root@hadoopmaster phoenix]# scp phoenix-4.14.1-HBase-1.4-server.jar hadoop001:/home/hbase/lib/ phoenix-4.14.1-HBase-1.4-server.jar 100% 40MB 60.4MB/s 00:00 [root@hadoopmaster phoenix]# scp phoenix-4.14.1-HBase-1.4-server.jar hadoop002:/home/hbase/lib/ phoenix-4.14.1-HBase-1.4-server.jar 100% 40MB 55.1MB/s 00:00 重启HBase[root@hadoopmaster bin]# stop-hbase.sh ... [root@hadoopmaster bin]# start-hbase.sh ... 使用sqlline.py命令行终端运行phoenix/bin/sqlline.py脚本，连接zookeeper [root@hadoopmaster bin]# pwd /home/phoenix/bin [root@hadoopmaster bin]# ./sqlline.py hadoopmaster:2181 报错： Error: SYSTEM.CATALOG (state=08000,code=101) org.apache.phoenix.exception.PhoenixIOException: SYSTEM.CATALOG at org.apache.phoenix.util.ServerUtil.parseServerException(ServerUtil.java:144) at org.apache.phoenix.query.ConnectionQueryServicesImpl.metaDataCoprocessorExec(ConnectionQueryServicesImpl.java:1379) at org.apache.phoenix.query.ConnectionQueryServicesImpl.metaDataCoprocessorExec(ConnectionQueryServicesImpl.java:1343) at org.apache.phoenix.query.ConnectionQueryServicesImpl.getTable(ConnectionQueryServicesImpl.java:1560) at org.apache.phoenix.schema.MetaDataClient.updateCache(MetaDataClient.java:643) ... at sqlline.SqlLine.start(SqlLine.java:398) at sqlline.SqlLine.main(SqlLine.java:291) Caused by: org.apache.hadoop.hbase.TableNotFoundException: SYSTEM.CATALOG at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1283) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1181) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1165) ... at org.apache.phoenix.query.ConnectionQueryServicesImpl.metaDataCoprocessorExec(ConnectionQueryServicesImpl.java:1362) ... 31 more 解决方案：查看hdfs中/hbase/data/default/目录下是否有SYSTEM.*等文件。如果没有，则： 停止HBase，保留zookeeper启动 执行hbase clean --cleanZk命令 重新启动HBase，使用Phoenix连接参考链接：stackoverflow 连接成功[root@hadoopmaster bin]# ./sqlline.py hadoop001:2181 Setting property: [incremental, false] Setting property: [isolation, TRANSACTION_READ_COMMITTED] issuing: !connect jdbc:phoenix:hadoop001:2181 none none org.apache.phoenix.jdbc.PhoenixDriver Connecting to jdbc:phoenix:hadoop001:2181 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/home/apache-phoenix-4.14.0-HBase-1.2-bin/phoenix-4.14.0-HBase-1.2-client.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/hadoop/hadoop-2.7.6/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 18/10/30 03:59:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Connected to: Phoenix (version 4.14) Driver: PhoenixEmbeddedDriver (version 4.14) Autocommit status: true Transaction isolation: TRANSACTION_READ_COMMITTED Building list of tables and columns for tab-completion (set fastconnect to true to skip)... 133/133 (100%) Done Done sqlline version 1.2.0 0: jdbc:phoenix:hadoopmaster:2181>| 开启schema对应namespacePhoenix默认使用的是HBase中default名字空间（namespace），如果要用自定义的namespace，Phoenix中与之对应的是schema的概念，但是默认是关闭的，需要单独配置。 在hbase/conf/hbase-site.xml、phoenix/bin/hbase-site.xml两个文件中增加以下代码： &lt;property> &lt;name>phoenix.schema.isNamespaceMappingEnabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;property> &lt;name>phoenix.schema.mapSystemTablesToNamespace&lt;/name> &lt;value>true&lt;/value> &lt;/property> 如果HBase是分布式，则需要将文件分发到其他节点（最好是将该文件也复制到phoenix/bin/保证客户端与服务端的一致性） Phoenix其他相关配置参照：https://phoenix.apache.org/tuning.html[root@hadoopmaster conf]# scp hbase-site.xml hadoop001:/home/hbase/conf/ hbase-site.xml 100% 1569 2.2MB/s 00:00 [root@hadoopmaster conf]# scp hbase-site.xml hadoop002:/home/hbase/conf/ hbase-site.xml 100% 1569 1.6MB/s 00:00 重启HBase，重新连接 报错：Error: ERROR 726 (43M10): Inconsistent namespace mapping properties. Ensure that config phoenix.schema.isNamespaceMappingEnabled is consistent on client and server. (state=43M10,code=726) [root@hadoopmaster conf]#sqlline.py hadoopmaster:2181 Setting property: [incremental, false] Setting property: [isolation, TRANSACTION_READ_COMMITTED] issuing: !connect jdbc:phoenix:hadoopmaster:2181 none none org.apache.phoenix.jdbc.PhoenixDriver Connecting to jdbc:phoenix:hadoopmaster:2181 18/12/18 09:41:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Error: ERROR 726 (43M10): Inconsistent namespace mapping properties. Ensure that config phoenix.schema.isNamespaceMappingEnabled is consistent on client and server. (state=43M10,code=726) java.sql.SQLException: ERROR 726 (43M10): Inconsistent namespace mapping properties. Ensure that config phoenix.schema.isNamespaceMappingEnabled is consistent on client and server. at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:494) at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:150) at org.apache.phoenix.query.ConnectionQueryServicesImpl.checkClientServerCompatibility(ConnectionQueryServicesImpl.java:1310) at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1154) at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1491) at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:2725) at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1114) at org.apache.phoenix.compile.CreateTableCompiler$1.execute(CreateTableCompiler.java:192) at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:408) at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:391) at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53) at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:390) at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:378) at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:1806) at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:2536) at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:2499) at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76) at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:2499) at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:255) at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:150) at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:221) at sqlline.DatabaseConnection.connect(DatabaseConnection.java:157) at sqlline.DatabaseConnection.getConnection(DatabaseConnection.java:203) at sqlline.Commands.connect(Commands.java:1064) at sqlline.Commands.connect(Commands.java:996) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at sqlline.ReflectiveCommandHandler.execute(ReflectiveCommandHandler.java:38) at sqlline.SqlLine.dispatch(SqlLine.java:809) at sqlline.SqlLine.initArgs(SqlLine.java:588) at sqlline.SqlLine.begin(SqlLine.java:661) at sqlline.SqlLine.start(SqlLine.java:398) at sqlline.SqlLine.main(SqlLine.java:291) sqlline version 1.2.0 原因：hbase和Phoenix配置不一致导致，需要将上述两个地方的文件都作修改。 基本操作（常用命令）注意：HBase是大小写敏感，Phoenix操作时需要添加双引号，如果不添加双引号的话会统一转换成大写 //查看帮助 $sqlline> !help //列出连接 $sqlline> !list //显式表 $sqlilne> !tables //列出所有列 $sqlline> !columns myns.test //创建schema（相当于数据库） $sqlline> create schema wndb; //删除表结构 $sqlline> drop table "test"; //创建表 $sqlline> create table "ns1"."test"(id integer primary key ,name varchar,age integer) ; //创建表并指定列族 $sqlline> create table "ns1"."test"(id integer primary key ,"cf1".name varchar,"cf2".age integer) ; //插入数据和更新数据 $sqlline> upsert into "myns"."test"(id,name,age) values(1,'tom',12) //删除 $sqlline> delete from "myns"."test" where id = 1 ; //条件查询 $sqlline> select * from "myns"."test" where name like 't%' ; 创建ID自增的表Phoenix中创建表时，必须制定主键。在关系型数据库中（如：mysql）设置主键自增非常容易，但是Phoenix中不能直接设置主键自增，需要额外新建一个自增序列sequence，当插入表数据时，根据sequence的值，来进行自增id的插入。 语法 CREATE SEQUENCE [IF NOT EXISTS] SCHEMA.SEQUENCE_NAME [START WITH number] [INCREMENT BY number] [MINVALUE number] [MAXVALUE number] [CYCLE] [CACHE number] 参数说明 start用于指定第一个值。如果不指定默认为1。 increment指定每次调用next value for后自增大小。 如果不指定默认为1。 minvalue和maxvalue一般与cycle连用, 让自增数据形成一个环，从最小值到最大值，再从最大值到最小值。 cache默认为100, 表示server端生成100个自增序列缓存在客户端，可以减少rpc次数。此值也可以通过phoenix.sequence.cacheSize来配置。 案例与注意事项 注意cache值：如果一次插入个数小于该数n，则断开连接后，由于缓存在客户端，下次重新连接后插入会接着生成n个缓存数来使用，导致自增序列不连续 案例如下： # 演示表a中无数据 $sqlline> select * from a; +-----+------+-------+ | ID | AGE | NAME | +-----+------+-------+ +-----+------+-------+ # 1、创建一个自增序列test，缓存大小设置为50 $sqlline> create sequence test cache 50; No rows affected (0.016 seconds) # 查看序列详情 $sqlline> select * from system."SEQUENCE"; +---------+---------------+-------------+----------+-------------+------------+----------+--------------------+-----------------+ |TENANT_ID|SEQUENCE_SCHEMA|SEQUENCE_NAME|START_WITH|CURRENT_VALUE|INCREMENT_BY|CACHE_SIZE| MIN_VALUE | MAX_VALUE | +---------+---------------+-------------+----------+-------------+------------+----------+--------------------+-----------------+ | | |TEST |1 |1 |1 |50 |-9223372036854775808|92233720368547758| +---------+---------------+-------------+----------+-------------+------------+----------+--------------------+-----------------+ # 2、向演示表a中插入数据，id使用test序列中的值 $sqlline> upsert into a values(next value for test, 20, 'zhangsan'); 1 row affected (0.019 seconds) $sqlline> upsert into a values(next value for test, 30, 'zhangsan1'); 1 row affected (0.019 seconds) $sqlline> upsert into a values(next value for test, 40, 'zhangsan2'); 1 row affected (0.019 seconds) # 成功插入多条数据，id自增 $sqlline> select * from a; +-----+-------+-----------+ | ID | AGE | NAME | +-----+-------+-----------+ | 1 | 20.0 | zhangsan | | 2 | 30.0 | zhangsan1 | | 3 | 40.0 | zhangsan2 | +-----+-------+-----------+ # 3、退出当前连接 $sqlline> !quit Closing: org.apache.phoenix.jdbc.PhoenixConnection # 4、重新连接 [root@hadoopmaster shell]# sqlline.py hadoopmaster,hadoop001,hadoop002:2181 ... 155/155 (100%) Done Done sqlline version 1.2.0 # 查看当前自增序列状态，发现当前值CURRENT_VALUE已经变为51 $sqlline> select * from system."SEQUENCE"; +---------+---------------+-------------+----------+-------------+------------+----------+--------------------+-----------------+ |TENANT_ID|SEQUENCE_SCHEMA|SEQUENCE_NAME|START_WITH|CURRENT_VALUE|INCREMENT_BY|CACHE_SIZE| MIN_VALUE | MAX_VALUE | +---------+---------------+-------------+----------+-------------+------------+----------+--------------------+-----------------+ | | |TEST |1 |51 |1 |50 |-9223372036854775808|92233720368547758| +---------+---------------+-------------+----------+-------------+------------+----------+--------------------+-----------------+ # 5、继续使用test序列值插入演示表a $sqlline> upsert into a values(next value for test, 21, 'lisi'); 1 row affected (0.067 seconds) # 6、发现id已经变为51，导致id不连续的问题 $sqlline> select * from a; +-----+-------+-----------+ | ID | AGE | NAME | +-----+-------+-----------+ | 1 | 20.0 | zhangsan | | 2 | 30.0 | zhangsan1 | | 3 | 40.0 | zhangsan2 | | 51 | 21.0 | lisi | +-----+-------+-----------+ 分页查询]]></content>
      <categories>
        <category>大数据</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器固定IP]]></title>
    <url>%2F2018%2F10%2F20%2FDocker%E5%AE%B9%E5%99%A8%E5%9B%BA%E5%AE%9AIP%2F</url>
    <content type="text"><![CDATA[Docker启动容器后一般是分配随机ip，本文将介绍如何使用Docker生成静态ip 创建自定义网络[root@hadoopmaster bin]# docker network create --subnet=172.18.0.0/16 my_network 037291f820f9104928d786bc83d123cc2a3dbf459816d4c3145e98faf97a348a [root@hadoopmaster bin]# docker network ls NETWORK ID NAME DRIVER SCOPE 0c3c0925f725 bridge bridge local bb4ac16f1205 host host local 037291f820f9 my_network bridge local 077d509d5c30 none null local 创建容器时指定ipdocker run -itd --net my_network --ip 172.18.0.100 --add-host hdp001:172.18.0.101 --add-host hdp002:172.18.0.102 -h hdpmaster --name hdpmaster -p 8088:8088 -p 50070:50070 cyanidehm/hadoop:0.3 /bin/bash docker run -itd --net my_network --ip 172.18.0.101 --add-host hdpmaster:172.18.0.100 --add-host hdp002:172.18.0.102 -h hdp001 --name hdp001 cyanidehm/hadoop:0.3 /bin/bash docker run -itd --net my_network --ip 172.18.0.102 --add-host hdp001:172.18.0.101 --add-host hdpmaster:172.18.0.100 -h hdp002 --name hdp002 cyanidehm/hadoop:0.3 /bin/bash &gt; --net 指定网络类型。 &gt; --ip 指定ip地址。 &gt; --add-host 添加主机到hosts文件。 &gt; -h 指定hostname主机名。 &gt; --name 指定容器名称]]></content>
      <categories>
        <category>容器</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>Docker</tag>
        <tag>IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keep-Class课程《跑步减脂不累腿的秘诀》笔记]]></title>
    <url>%2F2018%2F09%2F25%2FKeep-Class%E8%AF%BE%E7%A8%8B%E3%80%8A%E8%B7%91%E6%AD%A5%E5%87%8F%E8%84%82%E4%B8%8D%E7%B4%AF%E8%85%BF%E7%9A%84%E7%A7%98%E8%AF%80%E3%80%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[keep《跑步减脂不累腿的秘诀》–笔记 原文作者：Amazing_H链接：https://www.jianshu.com/p/894f942005c9转载來源：简书 怎么跑才能不粗腿腿部的肌肉和脂肪任何一样占比过大都会让腿在视觉上看起来粗壮。 但是，跑步作为一种高能量的有氧运动，是不会让腿长出脂肪的。 跑步作为一种耐力运动，从运动生理学的角度，也不会让肌肉有明显的增长。 为什么有很多人会认为跑步会让腿变粗呢？有两种原因： 客观上：不正确的跑步姿势，过多动员了大腿的肌肉，而不是臀部的肌肉。 主观上：跑步之后，腿部的疲劳感，会让你觉得跑步练到了腿，会把腿练粗，并且错误的姿势还会加深腿部的疲劳感，让你更怀疑腿变粗了。 想通过跑步减脂而又不累腿，解决的方法就是： 学习正确的跑步姿势，减少腿部的发力，增加臀部的发力。 臀部是下肢最发达的部位，是奔跑的发动力，学会动员臀部的发力，可以减少大腿发力的代偿。你的腿不容易累，就不会怀疑腿会变粗了。 但，新手的普遍状况是没有掌握正确的跑步姿势，臀部发力不足，导致腿部发力过多。比如：跑步时是向上跳着跑，导致了做功主要是克服重力，浪费了很多能量，腿会感觉非常累。 跑步时腿部并不是主要向前迈步或者向下踏步，我们应该向后摆腿部，这样能让力高效地推动身体向前进。 向后摆腿发力可以： 动员到臀部的力量，减少腿部发力 可以利用到鞭打效应，增加跑步时力学传递的效率 理想的跑步姿势，是向后摆腿跑，充分调动臀部发力，将力量传递到足部，推动身体向前，而且可以借助鞭打效应，向前的做功也会提升，让跑步更省力。 跑前热身 很多人虽然可以模仿向后甩腿的动作，但还是并不容易掌握臀部发力的感觉。 每次跑步前，通过以下两个动作，可以激活臀大肌，放松髂腰肌，感受到臀部的用力。再结合keep的《跑前热身课程》，可以有效预防损伤，让跑步过程更易达到最佳状态。 注： 臀大肌，在屁股两侧，主要负责发力，负责向后最初始的动力 髂腰肌，在大腿根部，足够放松，才能不阻碍腿向后摆 动作1--箭步蹲--这是激活臀大肌常用的动作--每侧15-30次，每次2-3组腰部挺直，两手叉腰，向前迈出一步，和自己腿长差不多相等的距离，下蹲成弓步，注意膝关节和髋关节呈90度。然后，想象自己后面的脚踩在冰面上，稍微一用力就会踩碎，靠前面的腿、臀部的力量推动自己身体站起来。这样可以帮你找到臀部发力的感觉，避免过多使用腿部的力量。 注意：做这个动作的时候，不要向前移动中心，或者前倾身体。或者后面的腿发力过多，这样都不利于找到臀部发力的感觉。 动作2--弓箭步拉伸--可以放松髂腰肌--每侧拉伸2次，每次60秒。 首先做一个弓箭步，让后面的腿放在地上，身体挺直，重心向前下方移动。然后用力挺胯，让大腿的根部得到拉伸。还可以让身体转向被拉伸的另外一侧，可以体会到更明显的拉伸感。 跑后放松 跑后是肌肉最需要冷身和放松的时候，跑后腿部肌肉拉伸或者按摩，可以避免疲劳累积到第二天，也可以有效缓解第二天的酸痛、不适等症状的出现。 以下两个跑后拉伸动作，结合keep的《跑后拉伸课程》和《小腿按摩课程》可以有效实现。 另外的两个动作： 动作1--髂胫束拉伸--可以缓解髂胫束的紧张造成的膝外侧疼痛--每侧30-60秒，每次1-2组 左脚向右脚前方交叉，身体倒向前偏左侧的方向，手摸到地面或者脚踝，膝盖保持平衡。体会拉伸大腿右外侧的部位。另外一侧的拉伸同理。 动作2--小腿拉伸--让小腿深层的肌肉得到充分地放松--每侧30-60秒，每次1-2组 伸直腿后，将被拉伸的腿向前移动40cm左右的距离，继续弯曲膝盖，中心下降拉伸小腿。体会靠近跟腱部位，及小腿深层的肌肉被拉伸。每一侧做30到60秒，重复1到2组。 注意：拉伸的过程中保持均匀呼吸，不用过分用力，体会肌肉被拉长的感觉即可。 怎么跑步不容易受伤在跑步姿势对的前提下，跑步时最简单有效的减脂运动方式之一。有效地慢跑每个小时可以消耗600-900大卡的热量，是单位时间里卡路里消耗最高的持续运动方式。但是，如果不注意跑步姿势和技巧，对身体的损伤则不堪设想。 正确的跑步姿势 一个远离受伤的跑步姿势，需要满足两个条件： 减小跑步时地面对双脚的冲击力 提高跑步时身体整体的稳定性 跑步有两种力量可能引起损伤： 是水平方向的扭转力，产生的研磨效应 是垂直方向的冲击力，产生的撞击效应 扭转力–提高身体的稳定性，可以有效减小扭转力 如果无法保证身体的稳定，会让我们的身体扭起来，会产生很多不必要的扭转力。扭转力比冲击力给关节的损伤更大，因为扭转力会对关节、韧带产生研磨效应，这种研磨效应发生在膝盖的髌骨就叫做髌骨软化，跑起来膝盖正前方就疼。研磨效应产生在髂胫束的末端，就叫髂胫束综合征，跑起来膝盖外侧就疼痛难忍。研磨效应还可能让半月板受损。 保持身体稳定 首先，保证躯干处于中立位置，调整头部，保持眼睛向水平方向看。想象自己想要量身高，尽可能向上伸展身体的感觉。还有感觉自己头顶有个苹果，不要让他掉下来。 其次，肩膀要放松，要让自己的肩膀有微微向后打开的感觉，并且保持整个躯干的竖立状态。 第三，收紧自己的腰腹，找到一种咳嗽，或者有人要打你腹部的感觉。 第四，保持自己的骨盆在中立位置上，不要前倾也不要后倾。 摆臂 摆臂是为了稳定身体。跑步的时候，腿部前后摆动会产生扭转的力矩，这个力矩会传递到上肢，刚好可以被手臂的摆动抵消掉，从而提高身体的稳定性。摆臂的时候，肘关节呈90度，肩膀放松，摆动自然。 通过保持躯干的中立位，以及自然的摆臂，就可以让身体从上到下保持稳定，避免产生多余的扭转力。 冲击力–减少冲击力的下半身模式 跑步每公里会产生几百上千次冲击，如果每次冲击力过大，可能会产生：胫骨前肌疼痛、足底筋膜炎、甚至是对关节造成伤害。 减少冲击力，需要一个良好的落地技巧 让落地点尽可能靠近身体的重心。经常有人跑步迈很大的步子，这样会让落地点超出身体很远，会产生一种刹车效应，每一步都会产生阻力，并增加落地时的冲击力。正确的落地方法，是让触地点，尽可能靠近身体重心的投影，有一种踩在重心上的感觉。 提高步频。提高步频可以减少身体腾空的时间，腾空的时间决定了腾空的高度，直接决定落地冲击力的大小 前脚掌落地和后脚跟落地的技巧。 落地最重要的是触地点落在身体的重心， 当速度低于12公里每小时，适合用脚跟先接触地面，这时如果用前脚掌接触地面，容易给小腿造成过多的压力。 当速度高于12公里每小时，适合用前脚掌先接触地面。 根据跑步的路况减少冲击力 塑胶跑道是最容易找到的适合跑步的路面，它有很好的缓冲减震效果，相当于一双高级跑鞋的效果。 公路硬度比较高，缓冲性能较强的跑鞋是必备的，另外，脚可以用一种滚动的感觉去落地，主动缓冲地面的冲击力，并且控制自己的脚步声尽可能的小。落地轻盈、声音小，就说明缓冲技巧控制的比较好。 跑步机和户外跑步的区别很大，户外跑是地面静止，腿推动身体向前移动。跑步机是履带向后，有一种带着人跑的感觉。使用跑步机跑步，需要有意的增加自己腿向后摆的幅度，尽可能与户外跑步保持一致。 远离损伤的跑步姿势和技巧： 稳定上身、自然摆臂、落地点靠近重心、用滚动的方式落地、保持一个高步频。 提升跑步速度与距离两大要领提升跑步速度 怎样跑步最省力 跑步不只是姿势那么简单，因为跑步不是静止的姿势，而是连贯的动作。前面两节通过静止的姿势讲解了正确的跑步姿势，静止的姿势是很容易模仿、实践的。但是如果想要跑更长时间，比如10公里，甚至是马拉松，就需要更高效的动作模式。 越跑越快、越跑越稳的动作模式应该是： 提高步频–推荐使用180每分钟的步频 步频就是每分钟的步数，高步频可以跑的更快，还可以提高稳定性，让腾空的时间更短，减少垂直方向重心的起伏。跑起来不仅可以更快，还能更省力。 提高步幅 步幅就是每一步的长度。提高步幅，并不是说步子迈的越大越好，有很多人想跑更快，就向前迈特别大的步子，这样子很容易让脚的落地点超越重心太远，产生刹车效应。 正确的提高步幅的方式是通过充分的蹬伸，提高每一步的步幅。 提高步幅的正确方式是，通过充分的蹬伸，提高每一步的步幅。这个过程可以拆分为四个阶段：落地、支撑、蹬伸、腾空。 落地：要想提高速度，落地要尽可能靠近重心的投影，落地的时候要注意膝盖自然弯曲，用肌肉的弹性做缓冲，把自己的双腿想象成弹簧，借助弹性，而不只是靠主动的发力去落地。 支撑：速度快的同时，支撑阶段也要尽可能地快。从双脚落地开始，迅速滚动到脚的中部做支撑。这个过程身体要保持稳定，保持手臂和腿的运动轨迹尽可能平行，不要出现交叉。 蹬伸：要想速度快，每一步需要有足够的力推动身体向前。有两个要点，一是以臀部为轴（臀部为轴，是指臀部向后发力要充分），用鞭打的方式传递力量。第二是提拉后摆，想要把后蹬的力量转化为向前的力量，就需要我们在蹬伸的过程中，收缩腘绳肌，改变力的方向，尽可能的向后发力，避免力量转化为向上的力量浪费掉。 前摆：后摆最重要的是发力，前摆最重要的是省力。转动惯量和力矩成正比，滑冰运动员展开身体的时候速度会变慢，收紧身体的时候转动速度会变快。同样的道理，下肢折叠的越充分，向前摆动的时候就越省力。 跑步常见的困扰与危险信号跑步中经常会有以下感觉： 第一种感觉叫极点，一般出现在跑步的10-15分钟左右，或者跑到第2-3公里的时候。这时候会感觉非常累、非常煎熬，冥冥之中有一种力量阻碍你迈不开腿、踹不上气，让你想要放弃。 冥冥之中的这个力量是内脏系统的阻力。在安静状态下，身体的血液是集中在躯干的内脏中。而在运动状态，身体的血液集中在四肢的肌肉当中。刚刚开始运动的时候，血液还没来得及从内脏到达肌肉，就会产生一种生理惰性，阻碍跑步，这就是极点现象，又叫惰性极点。当你继续运动一段时间，等待血液逐渐到达肌肉，就会跨过第一极点。 第二种感觉就是跑步完以后感觉腿疼。疼痛是身体给我们发出的信号，最常见的是延迟性肌肉酸痛。有这样一种疼法，刚跑完的时候，你并没有什么感觉，而是运动后一天活第二天开始感觉疼痛，疼的时候可能会感觉上楼都困难。这种疼痛的特点是两条腿都疼，是对称的，痛点主要在肌肉上面。延期性肌肉酸痛是肌肉进行大量运动刺激后，进行自我更新的结果，肌纤维正在重建。这种疼痛一般会在一周之后消退。如果进行低强度的运动，比如走路、慢跑、拉伸之后，都可以促进疼痛更快恢复。 第三种是跑完后感觉腿粗了一圈。这种感觉不用担心，是会自行恢复的。人体的80%是水，我们的身体，特别是肌肉，受到体液分布的影响很大。安静的时候血液会在内脏、大脑当中。跑步中和跑步后，血液会集中在下肢，导致暂时的体液分布型水肿，可能会持续1-3天。在跑步后，平躺并通过手臂的辅助做一些腿部后侧的拉伸，每侧3分钟左右，可以让这种暂时的水肿很快恢复。或者通过泡沫轴滚压、按摩的方式也可以减轻水肿的现象 还有可能出现这种危险信号：运动中单腿越跑越疼。 随着运动的持续，一条腿越来越疼，并且疼痛出现在关节，这时候要立刻停止运动，如果停止运动后，症状继续加重，就需要寻求医生的治疗了。 危险信号的出现，往往是因为姿势的不正确，比如： 步子迈太大，每迈一步，就是一次刹车效应，损耗能量不说，还有可能将肌肉拉扯疼痛。 身体前倾，这样会给后背，尤其是腰部造成过重的负担，容易产生腰痛。跑步的时候让整个身体保持重心向前，微微前倾5度左右就可以了。 用脚尖着地，课程中所说的前脚掌不是脚尖，而是整个脚掌前二分之一的部位。如果用脚尖 跑步，会让脚趾关节受到很大的冲击力造成损伤。 横向摆臂，这样不仅不会稳定身体，反而可能产生更多的扭转力，身体的不稳定更容易引起岔气状况的出现。 膝盖内扣，女生常出现这种问题，这样可能导致膝关节的损伤。 几乎所有人都会觉得跑步挺简单的，但是跑步是一项周期性的运动。相同的一个动作，需要重复重复再重复，动作中小的错误，也可能积累起来给你造成困扰，跑步姿势有很多可以优化和改进的地方。如果想避免危险的疼痛，除了保证跑步的姿势正确，还需要注意训练总量的控制。每周的跑步总量不要超过上一周的10%，比如第一周10公里，第二周尽量不要超过11公里。跑步是一个循序渐进的过程，需要不断地练习才能越跑越瘦、越跑越远。]]></content>
      <categories>
        <category>运动</category>
        <category>跑步</category>
      </categories>
      <tags>
        <tag>跑步</tag>
        <tag>Keep</tag>
        <tag>运动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker替换镜像源与常用命令]]></title>
    <url>%2F2018%2F09%2F18%2FDocker%E6%9B%BF%E6%8D%A2%E9%95%9C%E5%83%8F%E6%BA%90%E4%B8%8E%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Docker替换镜像源与常用命令 安装CentOS7下直接运行yum -y install docker 查看是否安装成功docker version或者docker info docker info显示内容需要启动docker服务才能看见 启动docker服务service docker start或者systemctl start docker 设置开机自启动systemctl enable docker 替换为国内镜像源修改或新增/etc/docker/daemon.json文件{ "registry-mirrors": ["http://hub-mirror.c.163.com"] } 修改之后重启一下docker服务 systemctl restart docker.service 或者 service docker restart Docker国内源Docker 官方中国区https://registry.docker-cn.com 网易http://hub-mirror.c.163.com 中国科技大学https://docker.mirrors.ustc.edu.cn 阿里云https://pee6w651.mirror.aliyuncs.com 基本命令列出本地所有image文件docker images 或者docker image ls 删除本地镜像docker image rm [镜像名] 拉取镜像docker image pull [镜像组/镜像名] 运行镜像生成容器docker container run [镜像名]或者docker run [镜像名] 如果本地没有该镜像，会自动去仓库pull 终止容器docker container kill [容器id]或者docker kill [容器id]或者docker stop [容器id] 制作docker容器步骤 编写Dockerfile文件 # 该 image 文件继承官方的 node image，冒号表示标签，这里标签是8.4，即8.4版本的 node。 FROM node:8.4 # 创建者信息 MAINTAINER cyanidehm # 将当前目录下的所有文件（除了.dockerignore排除的路径），都拷贝进入 image 文件的/app目录。 COPY . /app # 指定接下来的工作路径为/app。 WORKDIR /app # 在/app目录下，运行npm install命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。 RUN npm install --registry=https://registry.npm.taobao.org # 将容器 3000 端口暴露出来， 允许外部连接这个端口。 EXPOSE 3000 编写.dockerignore文件 .git node_modules npm-debug.log # 表示上面三个路径会排除，不打进image文件 创建image文件 有了 Dockerfile 文件以后，就可以使用docker image build命令创建 image 文件了。 $ docker image build -t demo . # 或者 $ docker image build -t demo:0.0.1 . # -t参数用来指定 image 文件的名字（该例中demo为image名），后面还可以用冒号指定标签。如果不指定，默认的标签就是latest。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。 生成image之后就可以使用docker images查看到了 生成容器 $ docker container run -p 8000:3000 -itd --name my_demo -h master -v /opt/java:/home/java --privileged=true demo:0.0.1 /bin/bash -p参数：容器的 3000 端口映射到本机的 8000 端口。-it参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。-d参数：容器后台运行。–name参数：表示生成的容器名称，这里为my_demo。-h参数：表示生成的容器主机名，这里为master。-v参数：表示主机地址/opt/java和容器中地址/home/java映射，上传到/opt/java目录就能同步上传到容器内。demo:0.0.1：镜像文件的名字（如果有标签，还需要提供标签，这里标签为0.0.1，如果不提供，默认是 latest 标签）。/bin/bash：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。--privileged: CentOS7中安全模块selinux会把容器读写权限禁掉，添加该参数赋予容器权限，也可以禁用CentOS7的selinux模块。 将运行的容器打包成镜像 登录docker hub网站注册账号。 https://hub.docker.com/ docker login命令登录，输入相应用户名和密码 Username: cyanidehm Password: Login Succeeded # 表示登录成功 使用docker ps查看当前运行的容器 [root@hadoopCDH opt]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9bffe3a2142e centos "/bin/bash" About an hour ago Up About an hour vigilant_dijkstra 得到容器id：9bffe3a2142e 使用docker commit 9bffe3a2142e my_centos命令提交到本地镜像，my_centos为镜像名（自己取名） docker commit [OPTIONS] [容器id或名称] [镜像名称：版本]，OPTIONS选项包括：-a，–author=””作者信息。-m，–message=””提交信息。-p，–pause=true提交时暂停容器运行。 查看本地镜像 [root@hadoopCDH opt]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE my_centos latest bcc2cf471c38 11 seconds ago 400 MB 将镜像改到自己账户名下，推送到docker hub [root@hadoopCDH opt]# docker tag my_centos cyanidehm/my_centos [root@hadoopCDH opt]# docker push cyanidehm/my_centos:latest 其他命令查看容器 docker ps查看正在运行的容器。 docker ps -a或docker container ls --all查看所有存在的容器。 退出容器bash 在容器的命令行，按下 Ctrl + c 停止 Node 进程，然后按下 Ctrl + d （或者输入 exit）退出容器。此外，也可以用docker container kill终止容器运行。 删除容器文件 容器停止运行后，不会消失，使用docker container ls --all查看所有存在的容器（id等信息）。 使用docker container rm [容器id]或者docker rm [容器id]删除容器。 清除所有容器 停止所有容器docker stop $(docker ps -aq) 删除所有容器docker rm $(docker ps -aq) 运行已存在的容器 docker container start [容器id]或者docker start [容器id] 进入已经运行的容器 docker container exec -it [容器id] [/bin/bash] 将容器里的文件拷贝到本机 docker container cp [容器id]:[/path/to/file] . 将镜像保存为tar文件、将tar文件加载到docker镜像 保存镜像docker save -o [路径/文件名] [镜像名]或者docker save [镜像名] &gt; [路径/文件名]```bash[root@hadoopCDH opt]# docker save -o my_centos.tar cyanidehm/my_centos:latest[root@hadoopCDH opt]# ll my_centos.tar -rw——-. 1 root root 779944960 10月 17 03:40 my_centos.tar &gt; 可以通过指定打包格式来打包成压缩文件： &gt; `docker save [镜像名] | gzip &gt; [路径/文件名.tar.gz]` 2. 加载镜像 `docker load --input [路径/文件名]`或者`docker load &lt; [路径/文件名]` ```bash [root@hadoopmaster opt]# docker load &lt; my_centos.tar 1d31b5806ba4: Loading layer [==================================================&gt;] 208.3 MB/208.3 MB a5789abfb72a: Loading layer [==================================================&gt;] 571.6 MB/571.6 MB Loaded image: cyanidehm/my_centos:latest [root@hadoopmaster opt]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE cyanidehm/my_centos latest 3ced2987d19a Less than a second ago 765 MB 批量save/load镜像脚本地址hydra1983/docker_images.sh批量保存./docker_images.sh save-images批量加载./docker_images.sh load-images 脚本将镜像文件保存到./images目录下，另外一个images.db文件与之对应将这两个东西拷贝到其他主机，执行批量加载命令就行实现批量转移镜像 [root@localhost save]# ./docker_images.sh save-images Create /opt/save/images.db Read /opt/save/images.db Create /opt/save/images [DEBUG] save 00ead811e8ae docker.io/portainer/portainer:latest to /opt/save/images/00ead811e8ae.dim real 0m27.343s [DEBUG] save d63b9b4bd205 rancher/server:v1.6.14 to /opt/save/images/d63b9b4bd205.dim real 4m46.480s [DEBUG] save 34a453d374b9 docker.io/rancher/agent:v1.2.9 to /opt/save/images/34a453d374b9.dim real 0m28.037s [root@localhost save]# ./docker_images.sh load-images… #### 容器启动后自动运行脚本 情景：1、镜像已经存在。2、镜像内包含脚本`/home/ssh.sh`需要在容器启动后运行 ```bash docker run -itd cyanidehm/base_ssh /bin/bash -c &quot;sh /home/ssh.sh;/bin/bash&quot; 说明：/bin/bash -c &quot;&quot;表示容器运行后使用bash执行引号内语句。引号内的 ; 表示命令分割，执行多条命令时用;进行分割引号内最后的/bin/bash表示容器启动以bash方式运行（如果容器启动后没有线程在运行，容器会停止退出） 查看容器相关信息在容器外面不进入容器查看容器信息docker inspect [容器名/id]：查看到容器的相关信息 # 查看容器的具体IP地址，如果输出是空的说明没有配置IP地址 docker inspect --format '{{ .NetworkSettings.IPAddress }}' [容器名/id]]]></content>
      <categories>
        <category>容器</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>容器</tag>
        <tag>Docker</tag>
        <tag>命令</tag>
        <tag>镜像源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7下搭建Hadoop集群]]></title>
    <url>%2F2018%2F09%2F16%2FCentOS7%E4%B8%8B%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[CentOS7下搭建Hadoop集群 本文环境 节点 IP地址 hadoopmaster 192.168.171.10 hadoop001 192.168.171.11 hadoop002 192.168.171.12 ### 准备环境 #### 虚拟机创建多个Linux系统并配置静态IP 配置静态IP教程请点击这里 #### 配置DNS（每个节点） 进入配置文件，添加主节点和从节点的映射关系 vim /etc/hosts，添加如下代码（ip以及主机名以自己配置为准） ```shell 192.168.171.10 hadoopmaster 192.168.171.11 hadoop001 192.168.171.12 hadoop002 ``` #### 关闭防火墙（每个节点） 关闭服务 systemctl stop firewalld 关闭开机自启动 systemctl disable firewalld #### 配置免密码登录 配置免密码登录教程请点击这里 #### 配置java环境（每个节点） 配置java环境教程点击这里 ### 搭建Hadoop完全分布式集群 在各个节点上安装与配置Hadoop的过程都基本相同，因此可以在每个节点上安装好Hadoop后，在主节点master上进行统一配置，然后通过scp 命令将修改的配置文件拷贝到各个从节点上即可。 下载hadoop安装包，解压，配置环境变量点击这里选择适合的版本进行安装包下载找个目录（本文为/opt目录），rz 命令上传到Linux系统 解压tar -zxvf hadoop-*.tar.gz配置环境变量（每个节点）vim /etc/profile添加如下代码 export HADOOP_HOME=/opt/hadoop-2.7.3 # 该目录为解压安装目录 export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop 完成后刷新一下，使profile生效 [root@hadoopmaster ~]# source /etc/profile 配置环境脚本文件的JAVA_HOME参数进入hadoop安装目录下的etc/hadoop目录分别在hadoop-env.sh、mapred-env.sh、yarn-env.sh文件中添加或修改参数： export JAVA_HOME=&quot;/opt/jdk1.8&quot; # 路径为jdk安装路径修改配置文件 hadoop安装目录下的etc/hadoop目录,一共需要修改core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml、slaves(3.0之后为workers)文件，按实际情况修改配置信息 1.core-site.xml更多参数配置参考：core-default.xml &lt;configuration> &lt;property> &lt;!-- 配置hdfs地址 --> &lt;name>fs.defaultFS&lt;/name> &lt;value>hdfs://hadoopmaster:9000&lt;/value> &lt;/property> &lt;property> &lt;!-- 保存临时文件目录 --> &lt;name>hadoop.tmp.dir&lt;/name> &lt;value>/opt/hadoop/tmp&lt;/value> &lt;/property> &lt;property> &lt;name>hadoop.proxyuser.root.hosts&lt;/name> &lt;value>*&lt;/value> &lt;/property> &lt;property> &lt;name>hadoop.proxyuser.root.groups&lt;/name> &lt;value>*&lt;/value> &lt;/property> &lt;/configuration> 2.hdfs-site.xml更多参数配置参考：hdfs-default.xml &lt;configuration> &lt;property> &lt;!-- 主节点地址 --> &lt;name>dfs.namenode.http-address&lt;/name> &lt;value>hadoopmaster:50070&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.namenode.name.dir&lt;/name> &lt;value>file:/opt/hadoop/dfs/name&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.datanode.data.dir&lt;/name> &lt;value>file:/opt/hadoop/dfs/data&lt;/value> &lt;/property> &lt;property> &lt;!-- 备份份数 --> &lt;name>dfs.replication&lt;/name> &lt;value>2&lt;/value> &lt;/property> &lt;property> &lt;!-- 第二节点地址 --> &lt;name>dfs.namenode.secondary.http-address&lt;/name> &lt;value>hadoop001:9001&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.webhdfs.enabled&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;property> &lt;name>dfs.permissions&lt;/name> &lt;value>false&lt;/value> &lt;description>配置为false后，可以允许不要检查权限就生成dfs上的文件，方便倒是方便了，但是你需要防止误删除.&lt;/description> &lt;/property> &lt;/configuration> 3.mapred-site.xml更多参数配置参考：mapred-default.xml &lt;configuration> &lt;property> &lt;name>mapreduce.framework.name&lt;/name> &lt;value>yarn&lt;/value> &lt;/property> &lt;property> &lt;name>mapreduce.jobhistory.address&lt;/name> &lt;value>hadoopmaster:10020&lt;/value> &lt;/property> &lt;property> &lt;name>mapreduce.jobhistory.webapp.address&lt;/name> &lt;value>hadoopmaster:19888&lt;/value> &lt;/property> &lt;/configuration> 4.yarn-site.xml（资源管理器）更多参数配置参考：yarn-default.xml &lt;configuration> &lt;property> &lt;name>yarn.nodemanager.aux-services&lt;/name> &lt;value>mapreduce_shuffle&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name> &lt;value>org.apache.hadoop.mapred.ShuffleHandler&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.address&lt;/name> &lt;value>hadoopmaster:8032&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.scheduler.address&lt;/name> &lt;value>hadoopmaster:8030&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.resource-tracker.address&lt;/name> &lt;value>hadoopmaster:8031&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.admin.address&lt;/name> &lt;value>hadoopmaster:8033&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.resourcemanager.webapp.address&lt;/name> &lt;value>hadoopmaster:8088&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.nodemanager.resource.memory-mb&lt;/name> &lt;!-- NodeManage中的配置，这里配置过小可能导致nodemanager启动不起来 大小应该大于 spark中 executor-memory + driver的内存 --> &lt;value>6144&lt;/value> &lt;/property> &lt;property> &lt;!-- RsourceManager中配置 大小应该大于 spark中 executor-memory + driver的内存 --> &lt;name>yarn.scheduler.maximum-allocation-mb&lt;/name> &lt;value>61440&lt;/value> &lt;/property> &lt;property> &lt;!-- 使用核数 --> &lt;name>yarn.nodemanager.resource.cpu-vcores&lt;/name> &lt;value>2&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.log-aggregation-enable&lt;/name> &lt;value>true&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.log-aggregation.retain-seconds&lt;/name> &lt;value>604800&lt;/value> &lt;/property> &lt;property> &lt;name>yarn.nodemanager.vmem-check-enabled&lt;/name> &lt;value>false&lt;/value> &lt;discription>忽略虚拟内存的检查，如果你是安装在虚拟机上，这个配置很有用，配上去之后后续操作不容易出问题。&lt;/discription> &lt;/property> &lt;property> &lt;!-- 调度策略，设置为公平调度器 --> &lt;name>yarn.resourcemanager.scheduler.class&lt;/name> &lt;value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value> &lt;/property> &lt;/configuration> 5.slaves文件（3.0之后为workers文件）# 增加从节点地址（这里由于配置了hosts，直接使用主机名，也可以配ip地址） hadoop001 hadoop002将文件夹copy到其他子节点通过scp 命令将修改好的文件夹拷贝到各个从节点上 [root@hadoopmaster ~]# scp -r /opt/hadoop/ root@hadoop001:/opt ... [root@hadoopmaster ~]# scp -r /opt/hadoop/ root@hadoop002:/opt 初始化、启动[root@hadoopmaster hadoop]# bin/hdfs namenode -format 全部启动sbin/start-all.sh，也可以分开sbin/start-dfs.sh、sbin/start-yarn.sh启动 报错：Starting namenodes on [hadoopmaster]ERROR: Attempting to operate on hdfs namenode as rootERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.Starting datanodesERROR: Attempting to operate on hdfs datanode as rootERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.Starting secondary namenodes [hadoop001]ERROR: Attempting to operate on hdfs secondarynamenode as rootERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.Starting resourcemanagerERROR: Attempting to operate on yarn resourcemanager as rootERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.Starting nodemanagersERROR: Attempting to operate on yarn nodemanager as rootERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation. 原因：是因为缺少用户定义造成的，所以分别编辑开始和关闭脚本$ vim sbin/start-dfs.sh$ vim sbin/stop-dfs.sh在顶部空白处添加内容：HDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root start-yarn.sh，stop-yarn.sh顶部也需添加以下：YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root Web访问，要先开放端口或者直接关闭防火墙 关闭防火墙 # 查看防火墙状态 firewall-cmd --state # 临时关闭 systemctl stop firewalld # 禁止开机启动 systemctl disable firewalld 浏览器打开http://hadoopmaster:8088/ 浏览器打开http://hadoopmaster:50070/ 单例管理每个节点$> sbin/hadoop-daemon.sh start datanode # 启动数据节点 $> sbin/yarn-daemon.sh start nodemanager # 启动数据管理节点 $> bin/hadoop-daemon.sh start tasktracker # 启动任务管理器 yarn application命令介绍yarn application后接参数： -list 列出所有application信息 [root@master ~]# yarn application -list -appStates &lt;States&gt; 跟 -list 一起使用，用来筛选不同状态的 application，多个用”,”分隔； 所有状态：ALL, NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED [root@master ~]# yarn application -list -appStates RUNNING -appTypes &lt;Types&gt; 跟 -list 一起使用，用来筛选不同类型的 application，多个用”,”分隔； 如 MAPREDUCE,TEZ [root@master ~]# yarn application -list -appTypes MAPREDUCE -kill &lt;Application ID&gt; 杀死一个 application，需要指定一个 Application ID [root@master ~]# yarn application -kill application_1526100291229_206393 -status &lt;Application ID&gt; 列出 某个application 的状态 [root@master ~]# yarn application -status application_1526100291229_206393 -movetoqueue &lt;Application ID&gt; 移动 application 到其他的 queue，不能单独使用 -queue &lt;Queue Name&gt; 与 movetoqueue 命令一起使用，指定移动到哪个 queue [root@master ~]# yarn application -movetoqueue application_1526100291229_206393 -queue other]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>集群</tag>
        <tag>Linux</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux集群配置免密码登录]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Linux集群配置免密码登录 本文环境： 节点 IP地址 hadoopmaster 192.168.171.10 hadoop001 192.168.171.11 hadoop002 192.168.171.12 原理：每台主机authorized_keys文件里面包含的主机（ssh秘钥），该主机都能无密码登录，所以只要每台主机的authorized_keys文件里面都放入其他主机（需要无密码登录的主机）的ssh秘钥就行了。 配置每个节点的hosts文件vim /etc/hosts编辑hosts文件，添加如下代码 192.168.171.10 hadoopmaster 192.168.171.11 hadoop001 192.168.171.12 hadoop002 每个节点生成ssh秘钥[root@hadoopmaster ~]# ssh-keygen -t rsa # 执行命令生成秘钥 ... [root@hadoopmaster .ssh]# ls id_rsa id_rsa.pub 执行命令后会在~目录下生成.ssh文件夹，里面包含id_rsa和id_rsa.pub两个文件。 执行生成秘钥命令时会让用户选择生成地址，如果想直接使用默认地址（不想交互），则可以使用ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa命令 将每个节点的id拷贝到所有节点的authorized_keys中方法一（推荐）每个节点执行ssh自带命令，将该节点的id拷贝到其他节点中[root@hadoopmaster /]# ssh-copy-id hadoopmaster [root@hadoopmaster /]# ssh-copy-id hadoop001 [root@hadoopmaster /]# ssh-copy-id hadoop002 [root@hadoop001 /]# ssh-copy-id hadoopmaster [root@hadoop001 /]# ssh-copy-id hadoop001 [root@hadoop001 /]# ssh-copy-id hadoop002 [root@hadoop002 /]# ssh-copy-id hadoopmaster [root@hadoop002 /]# ssh-copy-id hadoop001 [root@hadoop002 /]# ssh-copy-id hadoop002 方法二在主节点上将公钥拷贝到一个特定文件中[root@hadoopmaster /]# cd ~/.ssh [root@hadoopmaster .ssh]# cp id_rsa.pub authorized_keys # 拷贝到authorized_keys文件中 [root@hadoopmaster .ssh]# ls authorized_keys id_rsa id_rsa.pub 将authorized_keys文件拷贝至下一个节点，并将该节点的ssh秘钥加入该文件中[root@hadoopmaster .ssh]# scp authorized_keys root@hadoop001:/root/.ssh/ root@hadoop001's password: # 此时会提示输入密码，输入hadoop001主机root密码即可 authorized_keys 100% 399 450.9KB/s 00:00 # 进入001主机 [root@hadoop001 /]# cd ~/.ssh [root@hadoop001 .ssh]# ls authorized_keys id_rsa id_rsa.pub [root@hadoop001 .ssh]# cat id_rsa.pub>>authorized_keys # 使用cat追加到authorized_keys文件 ssh-rsa AAAAB.....TnYjJ root@hadoop001 ssh-rsa AAAAB.....Ah+n9 root@hadoopmaster [root@hadoop001 .ssh]# 关于scp命令请点击这里查看 重复上一步动作，将每个节点的ssh秘钥都加入authorized_keys文件中将最后节点生成的authorized_keys文件复制到每个节点下即可[root@hadoop002 .ssh]# scp authorized_keys root@hadoopmaster:/root/.ssh ... [root@hadoop002 .ssh]# scp authorized_keys root@hadoop001:/root/.ssh ... 测试登录使用ssh 用户名@节点名（或ip地址）命令进行无密码登录测试 [root@hadoopmaster .ssh]# ssh root@hadoop001 Last login: Sun Sep 16 17:51:27 2018 from 192.168.171.1 [root@hadoop001 ~]# ssh root@hadoop002 Last login: Sun Sep 16 17:51:31 2018 from 192.168.171.1 [root@hadoop002 ~]# ssh root@hadoopmaster Last login: Sun Sep 16 17:51:23 2018 from 192.168.171.1 [root@hadoopmaster ~]#]]></content>
      <categories>
        <category>Linux</category>
        <category>SSH</category>
      </categories>
      <tags>
        <tag>集群</tag>
        <tag>Linux</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下安装Java]]></title>
    <url>%2F2018%2F09%2F14%2FLinux%E4%B8%8B%E5%AE%89%E8%A3%85Java%2F</url>
    <content type="text"><![CDATA[Linux下安装Java 将Java压缩包传到Linux使用rz命令将tar包上传到Linux系统 关于rz命令，点击查看介绍 解压安装包[root@hadoopmaster mnt]# ls jdk-8u101-linux-x64.tar.gz [root@hadoopmaster mnt]# tar -zxvf jdk-8u101-linux-x64.tar.gz ... jdk1.8.0_101/man/ja_JP.UTF-8/man1/javapackager.1 jdk1.8.0_101/man/ja_JP.UTF-8/man1/jstat.1 [root@hadoopmaster mnt]# ls jdk1.8.0_101 jdk-8u101-linux-x64.tar.gz [root@hadoopmaster mnt]# 配置环境变量 vim /etc/profile编辑配置文件，添加如下代码 # java export JAVA_HOME=/home/jdk # 该路径为java安装路径 export CLASSPATH=$JAVA_HOME/lib/ export PATH=$PATH:$JAVA_HOME/bin 保存后退出 source /etc/profile刷新配置文件 验证安装状态[root@hadoopmaster mnt]# java -version java version "1.8.0_101" Java(TM) SE Runtime Environment (build 1.8.0_101-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode) [root@hadoopmaster mnt]#]]></content>
      <categories>
        <category>Linux</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据选型]]></title>
    <url>%2F2018%2F09%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%80%89%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[大数据框架选型 各个框架优缺点 名称 优势和特点 不足 备注 HDFS 1、高扩展、低成本、成熟的生态圈2、处理超大文件（GB、TB甚至PB级别）3、运行于廉价的商用机器集群上4、高容错性和高可靠性（一份文件可以储存多份） 1、不适合低延迟数据访问（访问延迟高，HBase补足）2、频繁的读写会对NameNode造成很大压力（采用Kafka过渡）3、不支持多用户写入和随机文件修改（只能在文件末尾进行追加操作） 用作Hbase和Hive的物理储存 HBase 1、HDFS高延迟数据访问的一种改进方式，适合实时查询2、可以提供高并发读写操作的支持3、可以动态增加列，并且列为空就不存储数据，节省存储空间4、可以自动切分数据，使得数据存储自动具有水平扩展功能 1、只支持简单的Row Key条件查询（除非和其他框架一起使用，例如：Phoenix、Hive）2、不能支持Master server的故障切换，当Master宕机后，整个存储系统就会挂掉（可以使用Zookeeper进行多Master待命，当工作的Master宕机，则推选待命的Master继续工作）3、只支持Java API（除非和其他框架一起使用，例如：Phoenix、Hive） 应用场景：不断插入新的信息，而不修改。不需要复杂查询条件来查询数据底层依赖HDFS来作为其物理储存（特殊情况可以使用本机文件系统） Hive 1、提供类SQL查询语言HQL2、可扩展3、避免去写MR，减少开发人员学习成本4、提供统一的元数据(Meta store)管理（推荐使用MySQL储存元数据），可以和impala/spark等共享元数据 1、HQL表达能力有限2、效率比较低，延迟较高（Hive用MR作为计算引擎，HDFS作为储存系统），自动生成的MR作业通常情况下不够智能化3、调优比较困难，粒度较粗 应用场景：不支持实时查询，一般用作对一段时间内的数据（海量数据）进行分析查询（数据分析），日志分析，海量结构化数据离线分析等底层依赖HDFS来作为其物理储存可以通过Hive操作HBase数据进行计算分析，但是速度很慢元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列、分区字段、表的类型（是否是外部表）、表的数据所在的目录等 Zookeeper 1、配置管理（使用Zab一致性协议提供一致性），动态管理HBase、Hive等的配置信息2、名字服务，提供类似DNS的方式映射主机3、分布式锁（Leader Election），HBase的Master选举就是这种方式4、集群管理，动态感知集群状态，对节点的新增和删除作动态处理 1、选举过程很慢，当Zookeeper的master宕机，选举新的master通常消耗30到120秒2、性能有限，TPS（Transactions Per Second每秒传输的事务处理个数）大概一万多，单个节点平均连接数是6K，watcher是30万，吞吐似乎还可以，但是时延就没那么乐观了，特别是响应时间、网络、缓存3、无法进行有效的权限控制，在大型的复杂系统里面，使用zookeeper必须自己再额外的开发一套权限控制系统，通过那套权限控制系统再访问zookeeper4、API使用复杂5、回调次数限制，ZK中所有Watch回调通知都是一次性的6、由于性能的限制，导致的业务系统的数据不一致 1、HBase Master 的 HA(High Available) 解决方案2、已经成为了分布式大数据框架中容错性的标准框架，几乎所有的分布式大数据相关的开源框架，都依赖于 Zookeeper 实现 HA Yarn（统一资源管理系统） 1、提高资源（内存、IO、网络、磁盘等）利用率2、能够支持不同的计算框架，可以跑Hadoop、Storm、Spark程序3、将资源管理和作业控制分离，减小JobTracker（整个MapReduce计算框架中的主服务）压力 1、各个应用无法感知集群整体资源的使用情况，只能等待上层调度推送信息2、资源分配采用轮询、ResourceOffer机制（mesos)，在分配过程中使用悲观锁，并发粒度小3、缺乏一种有效的竞争或优先抢占的机制 和Zookeeper的区别：Yarn相当于政府，负责管理机器资源的分配；Zookeeper相当于立法委员会,负责保持信息的一致 Kafka 1、实时性比直接将数据采集到HDFS高2、可以将一条数据提供给多个接收者做不同的处理3、消费点（数据读写记录）记录在Zookeeper中，有序且容易维护4、可扩展、高性能，支持Batch操作 1、复杂性，Kafka需要Zookeeper的支持，Topic一般需要人工创建，部署和维护比一般MQ成本更高2、不支持事务 1、可以使用Zookeeper来维护broker信息，实现HA Spark 1、MapReduce的替代方案，可以融入Hadoop生态系统（兼容HDFS、Hive和HBase等分布式储存层）2、支持复杂查询。在简单的“map”及“reduce”操作之外，Spark还支持SQL查询、流式计算、机器学习和图算法3、轻量级快速处理4、支持多语言、社区活跃度高5、可以直接对HDFS进行数据读写，支持YARN等部署模式 1、没有自带的分布式储存系统（可用HDFS、Hive和HBase等分布式储存层）2、处理数据量太大时容易造成内存问题（一般超过1T建议使用MR）3、JVM的内存overhead太大，1G的数据通常需要消耗5G的内存 与MapReduce的主要区别：1、Spark的大部分操作都是在内存中，速度较快，而Hadoop的MapReduce系统会在每次操作之后将所有数据写回到物理存储介质上，速度较慢，为了确保在出现问题时能够完全恢复。但Spark的弹性分布式数据存储也能实现这一点。2、在高级数据处理（如实时流处理和机器学习）方面，Spark的功能要胜过Hadoop MR和Flink一样可以部署在Yarn上 Flink 1、支持增量迭代计算，具有对迭代自动优化的功能，性能优于Spark，在迭代次数增加后尤为明显2、在流式计算方面支持毫秒级计算（和Storm一样），优于Spark流式处理的秒级计算 1、SQL的支持没有Spark好，Spark还支持对SQL的优化2、社区活跃度没有Spark高 在流数据处理方面，Spark数据处理采用的是分批处理模式而Flink采用的是实时处理模式。Spark处理由数据块构成的RDD(弹性分布式数据集) ，而Flink能够实时处理一行又一行的数据。因此，无论进行何种设置，Spark总是会存在一定的数据迟延，而Flink不会有这种情况。 Apache Beam 1、统一数据批处理（Batch）和流处理（Stream）编程的范式2、能运行在任何可执行的引擎之上 1、国内社区活跃度低，网上实战资料很少，基本都是官网文档翻译2、stackoverflow上仅有1156个问题，scala则有82,463个提问 与Spark的Scala开发库比较：1、Apache Beam提供的基础函数支持不及Scala，例如排序，Beam必须构建一个二维元组才可以排序，而且只能升序2、相同功能的代码量上来说，Scala远小于Apache Beam，Apache Beam开发效率相比Scala会低一点3、Scala是脚本语言，代码格式比较随意，如果开发不规范，维护成本会比较高 Kubernetes、Mesos以及Swarm对比 要比较的方面 Kubernetes Mesos Swarm 定位 专注于大规模容器集群管理。从Service 的角度定义微服务化的容器应用。整个框架考虑了很多生产中需要的功能，比如Proxy、ServiceDNS、LivenessProbe等，基本上不用经过二次开发就能应用到生产环境中 是一个分布式系统内核，将不同类型的主机组织在一起当作一台逻辑计算机。专注于资源的管理和任务调度，并不针对容器管理。Mesos上所有的应用部署都需要有专门的框架支撑，例如若要支撑Docker, 则必须安装Marathon;在安装Spark和Hadoop时需要不同的框架 是目前Docker社区原生支持的集群工具，它通过扩展DockerAPI,力图让用户像使用单机Docker API一样驱动整个集群 对容器的支持 天生针对容器和应用的云化，通过微服务的理念对容器进行服务化包装 支撑Docker,必须安装Marathon框架。只关注对应用层资源的管理，其余由框架完成 原生支持Docker,使用标准的Docker API， 任何使用Docker AP1与Docker进行通信的工具都可以无缝地和Swarm协同工作 对资源的控制 本身具备资源管控能力，可以控制容器对资源的调用 Mesos将所有的主机虚拟成一个大的CPU、内存池，可以定义资源分配，也可以动态调配 在Swarm集群下可以设置参数或编排模板对应用进行资源限制 是否支持资源分区 能通过Namespace和Node进行集群分区，控制到主机、CPU和内存 支持资源分区，可以定义CPU、内存、磁盘等 通过将集群分成具有不同属性的子集群来创建逻辑集群分区 开发成本 原生集成了Service Proxy 、Service DNS，在应用实例动态扩展时实时更新Proxy的转发规则。基本上没有二次开发成本，而且便于多集群的集成 要实现生产应用，需要增加很多功能，例如HA Proxy Service DNS等，需要自己实现集群扩展和Proxy的集成。二次开发成本高，需要专业的实施团队 由于对外提供完全标准的Docker API，所以只需理解Docker命令，用户就可以使用Swam集群，团队不需要有足够丰富的Linux 和分布式经验 非Docker应用的集成 不能实现Docker化的应用，可通过外部Service 方式集成到集群中 必须自行开发Framework 来集成到 Mesos中 通过外部Service方式集成到集群中]]></content>
      <categories>
        <category>大数据</category>
        <category>选型</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云操作系统基础架构选型]]></title>
    <url>%2F2018%2F08%2F18%2F%E4%BA%91%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%80%89%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[云作系统基础架构选型 ——by陶老师 为什么要有“云操作系统” 我自己的思考： 当有了大数据应用、用户应用集群时,我们如何高效地管理、协调、扩展集群中的应用，以及如何使应用本身高可用、使集群资源得到充分利用（弹性伸缩能力）? 引入 DCOS 数据中心操作系统（DCOS）是为整个数据中心提供分布式调度与协调功能，实现数据中心级弹性伸缩能力的软件堆栈，它将所有数据中心的资源当做一台计算机来调度。 主流有如下几种: Apache Mesos Apache Hadoop YARN Kubernetes DCOS 案例通过以下两个案例,我们应该就能清楚的了解到DCOS是什么、DCOS能做什么、DCOS能为我们带来什么收益了。 1.浙江移动浙江移动 讲述了 IT架构的演进、虚拟化问题以及实际场景应用。 2.天玑科技天玑科技 针对电信、银行、保险行业做的案例。 Apache mesos、Kubernetes 哪一个？ 为什么不考虑 Apache Hadoop YARN？在我看来，它比较适合大数据生态圈（hadoop和spark资源管理）,对于其他的生态圈不太友好，因此不考虑用来作为整个大平台的基础架构。 参考1有哪些是Apache Mesos能做到，而Kubernetes做不到的 社区，Mesos的社区比Kubernetes的小得多。Kubernetes得到了包括谷歌、英特尔、Mirantis、RedHat等在内的众多大公司的财务支持。Mesos主要由Mesosphere公司开发，并得到了苹果、微软等公司的支持 规模，Mesos从一开始就是专门面向大客户的。在Twitter、Apple、Verizon、Yelp和Netflix都有应用，并且在数千台服务器上运行了数十万个容器。Kubernetes总的来说：Mesos比较适合大型的公司，比较稳定。 参考2 实测 Kubernetes 和 Mesos 在高并发下的网络性能 并发测试，Kubernetes略好一点。 参考3Kubernetes和Mesos有啥区别，我该使用哪个好 节点，一万以下节点，Kubernetes 较好：开发、部署 开发难度，Mesos 开发难度较高，需要深度定制 参考4区块链与容器进阶应用发布会 阿里云开始提供Kubernetes服务 BaaS平台支持 总结现阶段总体来说，使用Kubernetes 是比较好的，二次开发较少，社区活跃，资料好查，发展快速，国内使用较多，带有谷歌光环。]]></content>
      <categories>
        <category>大数据</category>
        <category>选型</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>架构</tag>
        <tag>云操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2018%2F08%2F15%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用Linux命令 输入界面快捷操作（命令行快捷操作）操作方式及操作效果 操作方式 操作效果 Ctrl+k 剪切命令行中光标所在处之后的所有字符（包括自身） Ctrl+u 剪切命令行中光标所在处之前的所有字符（不包括自身） Ctrl+y 粘贴刚才所删除、剪切的字符 Ctrl+o 回车效果 Ctrl+j 回车效果 Ctrl+f 光标向后移动一个字符位,相当与-&gt;（右方向键） Ctrl+b 光标向前移动一个字符位,相当与&lt;-（左方向键） Alt+f 光标向后移动一个单词位 Alt+b 光标向前移动一个单词位 Ctrl+a 移动到当前行的开头 Ctrl+e 移动到当前行的结尾 Ctrl+l 清屏 清理缓存命令 说明 命令 查看内存使用情况 free -h 仅清除页面缓存（PageCache） echo 1 &gt; /proc/sys/vm/drop_caches 清除目录项和inode echo 2 &gt; /proc/sys/vm/drop_caches 清除页面缓存，目录项和inode echo 3 &gt; /proc/sys/vm/drop_caches 每个 Linux 系统有三种选项来清除缓存而不需要中断任何进程或服务。 （LCTT 译注：Cache，译作“缓存”，指 CPU 和内存之间高速缓存。Buffer，译作“缓冲区”，指在写入磁盘前的存储再内存中的内容。） 查看CPU信息总核数 = 物理CPU个数 X 每颗物理CPU的核数总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数 查看物理CPU个数[root@centos4 ~]# cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l 1 查看每个物理CPU中core的个数（核数）[root@centos4 ~]# cat /proc/cpuinfo| grep "cpu cores"| uniq cpu cores : 4 查看逻辑CPU的个数[root@centos4 ~]# cat /proc/cpuinfo| grep "processor"| wc -l 4 查看CPU信息（型号）[root@centos4 ~]# cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 4 Intel(R) Xeon(R) CPU E5-1603 0 @ 2.80GHz scp（跨服务器拷贝）命令格式scp [参数] [原路径] [目标路径] 命令功能scp是 secure copy的缩写, scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。 命令参数 -1 强制scp命令使用协议ssh1-2 强制scp命令使用协议ssh2-4 强制scp命令只使用IPv4寻址-6 强制scp命令只使用IPv6寻址-B 使用批处理模式（传输过程中不询问传输口令或短语）-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）-p 保留原文件的修改时间，访问时间和访问权限。-q 不显示传输进度条。-r 递归复制整个目录。-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。-l limit 限定用户所能使用的带宽，以Kbit/s为单位。-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，-P port 注意是大写的P, port是指定数据传输用到的端口号-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。 使用实例 复制文件 scp local_file remote_username@remote_ip:remote_folder 或者 scp local_file remote_username@remote_ip:remote_file 或者 scp local_file remote_ip:remote_folder 或者 scp local_file remote_ip:remote_file 复制目录 scp -r local_folder remote_username@remote_ip:remote_folder 或者 scp -r local_folder remote_ip:remote_folder 从远程拷贝到本地同理 jobs（查看后台进程）命令格式jobs(选项)(参数) 命令功能jobs命令用于显示Linux中的任务列表及任务状态，包括后台运行的任务。该命令可以显示任务号及其对应的进程号。其中，任务号是以普通用户的角度进行的，而进程号则是从系统管理员的角度来看的。一个任务可以对应于一个或者多个进程号。 命令参数 -l：显示进程号；-p：仅任务对应的显示进程号；-n：显示任务状态的变化；-r：仅输出运行状态（running）的任务；-s：仅输出停止状态（stoped）的任务。 使用实例&amp;这个用在一个命令的最后，可以把这个命令放到后台执行 Ctrl+z将当前任务加入后台任务 [root@hadoopmaster ~]# ping www.baidu.com PING www.a.shifen.com (180.97.33.108) 56(84) bytes of data. 64 bytes from 180.97.33.108 (180.97.33.108): icmp_seq=1 ttl=128 time=45.4 ms 64 bytes from 180.97.33.108 (180.97.33.108): icmp_seq=2 ttl=128 time=45.3 ms ^Z [1]+ 已停止 ping www.baidu.com jobs显示当前后台任务列表 [root@hadoopmaster ~]# jobs [1]+ 已停止 ping www.baidu.com bg 将一个在后台暂停的命令，变成继续执行 如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) fg 将后台中的命令调至前台继续运行 如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) kill杀死一个进程，用法同fg和bg alias（别名）命令格式alias ［别名=’命令’］ ，在定义别名时，等号两边不能有空格。 unalias [别名] 删除别名。 命令功能linux系统下给命令指定别名。 在linux系统中如果命令太长又不符合用户的习惯，那么我们可以为它指定一个别名。虽然可以为命令建立“链接”解决长文件名的问题，但对于带命 令行参数的命令，链接就无能为力了。而指定别名则可以解决此类所有问题。 使用实例[root@hadoopmaster ~]# java -version java version "1.8.0_101" Java(TM) SE Runtime Environment (build 1.8.0_101-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode) [root@hadoopmaster ~]# alias jv='java -version' [root@hadoopmaster ~]# jv java version "1.8.0_101" Java(TM) SE Runtime Environment (build 1.8.0_101-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode) 使用不带参数的alias时，显示当前所有已定义别名 [root@hadoopmaster ~]# alias alias cp='cp -i' alias egrep='egrep --color=auto' alias fgrep='fgrep --color=auto' alias grep='grep --color=auto' alias jv='java -version' alias l.='ls -d .* --color=auto' alias ll='ls -l --color=auto' alias ls='ls --color=auto' alias mv='mv -i' alias rm='rm -i' alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde' alias x='xcall.sh' \+回车（换行继续输入）命令格式[未输完的命令] \+回车 [继续未输完的命令] 命令功能当输入指令过长时，使用\+回车可以跳到下一行继续输入指令 使用实例[root@hadoopCDH mnt]# ls -al 总用量 0 drwxr-xr-x. 3 root root 19 8月 30 16:22 . dr-xr-xr-x. 18 root root 235 8月 31 09:07 .. drwxr-xr-x. 3 root root 53 8月 30 16:23 nginx [root@hadoopCDH mnt]# ls \ #输入\后再回车直接进入换行输入模式 > -al 总用量 0 drwxr-xr-x. 3 root root 19 8月 30 16:22 . dr-xr-xr-x. 18 root root 235 8月 31 09:07 .. drwxr-xr-x. 3 root root 53 8月 30 16:23 nginx [root@hadoopCDH mnt]# echo（定义变量）命令格式echo ${变量名} 命令功能查看当前变量的值 设置变量需遵从的规则 变量与变量内容之间只能用=来连接，不能有任何空格 myname=liming 如果要在变量内容里面输入空格，则用双引号 myname=&quot;li ming&quot; 变数名称只能是英文字母与数字，但是开头字符不能是数字，如下为错误示范： 2myname=VBird 变量内容若有空白字符可使用双引号&quot;或单引号&#39;将变量内容结合起来，但 双引号内的特殊字符如$等，可以保有原本的特性，如下所示：var=&quot;lang is $LANG&quot;则echo $var可得lang is zh_TW.UTF-8 单引号内的特殊字符则仅为一般字符(纯文字)，如下所示：var=&#39;lang is $LANG&#39;则echo $var可得lang is $LANG 可用字符\将特殊符号(如[Enter], $, ,空白字符, ‘等)变成一般字符，如： myname=VBird\ Tsai 一串指令的执行中，还需要藉由其他额外的指令所提供的资讯时，可以使用反单引号『`指令`』或『$(指令)』。特别注意，那个`是键盘上方的数字键1左边那个按键，而不是单引号！例如想要取得核心版本的设定： version=$(uname -r)再echo $version可得3.10.0-229.el7.x86_64 若该变量为扩增变量内容时，则可用&quot;$变量名称&quot;或${变量}累加内容，如下所示： PATH=&quot;$PATH&quot;:/home/bin或PATH=${PATH} :/home/bin 若该变量需要在其他子程序执行，则需要以export来使变量变成环境变量： export PATH 通常大写字符为系统预设变量，自行设定变量可以使用小写字符，方便判断(纯粹依照使用者兴趣与爱好) ； 取消变量的方法为使用unset 变量名称例如取消myname的设定： unset myname 使用实例[root@hadoopCDH mnt]# echo $NGINX_HOME /usr/local/nginx [root@hadoopCDH mnt]# myname=liming [root@hadoopCDH mnt]# echo $myname liming [root@hadoopCDH mnt]# echo ${myname} liming [root@hadoopCDH mnt]# ntpdate（服务器时间同步）命令格式ntpdate time1.aliyun.com 命令功能同步时间，与阿里云服务器时间同步。 使用实例[root@hadoopmaster bin]# xcall.sh ntpdate time1.aliyun.com ========== hadoopmaster ========== 11 Sep 09:45:55 ntpdate[6548]: adjust time server 203.107.6.88 offset 0.007047 sec ========== hadoop001 ========== 11 Sep 09:46:02 ntpdate[6278]: adjust time server 203.107.6.88 offset 0.002663 sec ========== hadoop002 ========== 11 Sep 09:46:08 ntpdate[4192]: adjust time server 203.107.6.88 offset 0.005169 sec [root@hadoopmaster bin]# history（历史命令查看）命令格式及功能 命令格式 命令 history 显示命令历史列表 ↑(Ctrl+p) 显示上一条命令 ↓(Ctrl+n) 显示下一条命令 !num 执行命令历史列表的第num条命令 !! 执行上一条命令 !?string? 执行含有string字符串的最新命令 使用实例 设置显示命令时间 [root@hadoopmaster ~]# echo &#39;export HISTTIMEFORMAT=&#39;%F %T &#39; &quot;&#39; &gt;&gt; /etc/profile # 将`export HISTTIMEFORMAT=&#39;%F %T &#39;`添加进配置文件`/etc/profile` [root@hadoopmaster ~]# source /etc/profile # 刷新配置文件使其生效 [root@hadoopmaster ~]# history 5 # 查看最近5条记录，发现显示了时间 1063 2018-09-15 11:00:39 history --help 1064 2018-09-15 11:04:14 cd ~ 1065 2018-09-15 11:04:50 echo &#39;HISTTIMEFORMAT=&quot;%F %T &quot;&#39; &gt;&gt; /etc/profile 1066 2018-09-15 11:05:01 vim /etc/profile 1067 2018-09-15 11:07:07 history 5 执行历史某条命令 [root@hadoopmaster ~]# history 4 # 显示最近4条记录 1066 2018-09-15 11:05:01 vim /etc/profile 1067 2018-09-15 11:07:07 history 5 1068 2018-09-15 11:08:30 ls 1069 2018-09-15 11:08:37 history 5 [root@hadoopmaster ~]# !1068 # 执行1068条，也就是ls命令 ls anaconda-ks.cfg zookeeper.out [root@hadoopmaster ~]# passwd（修改密码）命令格式passwd [用户名] 命令功能修改账户密码 使用实例[root@slave001 /]# passwd root # 修改root密码 Changing password for user root. New password: # 设置新密码 BAD PASSWORD: The password is shorter than 8 characters Retype new password: # 确认新密码 passwd: all authentication tokens updated successfully. 修改root密码也可以使用下面一句命令搞定 [root@a3c8baf6961e /]# echo "1234" | passwd --stdin root # 将root密码设置为1234 ss（查看端口占用）命令格式ss [参数] ss [参数] [过滤] 命令功能ss(Socket Statistics的缩写)命令可以用来获取 socket统计信息（查询端口占用），比 netstat 更快速高效 命令参数 -h, –help 帮助信息 -V, –version 程序版本信息 -n, –numeric 不解析服务名称 -r, –resolve 解析主机名 -a, –all 显示所有套接字（sockets） -l, –listening 显示监听状态的套接字（sockets） -o, –options 显示计时器信息 -e, –extended 显示详细的套接字（sockets）信息 -m, –memory 显示套接字（socket）的内存使用情况 -p, –processes 显示使用套接字（socket）的进程 -i, –info 显示 TCP内部信息 -s, –summary 显示套接字（socket）使用概况 -4, –ipv4 仅显示IPv4的套接字（sockets） -6, –ipv6 仅显示IPv6的套接字（sockets） -0, –packet 显示 PACKET 套接字（socket） -t, –tcp 仅显示 TCP套接字（sockets） -u, –udp 仅显示 UCP套接字（sockets） -d, –dccp 仅显示 DCCP套接字（sockets） -w, –raw 仅显示 RAW套接字（sockets） -x, –unix 仅显示 Unix套接字（sockets） 使用实例显示Sockets摘要ss -s [root@localhost ~]# ss -s Total: 7089 (kernel 10238) TCP: 600 (estab 267, closed 279, orphaned 0, synrecv 0, timewait 195/0), ports 0 Transport Total IP IPv6 * 10238 - - RAW 0 0 0 UDP 3 2 1 TCP 321 203 118 INET 324 205 119 FRAG 0 0 0 显示TCP连接ss -t -a [root@localhost ~]# ss -t -a State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 0 127.0.0.1:smux *:* LISTEN 0 0 *:3690 *:* LISTEN 0 0 *:ssh *:* ESTAB 0 0 192.168.120.204:ssh 10.2.0.68:49368 找出占用套接字/端口的应用程序ss -anlp | grep 8080 [root@localhost ~]# ss -anlp | grep 8088 tcp LISTEN 0 128 ::ffff:10.75.4.31:8088 :::* users:(("java",pid=11935,fd=225)) watch（检测命令运行结果）命令格式watch [参数] [命令] 命令功能监测一个命令的运行结果，周期性刷新结果，省得一遍遍的手动运行 命令参数 -n或–interval watch缺省每2秒运行一下程序，可以用-n或-interval来指定间隔的时间。 -d或–differences 用-d或–differences 选项watch 会高亮显示变化的区域。 而-d=cumulative选项会把变动过的地方(不管最近的那次有没有变动)都高亮显示出来。 -t或-no-title 会关闭watch命令在顶部的时间间隔,命令，当前时间的输出。 命令实例watch -n 1 -d netstat -ant 每隔一秒高亮显示网络链接数的变化情况watch -n 10 &#39;cat /proc/loadavg&#39; 10秒一次输出系统的平均负载 其他操作Ctrl+c退出观察模式 rz与sz（上传、下载文件）当我们使用虚拟终端软件，如Xshell、SecureCRT或PuTTY来连接远程服务器后，需要上传、下载文件到本地，可以使用该命令。使用前可能需要安装lrzsz软件：yum -y install lrzsz rz（Receive ZMODEM）命令格式rz [选项] 命令功能使用ZMODEM协议，将本地文件批量上传到远程Linux/Unix服务器，注意不能上传文件夹。 命令参数 -+, –append:将文件内容追加到已存在的同名文件-a,–ascii:以文本方式传输-b, –binary:以二进制方式传输，推荐使用--delay-startup N:等待N秒-e, –escape:对所有控制字符转义，建议使用-E, –rename:已存在同名文件则重命名新上传的文件，以点和数字作为后缀-p, –protect:对ZMODEM协议有效，如果目标文件已存在则跳过-q, –quiet:安静执行，不输出提示信息-v, –verbose:输出传输过程中的提示信息-y, –overwrite:存在同名文件则替换-X, –xmodem:使用XMODEM协议--ymodem:使用YMODEM协议-Z, –zmodem:使用ZMODEM协议--version：显示版本信息--h, –help：显示帮助信息 命令实例 输入rz，然后回车，选择本地文件上传 以二进制，并对控制字符进行转义，替换已存在的同名文件。[root@hadoopmaster opt]# rz -bye sz（Send ZMODEM）命令格式sz [选项] [filelist]命令功能通过ZMODEM协议，可将多个文件从远程服务器下载到本地。注意不能下载文件夹，如果下载文件夹，请先打包再下载命令参数选项参数与rz相同，请参考上文中rz命令参数，或者运行命令sz -h查看命令实例下载多个文件[root@hadoopmaster opt]# sz file1 file2 file3 wc（Word Count单词统计）命令格式wc [选项] [文件] 命令功能统计指定文件中的字节数、字数、行数，并将统计结果显示输出。 命令参数 -c 统计字节数。-l 统计行数。-m 统计字符数。这个标志不能与 -c 标志一起使用。-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L 打印最长行的长度。 使用实例[hdfs@centos3 log]$ wc -l mysqld.log 7756 mysqld.log [hdfs@centos3 log]$ jps 10842 Jps 19566 DataNode [hdfs@centos3 log]$ jps | wc -l 2 w（显示目前登入系统的用户信息）命令格式w [选项] 命令功能目前登入系统的用户有哪些人，以及他们正在执行的程序。 命令参数 -f 开启或关闭显示用户从何处登入系统。-h 不显示各栏位的标题信息列。-l 使用详细格式列表，此为预设值。-s 使用简洁格式列表，不显示用户登入时间，终端机阶段作业和程序所耗费的CPU时间。-u 忽略执行程序的名称，以及该程序耗费CPU时间的信息。 使用实例[root@centos3 ~]# w 16:23:23 up 29 days, 21:28, 5 users, load average: 0.41, 0.42, 0.40 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 10.75.4.14 16:11 27.00s 9.31s 9.28s /home/jdk/bin/java -Dproc_jar -Dhdp.version=3.1.0.0-78 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.1.0.0-78 -Xmx1024m -Xmx256m -Dlog4j.configurationFile=hive-log4j2.pr root pts/1 10.75.4.12 15:43 19.00s 43.79s 43.79s -bash root pts/2 10.76.34.243 09:22 7:00m 1.69s 1.62s vim messages root pts/3 10.75.4.12 16:23 3.00s 0.00s 0.00s -bash root pts/5 10.75.4.11 Mon15 3.00s 0.06s 0.06s -bash 查看相应ssh连接进程号，可以通过kill -9 命令杀掉进程 [root@centos3 ~]# w 16:36:49 up 29 days, 21:41, 4 users, load average: 2.03, 0.96, 0.66 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 10.75.4.14 16:26 9:05 0.00s 0.00s -bash root pts/1 10.75.4.12 15:43 2:49 2:15 2:15 -bash root pts/2 10.75.4.11 16:35 1.00s 20.22s 0.00s w root pts/3 10.75.4.12 16:23 2:49 1:27 1:27 -bash [root@centos3 ~]# ps aux | grep sshd root 1105 0.0 0.0 112796 1280 ? Ss Feb18 0:00 /usr/sbin/sshd -D root 4368 1.2 0.0 161400 6192 ? Ss 15:43 0:40 sshd: root@pts/1 root 19926 11.3 0.0 161400 6200 ? Ss 16:23 1:39 sshd: root@pts/3 root 21314 0.8 0.0 161400 6044 ? Ss 16:26 0:05 sshd: root@pts/0 root 24001 0.0 0.0 161400 6044 ? Ss 16:35 0:00 sshd: root@pts/2 root 24932 0.0 0.0 112728 996 pts/2 S+ 16:37 0:00 grep --color=auto sshd [root@centos3 ~]# kill -9 4368 [root@centos3 ~]# w 16:38:35 up 29 days, 21:43, 4 users, load average: 1.55, 1.18, 0.78 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 10.75.4.14 16:26 10:51 0.00s 0.00s -bash root pts/2 10.75.4.11 16:35 3.00s 20.23s 0.00s w root pts/3 10.75.4.12 16:23 4:35 2:33 2:33 -bash hostnamectl（查看修改主机名）命令格式hostnamectl [选项] 命令功能查看和修改主机名。 在CentOS中，有三种定义的主机名:静态的（static），瞬态的（transient），和灵活的（pretty）。静态主机名也称为内核主机名，是系统在启动时从/etc/hostname自动初始化的主机名。瞬态主机名是在系统运行时临时分配的主机名，例如，通过DHCP或mDNS服务器分配。静态主机名和瞬态主机名都遵从作为互联网域名同样的字符限制规则。而另一方面，灵活主机名则允许使用自由形式（包括特殊/空白字符）的主机名，以展示给终端用户（如hdp001）。 命令参数[root@hdp001 ~]# hostnamectl --help hostnamectl [OPTIONS...] COMMAND ... Query or change system hostname. -h --help 查看帮助 --version 显示包版本 --no-ask-password 不要提示输入密码 -H --host=[USER@]HOST 在远程主机上运行 -M --machine=CONTAINER 在本地容器上操作 --transient 仅设置瞬态主机名 --static 仅设置静态主机名 --pretty 只设置灵活的主机名 Commands: status 显示当前主机名设置 set-hostname NAME 设置系统主机名 set-icon-name NAME 设置主机的图标名称 set-chassis NAME 设置主机的机箱类型 set-deployment NAME 为主机设置部署环境 set-location NAME 设置主机的位置 使用实例查看当前主机名 [root@hdp002 ~]# hostnamectl Static hostname: hdp002 Icon name: computer-vm Chassis: vm Machine ID: e47dbc1c37d64b7ebcb988e0ecf1836a Boot ID: 93fa221f8aae49a183970941c4ad5d48 Virtualization: vmware Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-862.el7.x86_64 Architecture: x86-64 仅查看静态主机名 [root@hdp002 ~]# hostnamectl --static hdp002 修改主机名 [root@hdp002 ~]# hostnamectl set-hostname hdp002.segma.tech [root@hdp002 ~]# hostnamectl --static hdp002.segma.tech]]></content>
      <categories>
        <category>Linux</category>
        <category>命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware虚拟机静态IP配置]]></title>
    <url>%2F2018%2F08%2F09%2FVMware%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%9D%99%E6%80%81IP%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[使用虚拟机创建Linux服务器，固定ip地址的方法。 VMware虚拟机网卡的两种模式：Bridged（桥接）和NAT（转发）。 Bridged（桥接模式）桥接模式可以让虚拟机的网络和物理机的网络处于平行的网络中，和物理机处于同一个网段的其他物理机就能ping通该虚拟机。 NAT模式该模式虚拟机自己创建一套网络，只有物理机和虚拟机处于同一局域网内，与物理机处于同一网段的无法连接虚拟机。虚拟机通过物理机的网卡进行外网访问。 根据自己需求选择模式，如果是想搭建一套不受外界干扰的局域网络，则用NAT模式。如果想与外界其他处于同一网段的物理机通信，则选择桥接模式 NAT模式1.查看网关和网段虚拟机点击【编辑】→【虚拟网络编辑器】 使用NAT模式，选择VMnet8。取消勾选【使用本地DHCP…】，勾选会设置动态ip。 点击NAT设置，记住子网IP范围，如图表示我们只能设置虚拟机在192.168.40.0~192.168.40.255范围内。 注意： 192.168.40.2为网关地址，192.168.40.255为广播地址，192.168.40.0一般为网段IP，所以0,2,255这三个地址不能设置 2.设置虚拟机IP地址涉及文件列表： /etc/sysconfig/network-scripts/ifcfg-*（网卡）（*根据实际情况不同，本文为ens33） /etc/sysconfig/network（主机名） /etc/resolv.conf（DNS） 网卡信息修改 vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes NAME=ens33 UUID=d7a7196e-5033-4849-bcc5-f8c52acd245c DEVICE=ens33 ONBOOT=yes IPADDR=192.168.40.3 NETMASK=255.255.255.0 GATEWAY=192.168.40.2ONBOOT：开机启动。 NM_CONTROLLED：网络管理组件是否启用，精简版的是没有这个组件的。所以就不需要开启。 BOOTPROTO：网络分配方式，静态。 IPPADDR：手动指定ip地址。 NETMASK：子网掩码。 GATEWAY：网关ip。编辑好以后保存退出。 DNS配置 vi /etc/resolv.conf nameserver 8.8.8.8 nameserver 8.8.4.4nameserver：这里填对应的dns域名解析服务器的ip。 可以指定多个，其他的默认为备用DNS 主机名修改 vi /etc/sysconfig/network # Created by anaconda NETWORKING=yes HOSTNAME=hadoop001有需要就可以修改主机名。配置完三个文件重启一下机器（或者/etc/rc.d/init.d/network restart重启网络）。 3.确保Linux虚拟机网络适配器选项 选择虚拟机，点击右键→【设置】 【硬件】→【网络适配器】→选择【NAT模式】 4.Windows IP设置设置VMWare给我们配置的网络适配器，就是那个NAT8。右键属性 点击IPv4设置 设置Windows的ip地址，该地址也在范围内。 配置完之后就可以使用xshell等工具连接了。 Bridged（桥接模式）1.VM设置 查看物理机网卡名称 根据网卡，查看物理机ip网段信息进入cmd命令行，执行ipconfig /all 虚拟机点击【编辑】→【虚拟网络编辑器】选择正确的网卡 设置将要使用的虚拟机网络适配器，将其改为桥接模式。 2.设置虚拟机IP地址涉及文件列表： /etc/sysconfig/network-scripts/ifcfg-*（网卡）（*根据实际情况不同，本文为ens33） /etc/sysconfig/network（主机名） /etc/resolv.conf（DNS） 网卡信息修改 vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes NAME=ens33 UUID=d7a7196e-5033-4849-bcc5-f8c52acd245c DEVICE=ens33 ONBOOT=yes IPADDR=192.168.88.3 # 这里网段要和物理机网段一致 NETMASK=255.255.255.0 GATEWAY=192.168.88.1 # 网关也要和物理机一样ONBOOT：开机启动。 NM_CONTROLLED：网络管理组件是否启用，精简版的是没有这个组件的。所以就不需要开启。 BOOTPROTO：网络分配方式，静态。 IPPADDR：手动指定ip地址。 NETMASK：子网掩码。 GATEWAY：网关ip。编辑好以后保存退出。 DNS配置 vi /etc/resolv.conf nameserver 8.8.8.8 nameserver 8.8.4.4nameserver：这里填对应的dns域名解析服务器的ip。 可以指定多个，其他的默认为备用DNS 主机名修改 vi /etc/sysconfig/network # Created by anaconda NETWORKING=yes HOSTNAME=hadoop001有需要就可以修改主机名。配置完三个文件重启一下机器（或者/etc/rc.d/init.d/network restart重启网络）。]]></content>
      <categories>
        <category>Linux</category>
        <category>虚拟机</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>IP</tag>
        <tag>虚拟机</tag>
        <tag>VMware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遇到多个构造器参数时的最优解决方案]]></title>
    <url>%2F2017%2F08%2F17%2F%E9%81%87%E5%88%B0%E5%A4%9A%E4%B8%AA%E6%9E%84%E9%80%A0%E5%99%A8%E5%8F%82%E6%95%B0%E6%97%B6%E7%9A%84%E6%9C%80%E4%BC%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[今天在写代码的时候，忽然发现为bean设置属性代码特别的多，冗余，例如： MyData myData = new MyData(); myData.setA1(1.0); myData.setA2(1.0); myData.setA3(1.0); myData.setA4(1.0); myData.setA5(1.0); myData.setQuality("ok"); 经学长指点，发现可以用Builder模式解决这个问题，在此总结一下Builder模式。 遇到多个构造器参数时的最优解决方案一、传统的做法当遇到构造器里面有很多的参数需要传递时，最原始的做法可能就是直接传参数。 public class MyData{ private double A1; private double A2; private double A3; private double A4; private double A5; public MyData() { } public MyData(double a1, double a2, double a3, double a4, double a5) { A1 = a1; A2 = a2; A3 = a3; A4 = a4; A5 = a5; } } MyData myData = new MyData(1.0,0,2.0,3.3,4.2,5.9); 缺点：这个构造器调用通常需要设置你原本不想设置的参数，（比如上例中的A2）。而且这种方式可读性不高，就这样看，谁知道你的哪个参数对应着哪个属性呢？ 二、一种替代方式JavaBeans模式，这种模式调用一个无参构造器来创建对象，然后调用setter方法来设置你想要设置的参数。 public class MyData{ private double A1; private double A2; private double A3; private double A4; private double A5; public MyData() { } public MyData(double a1, double a2, double a3, double a4, double a5) { A1 = a1; A2 = a2; A3 = a3; A4 = a4; A5 = a5; } public double getA1() { return A1; } public void setA1(double a1) { A1 = a1; } public double getA2() { return A2; } public void setA2(double a2) { A2 = a2; } public double getA3() { return A3; } public void setA3(double a3) { A3 = a3; } public double getA4() { return A4; } public void setA4(double a4) { A4 = a4; } public double getA5() { return A5; } public void setA5(double a5) { A5 = a5; } } MyData myData = new MyData(); myData.setA1(1.0); myData.setA2(1.0); myData.setA3(1.0); myData.setA4(1.0); myData.setA5(1.0); 这种方式貌似完全解决了构造器参数可读性不高的问题，但是这种方式有着严重的缺点：这个对象的构造过程被分到了几个调用当中，在构建过程中JavaBean可能处于不一致的状态，也就是说需要程序员付出额外的努力来确保它的线程安全。 三、Builder模式这种方式不直接生成对象，而是根据你需要设置的属性参数调用构造器，得到一个builder对象。然后在builder对象上调用类似setter的方法，来设置每个相关的可选参数。最后利用builder的无参build方法（create方法）来生成不可变的对象。 public class MyDataBuilder { private double a1; private double a2; private double a3; private double a4; private double a5; public MyDataBuilder setA1(double a1) { this.a1 = a1; return this; } public MyDataBuilder setA2(double a2) { this.a2 = a2; return this; } public MyDataBuilder setA3(double a3) { this.a3 = a3; return this; } public MyDataBuilder setA4(double a4) { this.a4 = a4; return this; } public MyDataBuilder setA5(double a5) { this.a5 = a5; return this; } public MyData createMyData() { return new MyData(a1, a2, a3, a4, a5); } } 这时，创建对象就可以这么写： MyData myData = new MyDataBuilder() .setA1(2.0) .setA3(3.0) .setA4(4.0) .setA5(5.0) .createMyData(); 可以看到，这种builder方式既能清晰的看到每个参数的对应属性，又能保证类在创建时的一致性（因为生成对象的操作在一句语句中），另外还便于阅读，较JavaBeans的方法也减少了一些无用的代码编写量，更简洁，便于编写。 在idea中如何快速生成Builder类不得不说idea是个很强大的编译器，它提供了类的构建器的自动创建，但前提是该类必须有包含所有必须属性参数的构造器。 为类生成带有全部必须属性的构造器在类中右键选择Generate（也可以使用快捷键：Alt+Insert）得到以下面板后点击Constructor选择需要生成构造器的属性点击ok生成 注意：在自定义有参构造器时，需要重写一个无参的构造器 在需要生成的类里面右键，选择Refactor生成后： 总结Builder模式也有它自身的不足之处。为了创建对象，必须先创建它的构建器。虽然创建构建器的开销在实践中可能不是那么明显，但是在十分注重性能的情况下，可能就成问题了。最后，如果类的构造器或者静态工厂中具有多个参数，设计这种类时，Builder模式就是中不错的选择。]]></content>
      <categories>
        <category>Java</category>
        <category>构造器</category>
      </categories>
      <tags>
        <tag>构造器</tag>
        <tag>创建对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员必知的8大排序算法]]></title>
    <url>%2F2017%2F08%2F09%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E7%9F%A5%E7%9A%848%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[程序员必知的8大排序算法 8中排序之间的关系 一、插入排序直接插入排序 思想：在要排序的一堆数组中，假设前面的（n-1）[n&gt;=2]个数已经是排好序的，现在要把第n个数插入到之前的有序数中，使得这n个数也是排好序的。如此反复直到所有的数都排好顺序。 实例： 用java实现： public int[] insertSort() { int a[] = {4, 2, 3, 1}; int temp = 0; for (int i = 1; i &lt; a.length; i++) { //第一次i是第二个元素，第一次是第二个元素和前面比较 int j = i - 1; //temp为当前需要插入的元素 temp = a[i]; //j为temp之前的一个元素 for (; j >= 0 &amp;&amp; temp &lt; a[j]; j--) {//如果j还指向数组中的元素（大于等于0），且大于temp a[j + 1] = a[j]; //将大于temp的值整体后移一个单位 } a[j + 1] = temp; //最后将temp放到自己应该存在的位置 } return a; } 希尔排序（最小增量排序）为直接插入排序的改进版，先进行分组的直接插入排序，直到整个数组呈基本有序的情况，再对整个数组进行一次直接插入排序。 思想： 划分组：把原始组分成多少分（按照每组d个元素分）。排序组：每个划分组中的第i个数所组成的组（用于排序）。 先将要排序的一组数按某个增量d（n/2，n为要排序数的个数，除不尽向上取整）分成若干组（这里暂且称它为划分组），每个划分组有d个数据（最后一组可能小于d），再取每划分组中对应的元素组成新的组（每一组的第i个为新的一组，如：每一组的第一个组成一组，这里暂且称它为排序组），对形成的排序组每组作直接插入排序。然后再用较小的增量d/2对它进行分划分组、排序组，每个排序组中再进行直接插入排序，直到增量减小到1，再最后进行一次直接插入排序。 实例： 用java实现： public int[] shellSort() { int a[] = {4, 2, 3, 1}; double d1 = a.length; int temp = 0; while (true) { //算出每一轮的增量 d1 = Math.ceil(d1/2); int d = (int) d1; //该循环取得每个需要进行直接插入排序的数组，x为原始数组中的脚标， //代表每个组中提取的第几个元素来组成新的分组进行排序 for (int x = 0; x &lt; d; x++) { //该循环为直接插入排序（只是排序的目标数组变成了角标相差为d的数所组成的数组） for (int i = x + d; i &lt; a.length; i += d) { int j = i - d; temp = a[i]; //直接插入排序的算法，只是将直接插入排序的1抽象为了d for (; j >= 0 &amp;&amp; temp &lt; a[j]; j -= d) { a[j + d] = a[j]; //将需要排序的组中大于temp的向后移（移动单位为d） } a[j + d] = temp; //将temp放到它应该在的位置 } } if (d == 1) { break; } } return a; } 二、选择排序简单选择排序堆排序三、交换排序冒泡排序 基本思路：对需要排序的数组自上而下对相邻的两个数进行比较，让较大的数往下沉，较小的数往上冒。即：每当相邻的两个数不满足要求的排序时，将它们交换 实例： 用java实现： public int[] bubbleSort(){ int a[] = {4, 2, 3, 1}; for(int i = a.length-1 ; i>0 ; i--){ //比较前面多少位 for(int j = 0 ; j&lt;i ; j++){ //从第一位开始，把大的数往后冒 if(a[j] > a[j+1]){ int temp = a[j]; a[j] = a[j+1]; a[j+1] = temp; } } } return a; } 快速排序四、归并排序五、基数排序]]></content>
      <categories>
        <category>算法</category>
        <category>排序</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2F2017%2F08%2F09%2FHTTP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[HTTP协议 HTTP（HyperText Transfer Protocol，超文本传输协议）TCP/IP的分层管理应用层、传输层、网络层、数据链路层TCP/IP协议族各层的作用： 名称 作用 协议 应用层 决定了向用户提供应用服务时通信的活动。 FTP（文件传输协议）、DNS（域名系统）、HTTP协议也处于该层 传输层 对上层应用层提供处于网络连接中的两台计算机之间的数据传输。 TCP（传输控制协议）、UDP（用户数据报协议） 网络层（网络互连层） 用来处理在网络上流动的数据包。数据包是网络传输的最小数据单位。规定了通过怎样的路径（所谓的传输路线）到达对方计算机，并把数据包传送给对方 链路层（数据链路层） 用来处理连接网络的硬件部分。包括控制操作系统、硬件的设备驱动、NIC（Network Interface Card，网络适配器，即网卡），及光纤等物理可见部分（还包括连接器等一切传输媒介） ### 与HTTP关系密切的协议：IP、TCP和DNS #### 负责传输的IP协议 IP（Internet Protocol，网际协议）几乎所有使用网络的系统都会用到IP协议。 不要把IP和IP地址混淆，IP其实是一种协议的名称，而IP地址是网络中计算机通讯时的地址。 IP协议的作用是把各种数据包传送给对方。而要保证确实传送到对方那里，则需要满足各类条件。其中两个重要的条件是IP地址和MAC地址（Media Access Control Address）。IP地址可以和MAC地址进行配对。 IP地址 MAC地址 指明了节点被分配到的地址 网卡所属的固定地址 可变换 基本上不会更改 #### 确保可靠性的TCP协议 提供可靠的字节流服务。 将大块数据分割成以报文段为单位的数据包进行管理。 为了准确无误的将数据送达目标处，TCP协议采用了三次握手策略，握手过程中使用到了TCP的标志（flag）：SYN（synchronize）和ACK（acknowledgement）。 发送端首先发送一个SYN的数据包给对方。接收端收到后，回传一个带有SYN/ACK标志的数据包以示传达确认信息。最后，发送端再回传一个带ACK标志的数据包，代表“握手”结束。 发送端 接收端 标有SYN的数据包发给你—–&gt; &lt;—–收到你的数据包（发送带有SYN/ACK的数据包） 明白！（发送带有ACK的数据包）—–&gt; #### 负责域名解析的DNS服务 DNS（Domain Name System）服务提供域名到IP地址之间的解析服务。 URI和URLURI统一资源标识符，用字符串标识某一互联网资源。URL统一资源定位符，表示资源的地点。是URI的子集。]]></content>
      <categories>
        <category>网络协议</category>
        <category>Http</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB的安装以及MongoDB Replicat Set复制集的搭建]]></title>
    <url>%2F2017%2F07%2F29%2FMongoDB%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8AMongoDB-Replicat-Set%E5%A4%8D%E5%88%B6%E9%9B%86%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[记录MongoDB在Linux下的安装过程和MongoDB Replicat Set复制集的搭建过程。 一、搭建MongoDB单机环境MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。 A、使用APT安装参考官方安装文档 1. 导入public keysudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C62. 导入源（不同Ubuntu版本的源不一样，此处采用Ubuntu 16.04）echo &quot;deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list3. 重新加载本地源sudo apt-get update4. 安装mongodbsudo apt-get install -y mongodb-org5. 修改配置文件 配置文件默认文件目录/etc/mongod.conf默认存储其数据文件/var/lib/mongodb默认日志文件/var/log/mongodb/mongod.log 6. 启动mongodbmongod -f /etc/mongod.conf7. 卸载mongodb1)停止mongodb服务 sudo service mongod stop2)删除软件包删除您以前安装的任何MongoDB包。 sudo apt-get purge mongodb-org *3)删除数据目录删除数据库和日志文件 sudo rm -r / var / log / mongodb sudo rm -r / var / lib / mongodbB、使用安装包安装到清华开源镜像下载相应的包 1. 解压到需要安装的目录tar zxvf mongodb-linux-x86_64-ubuntu1604-3.4.4.tgz tar命令参数详解：-c ：建立打包档案，可搭配-v 来察看过程中被打包的档名(filename)-t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了；-x ：解打包或解压缩的功能，可以搭配-C (大写) 在特定目录解开特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。-z ：透过gzip 的支援进行压缩/解压缩：此时档名最好为*.tar.gz-j ：透过bzip2 的支援进行压缩/解压缩：此时档名最好为*.tar.bz2-J ：透过xz 的支援进行压缩/解压缩：此时档名最好为*.tar.xz特别留意， -z, -j, -J 不可以同时出现在一串指令列中。-v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！-f filename：-f 后面要立刻接要被处理的档名！建议-f 单独写一个选项啰！(比较不会忘记)-C 目录：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。更多的压缩命令参考鸟哥的Linux私房菜之档案的压缩、解压 2. 创建存放数据文件目录以及日志文件目录（可以自己指定）可以创建多个数据库存放路径，每个数据库对应一个conf配置文件，mongodb只需要安装一次 mkdir -p /mongodb/db # 创建数据文件夹 mkdir -p /mongodb/log touch /mongodb/log/mongodb.log #创建文件3. 在适当的路径编辑配置文件（后缀可选conf和yaml）vi mongodb.conf配置文件模板： # mongodb.conf # Where and how to store data. storage: dbPath: /mongodb/data journal: enabled: true # engine: # mmapv1: # wiredTiger: # where to write logging data. systemLog: destination: file logAppend: true path: /mongodb/log/mongodb.log # network interfaces net: port: 28001 # bindIp: 127.0.0.1 processManagement: fork: true #security: #operationProfiling: 文件路径修改为设置的存储路径，端口可自定义（一般推荐27或者28开头的5位数） 4. 配置环境变量PATH路径# 这里也可以修改~/.bashrc文件（区别见下表） sudo vi /etc/profile # 追加(路径是放置Mongodb安装包的bin路径) export PATH=/home/dusk/mongodb/bin:$PATH Linux中profile、bashrc、bash_profile之间的区别和联系 文件名 作用区间 执行时间 修改后是否需要重启 /etc/profile 该系统下每个用户 用户第一次登陆 是（source /etc/profile） /etc/bashrc 该系统下每个用户 用户新打开shell 否 ~/.bash_profile 当前用户 用户第一次登陆 是（source ~/.bash_profile） ~/.bashrc 当前用户 用户新打开shell 否 ~/.bash_logout 当前用户 退出shell 否 /etc/profile：该文件为系统的每个用户的环境信息文件，对每个用户生效。当用户第一次登陆的时候会执行这个文件加载里面的shell配置，所以修改这个文件之后要重启设置才能生效–&gt;source /etc/profile /etc/bashrc：为每一个执行bash shell的用户执行此文件，当bash shell被打开时，该文件被读取。修改后只需要重新打开一个bash即可生效 ~/.bash_profile：每个用户目录下都有这个文件，用户可以通过修改这个文件来设置自己专用的shell信息，该文件中设置了一些环境变量，执行用户的.bashrc文件。该文件类似于/etc/profile，需要重新启动才会生效，只对当前用户生效。 ~/.bashrc：该文件包含专用于你自己的bash shell信息，每次打开新的shell时被读取，只对当前用户新打开的bash生效。 ~/.bash_logout：当该用户退出bash shell时执行 5. 如果修改的是/etc/profile，则需要重新启动配置文件source /etc/profile6. 在配置文件路径启动mongodbmongod -f mongodb.conf关闭MongoDB 使用mongod命令mongod --shutdown --dbpath /数据库储存路径 连接进mongodb数据库关闭# 进入mongoshell mongo --port=28001 # 使用admin数据库（只有在admin下才能执行shutdown方法） use admin # 关闭mongodb服务 db.shutdownServer() 查看mongodb的进程，用kill杀掉进程不推荐这种方式，会有丢失数据的风险 二、搭建MongoDB Replicat Set复制集MongoDB Replica Set是MongoDB官方推荐的主从复制和高可用方案，用于替代原有的Master-Slave主从复制方案。不懂原理的可以点击这里查看复制集原理（推荐了解原理后再搭建） 1.搭建环境（电脑配置）在需要布置为节点的机器上安装好MongoDB环境，参照上面的教程 节点 主机名(IP地址):端口号 Primary Node(主节点) node1:28001 Secondary Node(次节点) node2:28001 Arbiter Node(投票节点) node3:28001 当然，你也可以在一台电脑上搭建复制集，但是端口一定不要冲突 2.确保每个节点数据、日志文件都建立完毕# 数据目录 mkdir -p /mongodb/data/ # 日志目录 mkdir -p /mongodb/log/ # 创建日志文件 touch /mongodb/log/mongodb.log # 配置文件目录 mkdir -p /mongodb/conf 参考上面的mongodb环境搭建 3.修改每个节点的配置文件（重点）# 例： vi /mongodb/conf/mongodb.yaml在文件后面追加： # 副本 replication: #设置复制集名称，可自定义 replSetName: DBTEST #设置操作日志的大小(important) oplogSizeMB: 10240 #sharding: ## Enterprise-Only Options: #auditLog: #snmp: 复制集名称每个节点一定要一样 4.启动每个节点mongod -f /mongodb/conf/mongodb.conf5.进入一个节点配置复制集（重点）#任意一个节点上(最好选在主节点node1) mongo --port 28001在mongo shell中配置副本输入rs.help()可以查看rs的各种方法 # 初始化复制集 rs.initiate() # 显示复制集配置对象 rs.conf() # 将其余成员添加到副本集 # 返回{ &quot;ok&quot; : 1 }说明添加成功 rs.add(&quot;node2:28001&quot;) rs.addArb(&quot;node3:28001&quot;)查看副本集状态，可以看到复制集的全部信息都被显示出来 DBTEST:PRIMARY&gt; rs.status() { &quot;set&quot; : &quot;DBTEST&quot;, &quot;date&quot; : ISODate(&quot;2017-06-30T07:10:33.247Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;name&quot; : &quot;node1:28001&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 316, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1498806589, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2017-06-30T07:09:49Z&quot;), &quot;infoMessage&quot; : &quot;could not find member to sync from&quot;, &quot;electionTime&quot; : Timestamp(1498806520, 2), &quot;electionDate&quot; : ISODate(&quot;2017-06-30T07:08:40Z&quot;), &quot;configVersion&quot; : 4, &quot;self&quot; : true }, { &quot;_id&quot; : 1, &quot;name&quot; : &quot;node2:28001&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 58, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1498806589, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2017-06-30T07:09:49Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2017-06-30T07:10:31.761Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-06-30T07:10:32.765Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;syncingTo&quot; : &quot;node2:28001&quot;, &quot;configVersion&quot; : 4 }, { &quot;_id&quot; : 2, &quot;name&quot; : &quot;node3:28001&quot;, &quot;health&quot; : 1, &quot;state&quot; : 7, &quot;stateStr&quot; : &quot;ARBITER&quot;, &quot;uptime&quot; : 43, &quot;lastHeartbeat&quot; : ISODate(&quot;2017-06-30T07:10:31.761Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2017-06-30T07:10:29.970Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;configVersion&quot; : 4 } ], &quot;ok&quot; : 1 }6.删除子节点rs.remove(&quot;node3:28001&quot;) # 返回{&quot;ok&quot; : 1}7.可能遇到的问题在节点中执行show方法可能出现 DBTEST:SECONDARY&gt; show databases 2017-07-28T11:15:06.856+0800 E QUERY [thread1] Error: listDatabases failed:{ &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk=false&quot;, &quot;code&quot; : 13435, &quot;codeName&quot; : &quot;NotMasterNoSlaveOk&quot; } : _getErrorWithCode@src/mongo/shell/utils.js:25:13 Mongo.prototype.getDBs@src/mongo/shell/mongo.js:62:1 shellHelper.show@src/mongo/shell/utils.js:755:19 shellHelper@src/mongo/shell/utils.js:645:15 #解决方法是在报错的节点上执行rs.slaveOk()方法即可 DBTEST:STARTUP2&gt; rs.slaveOk() DBTEST:STARTUP2&gt; show databases admin 0.000GB local 0.000GB mydatabase 0.000GB8.关闭、重启复制集参考上边的关闭mongodb的方法，依次在每个节点上执行关闭操作注意关闭顺序，在重启节点的过程中，建议不要直接shutdown Primary，这样可能导致已经写入primary但未同步到secondary的数据丢失 1.shutdown Primary （shutdown会等待Secondary oplog追到10s以内）2.Primary退出后，剩余的节点选举出一个新的Primary（复制集只包含1或2节点例外）3.Primary重新启动，因为当前复制集已经有了新的Primary，这个Primary将以Secondary的角色运行。4.从新的Primary同步的过程中，发现自己有无效的oplog，会先进行rollback。（rollback的数据只要不超过300M是可以找回的） 上面这种操作可能会导致数据丢失 1.逐个重启复制集里所有的Secondary节点2.对Primary发送stepDown命令，等待primary降级为Secondary3.重启降级后的Primary]]></content>
      <categories>
        <category>数据库</category>
        <category>Mongodb</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB笔记]]></title>
    <url>%2F2017%2F07%2F26%2FMongoDB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MongoDB笔记 一、Mongo数据库为NoSQL数据库 关系型数据库 NoSQL数据库 数据库 数据库类似于MySQL 表 集合 行 文档 列 成员 主键 Object ID（自动维护） 与Node.js捆绑。 ### 启动MongoDB服务 * 默认端口启动 mongod --dbpath E:\MongoDB\db * 设置端口启动 mongod --dbpath E:\MongoDB\db --port=27001 连接MongoDB数据库mongo或者mongo --port=27001 切换到admin数据库 use admin 关闭数据库服务 db.shutdownServer() (必须在admin下才能执行成功) 重启服务 mongod -f E:\MongoDB\mongodb.conf ## 二、基本语法 ### 查看所有数据库 db.showDatabases; ### 使用某个数据库 use admin 这时并不会创建数据库，只有在数据库里面保存集合数据后，才真正创建数据库 创建一个集合db.createCollection(&quot;emp&quot;); 插入一条数据db.emp.insert({&quot;name&quot;: 10, &quot;ange&quot;: 10}); 一般在执行这一步的时候会直接创建集合emp，所以上面那句语句一般不会用，都是直接使用这句来插入数据的同时创建集合 查看所有集合show collections; 查看emp表的数据语法：db.集合名称.find({若干条件}，[ { 设置显示的字段 } ])db.emp.find(); 增加不规则的数据var deptData = { "name" : "123", "sex" : "man" } db.emp.insert(deptData); 关于ID问题在MongoDB集合中的每一行记录都会自动的生成一个“ *” _id” :ObjectId(“55949a13eecd74894d19d8dc”)*”数据，这个数据组成是：“时间戳 + 机器码 + PID + 计数器”，这个ID的信息是MongonDB数据自己为用户生成的。 单独的一个文档信息查看db.dept.findOne(); 删除一个数据db.dept.remove( { &quot;_id&quot; :ObjectId(&quot;55949a13eecd74894d19d8dc&quot;)} ); 更新数据var deptData = { "name" : "123", "sex" : "man" } db.dept.updata({ "_id" :ObjectId("55949a13eecd74894d19d8dc")},deptData); 删除集合语法：db.集合名称.drop()db.dept.drop(); 删除当前数据库db.dropDatabase(); 三、数据操作（重点）增加数据增加一个简单数据db.dept.insert({&quot;url&quot;:&quot;hm-dusk.github.io&quot;});保存一个数组 db.dept.insert([ {&quot;url&quot;:&quot;hm-dusk.github.io&quot;}, {&quot;url&quot;:&quot;https://hm-dusk.github.io&quot;} ]);可以使用JavaScript的循环添加 for(var x = 0; x &lt; 1000; x ++){ db.dept.insert({&quot;url&quot;:&quot;hm-dusk.github.io&quot;}); }数据查询任何的数据库之中，数据的查询操作都是最为麻烦的，而在MongoDB数据库里面，对于查询的支持非常到位，包含有关系运算、逻辑运算、数组运算、正则运算等等。 首先对于数据的查询操作核心的语法：“db.集合名称.find({查询条件} [,{设置显示的字段}])”。 db.dept.find();条件查询 db.dept.find( { &quot;url&quot;:&quot;hm-dusk.github.io&quot; } );对于设置的显示字段严格来讲就称为数据的投影操作，如果不需要显示的字段设置“0”，而需要显示的字段设置“1”。 //不想显示_id db.dept.find( {&quot;url&quot;:&quot;hm-dusk.github.io&quot;},{&quot;_id&quot;:0} ); db.dept.find( {&quot;url&quot;:&quot;hm-dusk.github.io&quot;},{&quot;_id&quot;:0,&quot;url&quot;:1} );大部分的情况下，这种投影操作的意义不大。同时对于数据的查询也可以使用“pretty()”函数进行漂亮显示。 db.dept.find( {&quot;url&quot;:&quot;hm-dusk.github.io&quot;},{&quot;_id&quot;:0} ).pretty();范例：查询单个数据 db.dept.findOne( {&quot;url&quot;:&quot;hm-dusk.github.io&quot;},{&quot;_id&quot;:0} );关系运算在MongoDB里面支持的关系查询操作：大于（$gt）、小于（$lt）、大于等于（$gte）、小于等于（$lte）、不等于（$ne）、等于（key:value、$eq）。但是要想让这些操作可以正常使用，那么需要准备出一个数据集合。 范例：查询姓名是张三的信息 db.students.find({&quot;name&quot;:&quot;张三&quot;}).pretty();范例：查询性别是男的信息 db.students.find({&quot;sex&quot;:&quot;男&quot;}).pretty();范例：查询年龄大于19岁的学生 db.students.find({&quot;age&quot;: {&quot;$gt&quot;:19} }).pretty();范例：查询成绩大于等于60分的学生 db.students.find({&quot;score&quot;: {&quot;$gte&quot;:60} }).pretty();范例：查询姓名不是王五的信息 db.students.find({&quot;name&quot;: {&quot;$ne&quot;:&quot;王五&quot;} }).pretty(); 此时与之前最大的区别就在于，在一个JSON结构里面需要定义其它的JSON结构，并且这种风格在日后通过程序进行操作的时候依然如此。 逻辑运算逻辑运算主要就是三种类型：与（$and）、或（$or）、非（$not、$nor）。 范例：查询年龄在19 ~ 20岁的学生信息 db.students.find({ &quot;age&quot;:{&quot;$gte&quot;:19,&quot;$lte&quot;:20} }).pretty();在进行逻辑运算的时候“and”的连接是最容易的，因为只需要利用“,”分割若干个条件就可以了。 范例：查询年龄不是19岁的学生 db.students.find({ &quot;age&quot;:{&quot;$ne&quot;:19} }).pretty();范例：查询年龄大于19岁，或者成绩大于90分的学生信息 db.students.find({ &quot;$or&quot;:[ {&quot;age&quot;:{&quot;$gt&quot;:19}}, {&quot;score&quot;:{&quot;$gt&quot;:90}} ] }).pretty(); 范例：也可以进行或的求反操作(针对于或的操作可以实现一个求反的功能。) db.students.find({ &quot;$nor&quot;:[ {&quot;age&quot;:{&quot;$gt&quot;:19}}, {&quot;score&quot;:{&quot;$gt&quot;:90}} ] }).pretty();求模模的运算使用“$mod”来完成，语法“{$mod : [数字,余数]}”。 范例：求模 db.students.find({&quot;age&quot;: {&quot;$mod&quot;:[20,1]} }).pretty();利用求模计算可以编写一些数学的计算公式。 范围查询只要是数据库，必须存在有“$in”（在范围之中）、“$nin”（不在范围之中）。 范例：查询姓名是“张三”、“李四”、“王五”的信息 db.students.find({&quot;name&quot;: {&quot;$in&quot;:[&quot;张三&quot;,&quot;李四&quot;,&quot;王五&quot;]} }).pretty();范例：不在范围 db.students.find({&quot;name&quot;: {&quot;$nin&quot;:[&quot;张三&quot;,&quot;李四&quot;,&quot;王五&quot;]} }).pretty();在实际的工作之中，范围的操作很重要。 数组查询首先在mongoDB里面是支持数组保存的，一旦支持了数组保存，就需要针对于数组的数据进行匹配。 范例：保存一部分数组内容此时的数据包含有数组内容，而后需要针对于数组数据进行判断，可以使用几个运算符：$all、$size、$slice、$elemMatch。 范例：查询同时参加语文和数学课程的学生现在两个数组内容都需要保存，所以使用“{&quot;$all&quot;,[内容1,内容2,..]}” db.students.find({&quot;course&quot;: {&quot;$all&quot;: [&quot;语文&quot;,&quot;数学&quot;]} }).pretty();现在所有显示的学生信息里面包含语文和数学的内容，而如果差一个内容的不会显示。 虽然“$all”计算可以用于数组上，但是也可以用于一个数据的匹配上。 范例：查询学生地址是“海淀区”的信息 db.students.find({&quot;address&quot;: {&quot;$all&quot;:[&quot;海淀区&quot;]} }).pretty(); 既然在集合里面现在保存的是数组信息，那么数组就可以利用索引操作，使用“key.index”的方式来定义索引。 范例：查询数组中第二个内容（index= 1，索引下标从0开始）为数学的信息 db.students.find({&quot;course.1&quot;:&quot;数学&quot;}).pretty();范例：要求查询出只参加两门课程的学生使用“$size”来进行数量的控制。 db.students.find({&quot;course&quot;: {&quot;$size&quot;:2} }).pretty();发现在进行数据查询的时候只要是内容复合条件，数组的内容就全部显示出来了，但是现在希望可以控制数组的返回的数量，那么可以使用“$slice”进行控制。 范例：返回年龄为19岁所有学生的信息，但是要求只显示两门参加课程 db.students.find({&quot;age&quot;:19},{&quot;course&quot;:{&quot;$slice&quot;:2}}).pretty();现在只取得了前两门的信息，那么也可以设置负数表示取出后两门的信息。 db.students.find({&quot;age&quot;:19},{&quot;course&quot;:{&quot;$slice&quot;:-2}}).pretty();或者只是取出中间部分的信息。 db.students.find({&quot;age&quot;:19},{&quot;course&quot;:{&quot;$slice&quot;:[1,2]}}).pretty(); 在此时设置的两个数据里面第一个数据表示跳过的数据量，而第二个数据表示返回的数量。 嵌套集合运算在MongoDB数据里面每一个集合数据可以继续保存其它的集合数据，例如：有些学生需要保存家长信息。 范例：增加数据此时给出的内容是嵌套的集合，而这种集合的数据的判断只能够通过“$elemMatch”完成。 范例：查询出年龄大于等于19岁且父母有人是局长的信息 db.students.find( {&quot;$and&quot;:[ {&quot;age&quot;:{&quot;$gte&quot;:19}}, {&quot;parents&quot;:{&quot;$elemMatch&quot;:{&quot;job&quot;:&quot;局长&quot;}}} ]}).pretty(); 由于这种查询的时候条件比较麻烦，所以如果可以，尽量别搞这么复杂的数据结构组成。 判断某个字段是否存在使用“$exists”可以判断某个字段是否存在，如果设置为true表示存在，如果设置为false就表示不存在。 范例：查询具有parents成员的数据 db.students.find({&quot;parents&quot;:{&quot;$exists&quot;:true}}).pretty();范例：查询不具有course成员的数据 db.students.find({&quot;course&quot;:{&quot;$exists&quot;:false}}).pretty(); 可以利用此类查询来进行一些不需要的数据的过滤。 条件过滤实际上习惯于传统关系型数据库开发的我们对于数据的筛选，可能首先想到的一定是where子句，所以在MongoDB里面也提供有“$where”。 范例：使用where进行数据查询 db.students.find({&quot;$where&quot;:&quot;this.age&gt;20&quot;}).pretty(); db.students.find(&quot;this.age&gt;20&quot;).pretty();对于“$where”是可以简化的，但是这类的操作是属于进行每一行的信息判断，实际上对于数据量较大的情况并不方便使用。实际上以上的代码严格来讲是属于编写一个操作的函数。 db.students.find(function(){ return this.age &gt; 20; }).pretty(); db.students.find({&quot;$where&quot;:function(){ return this.age &gt; 20; }}).pretty();以上只是查询了一个判断，如果要想实现多个条件的判断，那么就需要使用and连接。 db.students.find({&quot;$and&quot;:[ {&quot;$where&quot;:&quot;this.age&gt;19&quot;}, {&quot;$where&quot;:&quot;this.age&lt;21&quot;} ] }); 虽然这种形式的操作可以实现数据查询，但是最大的缺点是将在MongoDB里面保存的BSON数据变为了JavaScript的语法结构，这样的方式不方便使用数据库索引机制。 正则运算如果要想实现模糊查询，那么必须使用正则表达式，而且正则表达式使用的是语言Perl兼容的正则表达式的形式。如果要想实现正则使用，则按照如下的定义格式： 基础语法：{key : 正则标记} 完整语法：{key : {&quot;$regex&quot; : 正则标记 , &quot;$options&quot; : 选项}} |- 对于options主要是设置正则的信息查询的标记：|- “i”：忽略字母大小写；|- “m”：多行查找；|- “x”：空白字符串除了被转义的或在字符类中意外的完全被忽略；|- “s”：匹配所有的字符（圆点、“.”），包括换行内容。|- 需要注意的是，如果是直接使用（javascript）那么只能够使用i和m，而“x”和“s”必须使用“$regex”。 范例：查询以“谷”开头的姓名 db.students.find({&quot;name&quot;: /谷/ }).pretty();范例：查询姓名有字母A db.students.find({&quot;name&quot;: /a/i }).pretty(); db.students.find({&quot;name&quot;: {&quot;$regex&quot;: /a/i }}).pretty();如果要执行模糊查询的操作，严格来讲只需要编写一个关键字就够了。正则操作之中除了可以查询出单个字段的内容之外，也可以进行数组数据的查询。 范例：查询数组数据 db.students.find({&quot;course&quot;: /语?/ }).pretty(); db.students.find({&quot;course&quot;: /语/ }).pretty(); MongoDB中的正则符号和之前Java正则是有一些小小差别，不建议使用以前的一些标记，正则就将其应用在模糊数据查询上。 数据排序在MongoDB里面数据的排序操作使用“sort()”函数，在进行排序的时候可以有两个顺序：升序（1）、降序（-1）。 范例：数据排序 db.students.find().sort({&quot;score&quot;: -1}).pretty();但是在进行排序的过程里面有一种方式称为自然排序，按照数据保存的先后顺序排序，使用“$natural”表示。 范例：自然排序 db.students.find().sort({&quot;$natural&quot;: -1}).pretty(); 在MongoDB数据库里面排序的操作相比较传统关系型数据库的设置要简单。 数据分页显示在MongoDB里面的数据分页显示也是符合于大数据要求的操作函数： skip(n)：表示跨过多少数据行； limit(n)：取出的数据行的个数限制。 范例：分页显示（第一页，skip(0)、limit(5)） db.students.find().skip(0).limit(5).sort({&quot;age&quot;:-1}).pretty();范例：分页显示（第二页，skip(5),limit(5）) db.students.find().skip(5).limit(5).sort({&quot;age&quot;:-1}).pretty(); 这两个分页的控制操作，就是在以后只要是存在有大数据的信息情况下都会使用它。 四、数据更新操作对于MongoDB而言，数据的更新基本上是一件很麻烦的事情，如果在实际的工作之中，真的具有此类的操作支持，那么最好的做法，在MongoDB里面对于数据的更新操作提供了两类函数：save()、update()。 函数的基本使用如果要修改数据最直接的使用函数就是update()函数，但是这个函数的语法要求很麻烦： 语法：db.集合.update(更新条件 , 新的对象数据（更新操作符） , upsert , multi)； |- upsert：如果要更新的数据不存在，则增加一条新的内容（true为增加、false为不增加）；|-multi：表示是否只更新满足条件的第一行记录，如果设置为false，只更新第一条，如果是true全更新。 范例：更新存在的数据——将年龄是19岁的人的成绩都更新为100分（此时会返回多条数据） 只更新第一条数据：db.students.update({&quot;age&quot;:19}, {&quot;$set&quot;: {&quot;score&quot;:100}}, false,false); 所有满足条件的数据都更新db.students.update({&quot;age&quot;:19}, {&quot;$set&quot;:{&quot;score&quot;:100}}, false,true); 范例：更新不存在的数据 db.students.update({&quot;age&quot;:30}, {&quot;$set&quot;:{&quot;name&quot;:&quot;不存在&quot;}}, true,false); //由于没有年龄是30岁的学生信息，所以此时相当于进行了数据的创建。那么除了update()函数之外，还提供有一个save()函数，这个函数的功能与更新不存在的内容相似。 范例：使用save()操作 db.students.save({&quot;_id&quot;: ObjectID(&quot;55949a13eecd74894d19d8dc&quot;), &quot;age&quot;:50}); //由于此时对应的id数据存在了，所以就变为了更新操作。但是如果要保存的数据不存在（不能保存有“_id”），那么就变为了增加操作。修改器对MongoDB数据库而言，数据的修改会牵扯到内容的变更、结构的变更（包含有数组），所以在进行MongoDB设计的时候就提供有一系列的修改器的应用，那么像之前使用的“$set”就是一个修改器。 $inc：主要针对于一个数字字段，增加某个数字字段的数据内容；语法：{&quot;$inc&quot; : {&quot;成员&quot; : 内容}} 范例：将所有年龄为19岁的学生成绩一律减少30分,年龄加1 db.students.update({&quot;age&quot;:19}, {&quot;$inc&quot;: {&quot;score&quot;:-30,&quot;age&quot;:1} });$set：进行内容的重新设置；语法：{&quot;$set&quot; : {&quot;成员&quot; : &quot;新内容&quot;}} 范例：将年龄是20岁的人的成绩修改为89 db.students.update({&quot;age&quot;:20}, {&quot;$set&quot;: {&quot;score&quot;:89} });$unset：删除某个成员的内容；语法：{&quot;$unset&quot; : {&quot;成员&quot; : 1}} 范例：删除“张三”的年龄与成绩信息 db.students.update({&quot;name&quot;:&quot;张三&quot;}, {&quot;$unset&quot;: {&quot;age&quot;:1,&quot;score&quot;:1} }); //执行之后指定的成员内容就消失了。$push：相当于将内容追加到指定的成员之中（基本上是数组）；语法：${&quot;$push&quot; : {成员 : value}} 范例：向“李四”添加课程信息（此时张三信息下没有course信息） db.students.update({&quot;name&quot;:&quot;李四&quot;}, {&quot;$push&quot;: {&quot;course&quot;:&quot;语文&quot;} });范例：向“谷大神 - E”里面的课程追加一个“美术” db.students.update({&quot;name&quot;:&quot;谷大神 - E&quot;}, {&quot;$push&quot;: {&quot;course&quot;:&quot;美术&quot;} }); 就是进行数组数据的添加操作使用的，如果没有数组则进行一个新的数组的创建，如果有则进行内容的追加。 $pushAll：与“$push”是类似的，可以一次追加多个内容到数组里面；语法：${&quot;$pushAll&quot; : {成员 : 数组内容}} 范例：向“王五”的信息里面添加多个课程内容 db.students.update({&quot;name&quot;:&quot;王五&quot;}, {&quot;$pushAll&quot;: {&quot;cource&quot;:[&quot;美术&quot;,&quot;音乐&quot;,&quot;田径&quot;]} });$addToSet：向数组里面增加一个新的内容，只有这个内容不存在的时候才会增加；语法：{&quot;$addToSet&quot; : {成员 : 内容}} 范例：向王五的信息增加新的内容 db.students.update({&quot;name&quot;: &quot;王五&quot;}, {&quot;$addToSet&quot;: {&quot;course&quot;:&quot;舞蹈&quot;} }); //此时会判断要增加的内容在数组里面是否已经存在了，如果不存在则向数组之中追加内容，如果存在了则不做任何的修改操作。$pop：删除数组内的数据；语法：{&quot;$pop&quot; : {成员 : 内容}}内容如果设置为-1表示删除第一个，如果是1表示删除最后一个； 范例：删除王五的第一个课程 db.students.update({&quot;name&quot;:&quot;王五&quot;}, {&quot;$pop&quot;: {&quot;course&quot;:1}});$pull：从数组内删除一个指定内容的数据语法：{&quot;$pull&quot; : {成员 : 数据}}进行数据比对的，如果是此数据则删除； 范例：删除王五学生的音乐课程信息 db.students.update({&quot;name&quot;:&quot;王五&quot;}, {&quot;$pull&quot;:&quot;音乐&quot;});$pullAll：一次性删除多个内容；语法：{&quot;$pull&quot; : {成员 : [数据, 数据,...]}} 范例：删除“谷大神 - A”中的三门课程 db.students.update({&quot;name&quot;:&quot;谷大神 - A&quot;}, {&quot;$pullAll&quot;:{&quot;course&quot;:[&quot;语文&quot;,&quot;数学&quot;,&quot;英语&quot;]}});$rename：为成员名称重命名；语法：{&quot;$rename&quot; : {旧的成员名称 : 新的成员名称}} 范例：将“张三”name成员名称修改为“姓名” db.students.update({&quot;name&quot;:&quot;张三&quot;}, {&quot;$rename&quot;:{&quot;name&quot;:&quot;姓名&quot;}});在整个MongoDB数据库里面，提供的修改器的支持很到位。 删除数据]]></content>
      <categories>
        <category>数据库</category>
        <category>Mongodb</category>
      </categories>
      <tags>
        <tag>Mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim程式编辑器]]></title>
    <url>%2F2017%2F07%2F26%2FVim%E7%A8%8B%E5%BC%8F%E7%BC%96%E8%BE%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Linux下的vim编辑器 vim编辑器常用指令一般指令模式（command mode）：移动游标方法 指令 代表含义 [Ctrl]+[f] 向下移动一页，相当于[Page Down] [Ctrl]+[b] 向上移动一页，相当于[Page Up] 0或者功能键[Home] 移动到行最左侧 $或者功能键[End] 移动到行最右侧 G 移动到文档最后一列 nG n代表数字，移动到第n行(可配合:set nu) gg 移动到第一行，相当于1G n[Enter] n为数字。游标向下移动n行 保存离开 指 令 代表含义 :w 将编辑的资料写入磁盘中 :w! 当档案属性为只读时，强制写入该档案。具体能不能写入，跟用户对该档案的权限有关 :q 离开vi :q! 强制离开，不存储档案 :wq 储存后离开 删除、复制和粘贴 指令 代表含义 x,X x为向后删除一个字符（相当于[del]），X为向前删除一个字符（相当于[backspace]） dd 删除游标所在的一整行（可以理解为剪切，p对dd同样有效） ndd n表示数字，删除游标所在的向下n行 yy 复制游标所在行 nyy n表示数字，复制游标所在的向下n行 p,P p为粘贴插入到游标下一行，原始的文档向后推，P为上一行 u 复原上一个动作（撤销） Ctrl+r 重复上一个动作 . 小数点，重复上一个动作 搜索与替换 指令 代表含义 /word 从光标处开始向下查找名为word的字符串 ?word 从光标处开始向上查找名为word的字符串 n 重复上一个搜索动作 N 与n功能相反（反向搜索） :n1,n2s/word1/word2/g n1与n2为数字。在第n1与n2列之间寻找word1这个字串，并将该字串取代为word2 ！举例来说，在100到200列之间搜寻vim并取代为VIM则:100,200s/vim/VIM/g :1,$s/word1/word2/g 从第一列到最后一列寻找word1字串，并将该字串取代为word2 :1,$s/word1/word2/gc 从第一列到最后一列寻找word1字串，并将该字串取代为word2 ，且在取代前显示提示字元给使用者确认(confirm)是否需要取代 区块选择操作 指令 代表含义 v 字元选择，会将游标经过的地方反白选择！ V 列选择，会将游标经过的列反白选择！ Ctrl+v 区块选择，可以用长方形的方式选择资料 y 将反白的地方复制起来 d 将反白的地方删除掉 p 将刚刚复制的区块，在游标所在处贴上！ 一般指令模式切换到编辑模式： 指令 代表含义 i或I 进入插入模式(Insert mode)：i为『从目前游标所在处插入』， I为『在目前所在列的第一个非空白字元处开始插入』。 a或A 进入插入模式(Insert mode)：a为『从目前游标所在的下一个字元处开始插入』， A为『从游标所在列的最后一个字元处开始插入』。 o或O 进入插入模式(Insert mode)：这是英文字母o的大小写。o为『在目前游标所在的下一列处插入新的一列』； O为在目前游标所在处的上一列插入新的一列！ r或R 进入取代模式(Replace mode)：r只会取代游标所在的那一个字元一次；R会一直取代游标所在的文字，直到按下ESC为止； vim编辑器页面设置 指令 代表含义 :set nu 设置vim编辑器显示行号 :set nonu 取消行号 设置vim编辑器打开默认显示行号： 新建或修改$HOME/.vimrc文件，在文件中添加 ```vim set number ``` 参考文档：鸟哥的Linux私房菜之vim程式编辑器]]></content>
      <categories>
        <category>Linux</category>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java去掉字符串特殊字符的方法]]></title>
    <url>%2F2017%2F07%2F25%2FJava%E5%8E%BB%E6%8E%89%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%89%B9%E6%AE%8A%E5%AD%97%E7%AC%A6%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[java中利用Unicode编码过滤字符串中特定字符的方法 语法：str = str.replaceAll(&quot;[\pP]&quot;, &quot;&quot;);\pP 中小写 p 是property的意思，表示 Unicode 属性，用于 Unicode 正表达式的前缀。大写 P 表示 Unicode 字符集七个字符属性之一：标点字符。 符号 表示的意思 P 标点字符 L 字母 M 标记符号（一般不会单独出现） Z 分隔符（比如空格、换行等） S 符号（比如数学符号、货币符号等） N 数字（比如阿拉伯数字、罗马数字等） C 其他字符 例： String result = &quot;,.!，，D_NAME。！；‘’”“《》**dfs #$%^&amp;()-+1431221厉害123漢字どうかのjavaを決繁体&quot;; result = result.replaceAll(&quot;[\\pP\\pZ\\pS\\pC\\pM]&quot;, &quot;&quot;); //去掉标点符号、空格，换行、等所有特殊字符 /* 输出： DNAMEdfs1431221厉害123漢字どうかのjavaを決繁体 */以后字符串的对应字符处理就可以用这个简单可靠的方法了]]></content>
      <categories>
        <category>Java</category>
        <category>字符串</category>
      </categories>
      <tags>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年经历过的异常信息]]></title>
    <url>%2F2017%2F07%2F24%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%BB%8F%E5%8E%86%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[记录遇到的异常以及原由和解决方式 java.lang.UnsupportedOperationException不支持功能异常，常常发生在Arrays.asList()后再对List进行add、remove这些操作时 File file = new File("e:/123/"); List&lt;File> files = Arrays.asList(file.listFiles()); ListIterator&lt;File> lit = files.listIterator(); while(lit.hasNext()){//遍历该分类下的文件 File file = lit.next(); if(file.getName().contains("123")){//包含123 lit.remove(); //将该文件从list中移除 } } Arrays.asList() 返回java.util.Arrays$ArrayList， 而不是ArrayList。Arrays$ArrayList和ArrayList都是继承AbstractList。remove，add等 方法在AbstractList中是默认抛出 UnsupportedOperationException异常而且不作任何操作。ArrayList 重写了这些方法来对list进行操作，但是Arrays$ArrayList没有重写 remove(int)，add(int)等方法，所以会抛出UnsupportedOperationException异常。 解决方法:在遍历List（由数组转换而来）需要add和remove时，转换成List时不要用Arrays.asList()方法 第一种：Collections的addAll静态方法 File file = new File("e:/123/"); List&lt;File> files = Lists.newArrayList(); Collections.addAll(files,file.listFiles()); ListIterator&lt;File> lit = files.listIterator(); while(lit.hasNext()){//遍历该分类下的文件 ... } 第二种：遍历数组，一个元素一个元素的add进List File file = new File("e:/123/"); List&lt;File> files = Lists.newArrayList(); for(File file : file.listFiles()){ files.add(file); }]]></content>
      <categories>
        <category>Java</category>
        <category>代码规范</category>
      </categories>
      <tags>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浮点型计算中丢失精度问题]]></title>
    <url>%2F2017%2F07%2F23%2F%E8%AE%A1%E7%AE%97%E4%B8%AD%E4%B8%A2%E5%A4%B1%E7%B2%BE%E5%BA%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[今天在写代码过程中遇到了double类型丢失精度的问题 在进行浮点数计算时，用BigDecimalpublic class test{ @Test public void fun(){ double a = 0.3; double b = 0.6; double c = a+b; System.out.println(c); //输出0.8999999999999999 } } 上面语句按道理说输出应该是0.9，但是实际输出为0.8999999999999999。在计算机内部，首先将double转换成二进制，再进行二进制的计算，最后把计算好的二进制结果转换成double。在两次转换的过程中由于计算机表示的位数有限，而0.3和0.6表示成二进制都是无限的（尴尬），所以在转换成二进制的时候就已经改变了0.3和0.6的值，计算出来的结果当然是错的。实际上，0.0到0.9的10个数中，只有0.0和0.5能精确表示。 同理，下面的代码输出结果也丢失精度。 public class test{ public static void main(String[] args){ double a = 0.2 + 0.4; double b = 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1; System.out.println(a);//输出0.6000000000000001 System.out.println(b);//输出0.8999999999999999 } } 那么在java中如何避免这个问题呢？当然最好的方式是不用这种计算方式，但是如果必须用到的话，就只能用BigDecimal这个类。 参考资料：代码之谜（四）- 浮点数（从惊讶到思考）代码之谜（五）- 浮点数（谁偷了你的精度？）]]></content>
      <categories>
        <category>Java</category>
        <category>精度问题</category>
      </categories>
      <tags>
        <tag>精度问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java遍历Map的几种方式]]></title>
    <url>%2F2017%2F07%2F19%2FJava%E9%81%8D%E5%8E%86Map%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[今天突然要用到Map的遍历，在此总结几种Map的遍历方法 第一种 在for-each循环中使用entries来遍历 最常见的且大多数情况下使用的方式，在键和值都需要的时候使用 但是，如果要在遍历中删除某个键值对，则不能使用该方法 Map&lt;Integer, Integer> map = Maps.newHashMap(); for(Map.Entry&lt;Integer,Integer> entry:map.entrySet()){ System.out.println("键：" + entry.getKey() + "，值：" + entry.getValue()); } 注意：for-each循环在Java 5中被引入所以该方法只能应用于java 5或更高的版本中。如果你遍历的是一个空的map对象，for-each循环将抛出NullPointerException，因此在遍历前你总是应该检查空引用。 第二种 在for-each循环中遍历keys或values 如果只需要map中的键或者值，你可以通过keySet或values来实现遍历，而不是用entrySet Map&lt;Integer, Integer> map = Maps.newHashMap(); //遍历map中的键 for (Integer key : map.keySet()) { System.out.println("Key = " + key); } //遍历map中的值 for (Integer value : map.values()) { System.out.println("Value = " + value); } 该方法比entrySet遍历在性能上稍好（快了10%），而且代码更加干净。 第三种 使用Iterator遍历Map&lt;Integer, Integer> map = Maps.newHashMap(); Iterator&lt;Map.Entry&lt;Integer, Integer>> entries = map.entrySet().iterator(); while (entries.hasNext()) { Map.Entry&lt;Integer, Integer> entry = entries.next(); System.out.println("Key = " + entry.getKey() + ", Value = " + entry.getValue()); //在这里面可以删除键值对 if(entries.next().getValue()>0){ entries.remove(); } } 该种方式看起来冗余却有其优点所在。首先，在老版本java中这是惟一遍历map的方式。另一个好处是，你可以在遍历时调用iterator.remove()来删除entries，另两个方法则不能。根据javadoc的说明，如果在for-each遍历中尝试使用此方法，结果是不可预测的。 从性能方面看，该方法类同于for-each遍历（即方法二）的性能。 第四种 通过键找值遍历（效率低）Map&lt;Integer, Integer> map = new HashMap&lt;Integer, Integer>(); for (Integer key : map.keySet()) { Integer value = map.get(key); System.out.println("Key = " + key + ", Value = " + value); } 作为方法一的替代，这个代码看上去更加干净；但实际上它相当慢且无效率。因为从键取值是耗时的操作（与方法一相比，在不同的Map实现中该方法慢了20%~200%）。如果你安装了FindBugs，它会做出检查并警告你关于哪些是低效率的遍历。所以尽量避免使用。 总结如果仅需要键(keys)或值(values)使用方法二。如果你使用的语言版本低于java 5，或是打算在遍历时删除entries，必须使用方法三。否则使用方法一(键值都要)。]]></content>
      <categories>
        <category>Java</category>
        <category>Map</category>
      </categories>
      <tags>
        <tag>集合</tag>
        <tag>遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL笔记]]></title>
    <url>%2F2017%2F07%2F18%2FMySQL%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[MySql笔记 一、常见数据库Oracle：甲骨文（占有率最高）DB2：IBMSQL Server：微软Sybase：赛尔斯MySQL：甲骨文 二、RDBMS（关系型数据库管理系统）表结构： 三、SQL（结构化查询语句）SQL语句分类： DDL：数据库或表的结构操作 （重点） DML：对表的记录进行更新（增、删、改）（重点） DQL：对表的记录的查询（重点、难点） DCL：对用户的创建，及授权 四、DDL（数据库或表的结构操作）操作库： 查看所有数据库：SHOW DATABASES 切换（选择要操作的）数据库：USE 数据库名 创建数据库：CREATE DATABASE [IF NOT EXISTS] 数据库名 删除数据库：DROP DATABASE [IF EXISTS] 数据库名 修改数据库编码：ALTER DATABASE 数据库名 CHARACTER SET utf8 操作表： 创建表： CREATE TABLE [IF NOT EXISTS] 表名（ 列名 列类型， 列名 列类型， 。。。 ）； 查看当前数据库中所有表：SHOW TABLES 查询某一张表结构：DESC 表名 删除表：DROP TABLE 表名 修改表：前缀：ALTER TABLE 表名称添加列：add（ 列名 列类型， 列名 列类型， 。。。 ）； 修改列类型：modify 列名 列新的类型； 修改列名：change 原列名 新列名 列类型； 删除列：drop 列名； 修改表名称：rename to 新表名；五、数据类型（列类型）在数据库中所有的字符串类型，必须使用单引，不能使用双引！日期类型也要使用单引！ int：整型double：浮点型，double（5，2）：表示最多5位，必须有两位小数decimal：浮点型char：固定长度字符串类型：char（255）最大255，数据长度不足指定长度，会补足到指定长度varchar：可变长度字符串类型text（clob）：字符串类型（mysql特有，sql server里面也有）blod：字节类型，二进制date：日期类型，格式为：yyyy-MM-ddtime：时间类型，格式为：hh：mm：sstimestamp：时间戳类型 六、DML（对表的记录更新（增、删、改））插入数据INSERT INTO 表名（列名1，列名2，。。。）VALUES（列值1，列值2，。。。）； 在表名后给出要插入的列名，其他没有指定的列等同与插入null值。所以插入记录总是插入一行，不可能是半行。在VALUES后给出列值，值的顺序和个数必须与前面指定的列对应如果表中有自增的主键，那么可以用这种方法不指定主键列，也可以用null和0代替，MySQL会自己处理 INTERT INTO 表名 VALUES(列值1, 列值2) 没有给出要插入的列，那么表示插入所有列。值的个数必须是该表列的个数。值的顺序，必须与表创建时给出的列的顺序相同。 修改数据UPDATE 表名 SET 列名1=列值1, 列名2=列值2, ... [WHERE 条件] 条件(条件可选的)： 条件必须是一个boolean类型的值或表达式：UPDATE t_person SET gender=’男’, age=age+1 WHERE sid=’1’; 运算符：=、!=、&lt;&gt;、&gt;、&lt;、&gt;=、&lt;=、BETWEEN…AND、IN(…)、IS NULL、NOT、OR、AND WHERE age &gt;= 18 AND age &lt;= 80 等价于WHERE age BETWEEN 18 AND 80WHERE name=&#39;zhangSan&#39; OR name=&#39;liSi&#39; 等价于WHERE name IN (&#39;zhangSan&#39;, &#39;liSi&#39;) WHERE age IS NULL, 不能使用等号 WHERE age IS NOT NULL删除数据DELETE FROM 表名 [WHERE 条件]; TRUNCATE TABLE 表名：TRUNCATE是DDL语句，它是先删除drop该表，再create该表。而且无法回滚！！！七、DCL（对用户的创建，及授权） 一个项目创建一个用户！一个项目对应的数据库只有一个！ 这个用户只能对这个数据库有权限，其他数据库你就操作不了了！ 创建用户CREATE USER 用户名@IP地址 IDENTIFIED BY &#39;密码&#39;; &gt; 用户只能在指定的IP地址上登录 CREATE USER 用户名@&#39;%&#39; IDENTIFIED BY &#39;密码&#39;; &gt; 用户可以在任意IP地址上登录给用户授权GRANT 权限1, … , 权限n ON 数据库.* TO 用户名@IP地址 &gt; 权限、用户、数据库 &gt; 给用户分派在指定的数据库上的指定的权限 &gt; &gt; 例如；GRANT,CREATE,ALTER,DROP,INSERT,UPDATE,DELETE,SELECT ON mydb1.* TO user1@localhost; &gt; 给user1用户分派在mydb1数据库上的create、alter、drop、insert、update、delete、select权限 * GRANT ALL ON 数据库.* TO 用户名@IP地址; &gt; 给用户分派指定数据库上的所有权限撤销授权 * REVOKE 权限1, … , 权限n ON 数据库.* FROM 用户名@IP地址; &gt; 撤消指定用户在指定数据库上的指定权限 &gt; &gt; 例如；REVOKE CREATE,ALTER,DROP ON mydb1.* FROM user1@localhost; &gt; 撤消user1用户在mydb1数据库上的create、alter、drop权限查看权限SHOW GRANTS FOR 用户名@IP地址 &gt; 查看指定用户的权限删除用户DROP USER 用户名@IP地址八、DQL数据库查询语言一、 基本查询1. 字段(列)控制1) 查询所有列 SELECT * FROM 表名; SELECT * FROM emp; –&gt;其中“ * ”表示查询所有列 2) 查询指定列 SELECT 列1 [, 列2, ... 列N] FROM 表名; SELECT empno, ename, sal, comm FROM 表名; 3) 完全重复的记录只一次 当查询结果中的多行记录一模一样时，只显示一行。一般查询所有列时很少会有这种情况，但只查询一列（或几列）时，这种可能就大了！ SELECT DISTINCT * | 列1 [, 列2, ... 列N] FROM 表名; SELECT DISTINCT sal FROM emp; –&gt;保查询员工表的工资，如果存在相同的工资只显示一次！ 4) 列运算 数量类型的列可以做加、减、乘、除运算SELECT sal*1.5 FROM emp;SELECT sal+comm FROM emp; 字符串类型可以做连续运算SELECT CONCAT(&#39;$&#39;, sal) FROM emp; 转换NULL值有时需要把NULL转换成其它值，例如comm+1000时，如果comm列存在NULL值，那么NULL+1000还是NULL，而我们这时希望把NULL当前0来运算。SELECT IFNULL(comm, 0)+1000 FROM emp;–&gt;IFNULL(comm, 0)：如果comm中存在NULL值，那么当成0来运算。 给列起别名你也许已经注意到了，当使用列运算后，查询出的结果集中的列名称很不好看，这时我们需要给列名起个别名，这样在结果集中列名就显示别名了SELECT IFNULL(comm, 0)+1000 AS &#39;奖金&#39; FROM emp;–&gt; 其中AS可以省略 2. 条件控制1.条件查询 与前面介绍的UPDATE和DELETE语句一样，SELECT语句也可以使用WHERE子句来控制记录。SELECT empno,ename,sal,comm FROM emp WHERE sal &gt; 10000 AND comm IS NOT NULL;SELECT empno,ename,sal FROM emp WHERE sal BETWEEN 20000 AND 30000;SELECT empno,ename,job FROM emp WHERE job IN (&#39;经理&#39;, &#39;董事长&#39;); 2.模糊查询 当你想查询姓张，并且姓名一共两个字的员工时，这时就可以使用模糊查询SELECT * FROM emp WHERE ename LIKE &#39;张_&#39;; –&gt; 模糊查询需要使用运算符：LIKE，其中_匹配一个任意字符，注意，只匹配一个字符而不是多个。 –&gt; 上面语句查询的是姓张，名字由两个字组成的员工。SELECT * FROM emp WHERE ename LIKE &#39;___&#39;; –&gt;姓名由3个字组成的员工 如果我们想查询姓张，名字几个字可以的员工时就要使用“%”了。SELECT * FROM emp WHERE ename LIKE &#39;张%&#39;;–&gt; 其中%匹配0~N*个任意字符，所以上面语句查询的是姓张的所有员工。SELECT * FROM emp WHERE ename LIKE &#39;%阿%&#39;;–&gt; 千万不要认为上面语句是在查询姓名中间带有阿字的员工，因为%匹配0~N个字符，所以姓名以阿开头和结尾的员工也都会查询到。SELECT * FROM emp WHERE ename LIKE &#39;%&#39;;–&gt; 这个条件等同与不存在，但如果姓名为NULL*的查询不出来！ 二、排序 升序SELECT * FROM WHERE emp ORDER BY sal ASC; –&gt; 按sal排序，升序！–&gt; 其中ASC是可以省略的 降序SELECT * FROM WHERE emp ORDER BY comm DESC; –&gt; 按comm排序，降序！–&gt; 其中DESC不能省略 使用多列作为排序条件SELECT * FROM WHERE emp ORDER BY sal ASC, comm DESC; –&gt; 使用sal升序排，如果sal相同时，使用comm的降序排 三、聚合函数 聚合函数用来做某列的纵向运算。 COUNTSELECT COUNT(*) FROM emp;–&gt; 计算emp表中所有列都不为NULL的记录的行数SELECT COUNT(comm) FROM emp;–&gt; 云计算emp表中comm列不为NULL的记录的行数 MAXSELECT MAX(sal) FROM emp;–&gt; 查询最高工资 MINSELECT MIN(sal) FROM emp;–&gt; 查询最低工资 SUMSELECT SUM(sal) FROM emp;–&gt; 查询工资合 AVGSELECT AVG(sal) FROM emp;–&gt; 查询平均工资 四、分组查询 分组查询是把记录使用某一列进行分组，然后查询组信息。 例如：查看所有部门的记录数。 SELECT deptno, COUNT(*) FROM emp GROUP BY deptno; –&gt; 使用deptno分组，查询部门编号和每个部门的记录数 SELECT job, MAX(SAL) FROM emp GROUP BY job; –&gt; 使用job分组，查询每种工作的最高工资 组条件 以部门分组，查询每组记录数。条件为记录数大于3 SELECT deptno, COUNT(*) FROM emp GROUP BY deptno HAVING COUNT(*) &gt; 3; 五、limit子句(方言) LIMIT用来限定查询结果的起始行，以及总行数。 例如：查询起始行为第5行，一共查询3行记录 SELECT * FROM emp LIMIT 4, 3; –&gt; 其中4表示从第5行开始，其中3表示一共查询3行。即第5、6、7行记录。 select * from emp limit 0, 5; 一页的记录数：10行 查询第3页 select * from emp limit 20, 10; (当前页-1) * 每页记录数(3-1) * 10 (17-1) * 8, 8 九、备份恢复数据库 –&gt; sql语句sql语句 –&gt; 数据库 数据库导出SQL脚本(备份数据库内容，并不是备份数据库！) mysqldump –u用户名 –p密码 数据库名&gt;生成的脚本文件路径例如：mysqldump -uroot -p123 mydb1&gt;C:\mydb1.sql(与mysql.exe和mysqld.exe一样, 都在bin目录下)注意：不要打分号，不要登录mysql，直接在cmd下运行注意：生成的脚本文件中不包含create database语句 执行SQL脚本第一种方式 mysql -u用户名 -p密码 数据库&lt;脚本文件路径例如：先删除mydb1库，再重新创建mydb1库mysql -uroot -p123 mydb1&lt;C:\mydb1.sql注意：不要打分号，不要登录mysql，直接在cmd下运行 第二种方式 登录mysqlsource SQL脚本路径例如：先删除mydb1库，再重新创建mydb1库切换到mydb1库source c:\mydb1.sql 十、主键约束（唯一标识） 非空 唯一 被引用（学习外键时） 当表的某一列被指定为主键后，该列就不能为空，不能有重复值出现。创建表时指定主键的两种方式： ``` 1. CREATE TABLE stu( sid CHAR(6) PRIMARY KEY, sname VARCHAR(20), age INT, gender VARCHAR(10) ); 指定sid列为主键列，即为sid列添加主键约束 2. CREATE TABLE stu( sid CHAR(6), sname VARCHAR(20), age INT, gender VARCHAR(10), PRIMARY KEY(sid) ); 指定sid列为主键列，即为sid列添加主键约束 ``` 修改表时指定主键： ALTER TABLE stu ADD PRIMARY KEY(sid); 删除主键： ALTER TABLE stu DROP PRIMARY KEY; 十一、主键自增长 因为主键列的特性是：必须唯一、不能为空，所以我们通常会指定主键类为整型，然后设置其自动增长，这样可以保证在插入数据时主键列的唯一和非空特性。 创建表时指定主键自增长 CREATE TABLE stu( sid INT PRIMARY KEY AUTO_INCREMENT, sname VARCHAR(20), age INT, gender VARCHAR(10) ); 修改表时设置主键自增长：ALTER TABLE stu CHANGE sid sid INT AUTO_INCREMENT; 修改表时删除主键自增长：ALTER TABLE stu CHANGE sid sid INT; 测试主键自增长： INSERT INTO stu VALUES(NULL, ‘zhangSan’,23,’male’);INSERT INTO stu(sname,age,gender) VALUES(‘zhangSan’,23,’male’); 十二、非空约束与唯一约束 非空约束 因为某些列不能设置为NULL值，所以可以对列添加非空约束。 例如：```CREATE TABLE stu( sid INT PRIMARY KEY AUTO_INCREMENT, sname VARCHAR(20) NOT NULL, age INT, gender VARCHAR(10)); 对sname列设置了非空约束``` 唯一约束 车库某些列不能设置重复的值，所以可以对列添加唯一约束。 例如：```CREATE TABLE stu( sid INT PRIMARY KEY AUTO_INCREMENT, sname VARCHAR(20) NOT NULL UNIQUE, age INT, gender VARCHAR(10)); 对sname列设置了唯一约束 十三、概述模型、对象模型、关系模型对象模型：可以双向关联，而且引用的是对象，而不是一个主键！关系模型：只能多方引用一方，而且引用的只是主键，而不是一整行记录。 对象模型：**在java中是domain！！！例如：User、Student is ahas a(关联)1对11对多多对多use a 关系模型：在数据库中表！！！ 当我们要完成一个软件系统时，需要把系统中的实体抽取出来，形成概念模型。 例如部门、员工都是系统中的实体。概念模型中的实体最终会成为Java中的类、数据库中表。 实体之间还存在着关系，关系有三种： 1对多：例如每个员工都从属一个部门，而一个部门可以有多个员工，其中员工是多方，而部门是一方。 1对1：例如老公和老婆就是一对一的关系，一个老公只能有一个老婆，而一个老婆只能有一个老公。 多对多：老师与学生的关系就是多对多，一个老师可以有多个学生，一个学生可以有多个老师。 概念模型：在Java中成为实体类（javaBean） 类就使用成员变量来完成关系，一般都是双向关联！ 多对一双向中关联，即员工关联部门，部门也关联员工 class Employee {//多方关联一方 ... private Department department; } class Department {//一方关联多方 ... private List&lt;Employee&gt; employees; }``` class Husband { … private Wife wife; } class Wife { … private Husband } class Student { … private List teachers } class Teacher { … private List students; }``` 十四、外键约束 外键必须是另一表的主键的值(外键要引用主键！) 外键可以重复 外键可以为空 一张表中可以有多个外键！ 概念模型在数据库中成为表 数据库表中的多对一关系，只需要在多方使用一个独立的列来引用1方的主键即可 /*员工表*/ create talbe emp ( empno int primary key,/*员工编号*/ ... deptno int/*所属部门的编号*/ ); /*部门表*/ create table dept ( deptno int primary key,/*部门编号*/ ... ); emp表中的deptno列的值表示当前员工所从属的部门编号。也就是说emp.deptno必须在dept表中是真实存在！ 但是我们必须要去对它进行约束，不然可能会出现员工所属的部门编号是不存在的。这种约束就是外键约束。 我们需要给emp.deptno添加外键约束，约束它的值必须在dept.deptno中存在。外键必须是另一个表的主键！语法：CONSTRAINT 约束名称 FOREIGN KEY(外键列名) REFERENCES 关联表(关联表的主键) 创建表时指定外键约束create talbe emp ( empno int primary key, ... deptno int, CONSTRAINT fk_emp FOREIGN KEY(mgr) REFERENCES emp(empno) ); 修改表时添加外键约束ALERT TABLE emp ADD CONSTRAINT fk_emp_deptno FOREIGN KEY(deptno) REFERENCES dept(deptno); 修改表时删除外键约束ALTER TABLE emp DROP FOREIGN KEY fk_emp_deptno;/*约束名称*/ 十五、数据库关系1.一对一关系在表中建立一对一关系比较特殊，需要让其中一张表的主键，即是主键又是外键。 create table husband( hid int PRIMARY KEY, ... ); create table wife( wid int PRIMARY KEY, ... ADD CONSTRAINT fk_wife_wid FOREIGN KEY(wid) REFERENCES husband(hid) );&gt; 其中wife表的wid即是主键，又是相对husband表的外键！ husband.hid是主键，不能重复！ wife.wid是主键，不能重复，又是外键，必须来自husband.hid。 所以如果在wife表中有一条记录的wid为1，那么wife表中的其他记录的wid就不能再是1了，因为它是主键。 同时在husband.hid中必须存在1这个值，因为wid是外键。这就完成了一对一关系。 *从表的主键即是外键！ 2.多对多关系在表中建立多对多关系需要使用中间表，即需要三张表，在中间表中使用两个外键，分别引用其他两个表的主键。 create table student( sid int PRIMARY KEY, ... ); create table teacher( tid int PRIMARY KEY, ... ); create table stu_tea( sid int, tid int, ADD CONSTRAINT fk_stu_tea_sid FOREIGN KEY(sid) REFERENCES student(sid), ADD CONSTRAINT fk_stu_tea_tid FOREIGN KEY(tid) REFERENCES teacher(tid) ); 这时在stu_tea这个中间表中的每条记录都是来说明student和teacher表的关系 例如在stu_tea表中的记录：sid为1001，tid为2001，这说明编号为1001的学生有一个编号为2001的老师 sid tid 101 201 /*编号为101的学生有一个编号为201的老师*/ 101 202 /*编号为101的学生有一个编号为202的老师*/ 101 203 /*编号为101的学生有一个编号为203的老师*/ 102 201 /*编号为102的学生有一个编号为201的老师*/ 102 204 /*编号为102的学生有一个编号为204的老师*/ 十六、多表查询分类： 合并结果集(了解) 连接查询 子查询 1. 合并结果集 要求被合并的表中，列的类型和列数相同 UNION：去除重复行 UNION ALL：不去除重复行SELECT * FROM cd UNION ALL SELECT * FROM ab; 2. 连接查询 分类 内连接 外连接 左外连接 右外连接 全外连接(MySQL不支持) 自然连接（属于一种简化方式） 内连接 方言：SELECT * FROM 表1 别名1, 表2 别名2 WHERE 别名1.xx=别名2.xx 标准：SELECT * FROM 表1 别名1 INNER JOIN 表2 别名2 ON 别名1.xx=别名2.xx 自然：SELECT * FROM 表1 别名1 NATURAL JOIN 表2 别名2 内连接查询出的所有记录都满足条件。 外连接 左外：SELECT * FROM 表1 别名1 LEFT OUTER JOIN 表2 别名2 ON 别名1.xx=别名2.xx 左表记录无论是否满足条件都会查询出来，而右表只有满足条件才能出来。左表中不满足条件的记录，右表部分都为NULL 左外自然：SELECT * FROM 表1 别名1 NATURAL LEFT OUTER JOIN 表2 别名2 ON 别名1.xx=别名2.xx 右外：SELECT * FROM 表1 别名1 RIGHT OUTER JOIN 表2 别名2 ON 别名1.xx=别名2.xx 右表记录无论是否满足条件都会查询出来，而左表只有满足条件才能出来。右表不满足条件的记录，其左表部分都为NULL 右外自然：SELECT * FROM 表1 别名1 NATURAL RIGHT OUTER JOIN 表2 别名2 ON 别名1.xx=别名2.xx 全链接：把left或者right改为full，但是mysql不支持，可以使用UNION来完成全链接 3. 子查询 ：查询中有查询（查看select关键字的个数！） 出现的位置： where后作为条件存在 from后作为表存在(多行多列) 条件 (***)单行单列：SELECT * FROM 表1 别名1 WHERE 列1 [=、&gt;、&lt;、&gt;=、&lt;=、!=] (SELECT 列 FROM 表2 别名2 WHERE 条件) (**)多行单列：SELECT * FROM 表1 别名1 WHERE 列1 [IN, ALL, ANY] (SELECT 列 FROM 表2 别名2 WHERE 条件) (*)单行多列：SELECT * FROM 表1 别名1 WHERE (列1,列2) IN (SELECT 列1, 列2 FROM 表2 别名2 WHERE 条件) (***)多行多列：SELECT * FROM 表1 别名1 , (SELECT ....) 别名2 WHERE 条件]]></content>
      <categories>
        <category>数据库</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年经历过的编码警告（持续更新）]]></title>
    <url>%2F2017%2F07%2F17%2F%E4%BB%A3%E7%A0%81%E8%AD%A6%E5%91%8A%2F</url>
    <content type="text"><![CDATA[记录遇到的代码警告 try catch中catch块臃肿catch块臃肿 虽然不会报错，但是会有警告。 正确的书写方式如下： try{ //... } catch (IOException | ClassNotFoundException e){ e.printStackTrace(); } 可以看到，通过 | 运算符号折叠成一个判断语句，这个符号有”或”的意思，在这里理解非常合适。 应该是java意识到了catch语句的臃肿，所以在JDK中开始建议这种模式来捕获异常。 集合泛型检查警告 该警告就是集合在创建是没有用泛型指定存储的对象类型 解决方式： 为集合指定泛型 List&lt;String> list = new ArrayList&lt;String>(); 利用Commons工具包生成集合import com.google.common.collect.Lists; List&lt;String> list = Lists.newArrayList(); 其他集合类型如：Map、Set类似 一些清晰易懂的警告1、空指针警告 该警告意思是可能报空指针异常，也就是没有做安全处理。消除警告的做法就是在调用方法之前，对有可能出现空指针异常的对象做是否为空的判断，也就是安全处理。 2、忽略返回值警告 该警告消除方式为定义一个该方法的返回值类型去接收返回值。]]></content>
      <categories>
        <category>Java</category>
        <category>代码规范</category>
      </categories>
      <tags>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea创建Maven项目注意事项]]></title>
    <url>%2F2016%2F07%2F23%2FIdea%E5%88%9B%E5%BB%BAMaven%E9%A1%B9%E7%9B%AE%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[新建项目的时候选择maven项目。 接着下一步，这里需要注在Properties中添加一个参数archetypeCatalog=internal，不加这个参数，在maven生成骨架的时候将会非常慢，有时候直接卡住。 来自网上的解释：archetypeCatalog表示插件使用的archetype元数据，不加这个参数时默认为remote，local，即中央仓库archetype元数据，由于中央仓库的archetype太多了所以导致很慢，指定internal来表示仅使用内部元数据。 这样，在初始化maven项目的时候就不会被卡住了。]]></content>
      <categories>
        <category>Java</category>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>Idea</tag>
        <tag>编辑器</tag>
      </tags>
  </entry>
</search>
